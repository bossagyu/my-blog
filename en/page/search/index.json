[{"content":"Overview In this article, we’ll follow the Feast tutorial and run it on a Mac.\nPrerequisites Refer to Setting up a Python development environment on Mac with UV to prepare your environment. Once you’ve set up UV, install Feast in that environment.\nInstalling Feast and Launching the UI 1 uv pip install feast==0.40.1 Note\nAs of 2025/01/13, there is a known bug in Feast causing the UI to fail on the latest version. (Issue)\nInitialize a Feature Repository:\n1 2 3 feast init my_feature_repo Creating a new Feast repository in /Users/kouhei/Program/ML/feast/my_feature_repo. You’ll see a new repository like this:\n1 2 3 4 5 6 7 8 9 10 11 12 tree my_feature_repo . └── my_feature_repo ├── README.md ├── __init__.py └── feature_repo ├── __init__.py ├── data │ └── driver_stats.parquet ├── example_repo.py ├── feature_store.yaml └── test_workflow.py Apply the repository configuration:\n1 2 cd my_feature_repo/feature_repo feast apply Then launch the Feast UI:\n1 feast ui Access http://0.0.0.0:8888/p/my_feature_repo to see the interface.\nManipulating Data in Feast 1. Creating a Training Dataset Starting from step 5 in the tutorial, you’ll use Jupyter Notebook. Install it first:\n1 uv pip install jupyter Launch the notebook:\n1 jupyter notebook In a Jupyter Notebook, run the following code to create a dataset for training:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from feast import FeatureStore import pandas as pd from datetime import datetime entity_df = pd.DataFrame.from_dict({ \u0026#34;driver_id\u0026#34;: [1001, 1002, 1003, 1004], \u0026#34;event_timestamp\u0026#34;: [ datetime(2021, 4, 12, 10, 59, 42), datetime(2021, 4, 12, 8, 12, 10), datetime(2021, 4, 12, 16, 40, 26), datetime(2021, 4, 12, 15, 1, 12) ] }) store = FeatureStore(repo_path=\u0026#34;.\u0026#34;) training_df = store.get_historical_features( entity_df=entity_df, features=[ \u0026#39;driver_hourly_stats:conv_rate\u0026#39;, \u0026#39;driver_hourly_stats:acc_rate\u0026#39;, \u0026#39;driver_hourly_stats:avg_daily_trips\u0026#39; ], ).to_df() print(training_df.head()) # Train model # model = ml.fit(training_df) Below is an example of the notebook output:\n2. Materializing the Online Store To populate the Online Store, run:\n1 feast materialize 1970-01-01T00:00:00Z 2025-01-04T01:24:24Z Note\nIn the provided sample, the command feast materialize-incremental $CURRENT_TIME may not work, so we specify a broader time range here.\n1 2 3 4 5 6 7 8 01/04/2025 10:28:40 AM root WARNING: _list_feature_views will make breaking changes. ... Materializing 2 feature views from 1970-01-01 09:00:00+09:00 to 2025-01-04 10:24:24+09:00 into the sqlite online store. driver_hourly_stats_fresh: 0%| | 0/5 ... 100%|███████████████████████████████████████████████████████████████| 5/5 ... driver_hourly_stats: 100%|███████████████████████████████████████████████████████████████| 5/5 ... 3. Retrieving Data from the Online Store Use Jupyter Notebook to fetch data from the Online Store:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pprint import pprint from feast import FeatureStore store = FeatureStore(repo_path=\u0026#34;.\u0026#34;) feature_vector = store.get_online_features( features=[ \u0026#39;driver_hourly_stats:conv_rate\u0026#39;, \u0026#39;driver_hourly_stats:acc_rate\u0026#39;, \u0026#39;driver_hourly_stats:avg_daily_trips\u0026#39; ], entity_rows=[{\u0026#34;driver_id\u0026#34;: 1001}] ).to_dict() pprint(feature_vector) # Make prediction # model.predict(feature_vector) Example output:\n1 2 3 4 {\u0026#39;acc_rate\u0026#39;: [0.5004482269287109], \u0026#39;avg_daily_trips\u0026#39;: [691], \u0026#39;conv_rate\u0026#39;: [0.3067885637283325], \u0026#39;driver_id\u0026#39;: [1001]} We’ve successfully materialized data in the Online Store and fetched it using Feast.\nSummary We followed the Feast tutorial to prepare training data, materialize that data into the Online Store, and retrieve it. By managing both training and inference datasets in Feast, you can avoid training-serving skew, enhancing the consistency of your ML workflows.\n","date":"2025-01-13T11:49:53+09:00","permalink":"https://bossagyu.com/en/blog/033-feast-tutorial/","title":"Running the Feast Tutorial on macOS"},{"content":"Overview This article introduces how to set up a Python development environment on Mac using UV.\nWhat is UV? UV is a package management tool announced in mid-2024.\nIt is written in Rust and is characterized by faster performance compared to other package managers.\nFor official explanations, please refer to this site.\nUsage Installing UV 1 curl -LsSf https://astral.sh/uv/install.sh | sh Add UV to your PATH:\n1 2 source $HOME/.local/bin/env (sh, bash, zsh) source $HOME/.local/bin/env.fish (fish) Verify installation:\n1 2 uv --version uv 0.5.13 (c456bae5e 2024-12-27) How to Use Create a virtual environment:\n1 uv venv Activate the virtual environment:\n1 source .venv/bin/activate Install packages:\n1 uv pip install \u0026lt;package name\u0026gt; For more details on UV, refer to the official documentation.\nSummary This article explained how to set up a Python development environment on Mac.\nAs of January 2025, UV is both fast and easy to use, making it a valuable tool for setting up a Python development environment.\n","date":"2025-01-01T14:39:53+09:00","permalink":"https://bossagyu.com/en/blog/032-python-uv/","title":"Setting Up a Python Development Environment on Mac with UV"},{"content":"Overview In Scrum teams, ceremonies are essential for aligning team members, sharing progress, addressing challenges, and planning for the next sprint. However, when these ceremonies are conducted out of habit without revisiting their purpose, they can become time-consuming and ineffective.\nThis article explores ways to improve team performance by revisiting and optimizing Scrum ceremonies.\nClarify the Purpose of Each Ceremony Each Scrum ceremony serves a distinct purpose, and ensuring that the agenda aligns with this purpose is crucial. Below are the purposes of the main Scrum ceremonies, based on the Scrum Guide with some interpretive notes.\nCeremony Purpose Daily Scrum Plan the day to achieve the sprint goal. Sprint Review Inspect the sprint\u0026rsquo;s outcome and adapt for future work. Product Backlog Refinement Organize the backlog to support achieving the product goal. Sprint Retrospective Reflect on the sprint to identify improvements for quality and effectiveness. Sprint Planning Define the work to be completed during the next sprint. Enforce Timeboxing Adhering to timeboxes is vital for maintaining efficient ceremonies. Timeboxing limits the duration of meetings to a set timeframe, ensuring focus and productivity. If a ceremony exceeds its timebox, reflect on why and plan adjustments to prevent recurrence.\nTips for Efficient Discussions in Specific Ceremonies Certain ceremonies, like the Daily Scrum, Sprint Review, and Product Backlog Refinement, often consume more time than necessary. Here are some strategies to enhance their effectiveness.\nDaily Scrum The Daily Scrum should focus on clarifying what the team needs to accomplish that day. Any other discussions should be deferred to smaller, focused meetings involving only relevant members.\nTips: Keep it under 15 minutes. Limit reporting to essential information. Defer in-depth discussions to separate, focused sessions. Sprint Review The Sprint Review aims to gather feedback from stakeholders to guide future planning. Prepare for the session by clarifying what feedback is needed and from whom.\nTips: Prepare in advance to showcase progress effectively. Clearly define what feedback you want and why. Product Backlog Refinement Product Backlog Refinement should ideally take up no more than 10% of the sprint duration. Avoid delving too deeply into items that are not imminent, and focus instead on high-priority tasks.\nTips: Avoid excessive detail on future backlog items; focus on what\u0026rsquo;s next. Use relative estimation techniques and avoid overly precise estimates, acknowledging inherent uncertainty. Summary This article discussed methods to conduct Scrum ceremonies more efficiently, ultimately improving team performance. By enforcing timeboxes, aligning discussions with ceremony objectives, and focusing on value-driven conversations, teams can optimize their workflows and deliver better results.\n","date":"2024-11-18T09:36:05+09:00","permalink":"https://bossagyu.com/en/blog/028-scrum-ceremony/","title":"Improving Team Performance by Revisiting Scrum Ceremonies"},{"content":"Overview This article introduces a simple way to read Parquet files on macOS via the command line.\nReading with parquet-cli We will use the Parquet file provided as a sample by Feast.\nInstall parquet-cli on macOS using Homebrew.\n1 brew install parquet-cli Check the meta information.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ parquet meta driver_stats.parquet File path: driver_stats.parquet Created by: parquet-cpp-arrow version 18.1.0 Properties: pandas: {\u0026#34;index_columns\u0026#34;: [{\u0026#34;kind\u0026#34;: \u0026#34;range\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;start\u0026#34;: 0, \u0026#34;stop\u0026#34;: 1807, \u0026#34;step\u0026#34;: 1}], \u0026#34;column_indexes\u0026#34;: [{\u0026#34;name\u0026#34;: null, \u0026#34;field_name\u0026#34;: null, \u0026#34;pandas_type\u0026#34;: \u0026#34;unicode\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;metadata\u0026#34;: {\u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;}}], \u0026#34;columns\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;event_timestamp\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;event_timestamp\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;datetimetz\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;datetime64[ns]\u0026#34;, \u0026#34;metadata\u0026#34;: {\u0026#34;timezone\u0026#34;: \u0026#34;UTC\u0026#34;}}, {\u0026#34;name\u0026#34;: \u0026#34;driver_id\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;driver_id\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;int64\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;int64\u0026#34;, \u0026#34;metadata\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;conv_rate\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;conv_rate\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;metadata\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;acc_rate\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;acc_rate\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;metadata\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;avg_daily_trips\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;avg_daily_trips\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;int32\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;int32\u0026#34;, \u0026#34;metadata\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;datetime\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;datetime64[us]\u0026#34;, \u0026#34;metadata\u0026#34;: null}], \u0026#34;creator\u0026#34;: {\u0026#34;library\u0026#34;: \u0026#34;pyarrow\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;18.1.0\u0026#34;}, \u0026#34;pandas_version\u0026#34;: \u0026#34;2.2.3\u0026#34;} ARROW:schema: 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 Schema: message schema { optional int64 event_timestamp (TIMESTAMP(NANOS,true)); optional int64 driver_id; optional float conv_rate; optional float acc_rate; optional int32 avg_daily_trips; optional int64 created (TIMESTAMP(MICROS,false)); } Row group 0: count: 1807 16.88 B records start: 4 total(compressed): 29.796 kB total(uncompressed):29.760 kB -------------------------------------------------------------------------------- type encodings count avg size nulls min / max event_timestamp INT64 S _ R 1807 2.78 B 0 \u0026#34;2021-04-12T07:00:00.00000...\u0026#34; / \u0026#34;2024-12-28T14:00:00.00000...\u0026#34; driver_id INT64 S _ R 1807 0.07 B 0 \u0026#34;1001\u0026#34; / \u0026#34;1005\u0026#34; conv_rate FLOAT S _ R 1807 5.42 B 0 \u0026#34;1.9221554E-4\u0026#34; / \u0026#34;0.9998668\u0026#34; acc_rate FLOAT S _ R 1807 5.42 B 0 \u0026#34;2.1329636E-4\u0026#34; / \u0026#34;0.99993944\u0026#34; avg_daily_trips INT32 S _ R 1807 3.14 B 0 \u0026#34;0\u0026#34; / \u0026#34;999\u0026#34; created INT64 S _ R 1807 0.05 B 0 \u0026#34;2024-12-28T15:20:28.266000\u0026#34; / \u0026#34;2024-12-28T15:20:28.266000\u0026#34; View the data using head.\n1 2 3 4 5 6 7 8 9 10 11 12 $ parquet head driver_stats.parquet {\u0026#34;event_timestamp\u0026#34;: 1734102000000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.27734742, \u0026#34;acc_rate\u0026#34;: 0.7152132, \u0026#34;avg_daily_trips\u0026#34;: 823, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734105600000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.57354224, \u0026#34;acc_rate\u0026#34;: 0.9831811, \u0026#34;avg_daily_trips\u0026#34;: 851, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734109200000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.3287562, \u0026#34;acc_rate\u0026#34;: 0.6172164, \u0026#34;avg_daily_trips\u0026#34;: 116, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734112800000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.045716193, \u0026#34;acc_rate\u0026#34;: 0.032996926, \u0026#34;avg_daily_trips\u0026#34;: 741, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734116400000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.12863782, \u0026#34;acc_rate\u0026#34;: 0.8951942, \u0026#34;avg_daily_trips\u0026#34;: 534, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734120000000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.9555806, \u0026#34;acc_rate\u0026#34;: 0.62216556, \u0026#34;avg_daily_trips\u0026#34;: 216, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734123600000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.75297666, \u0026#34;acc_rate\u0026#34;: 0.37602386, \u0026#34;avg_daily_trips\u0026#34;: 954, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734127200000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.46957988, \u0026#34;acc_rate\u0026#34;: 0.6454945, \u0026#34;avg_daily_trips\u0026#34;: 360, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734130800000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.6702387, \u0026#34;acc_rate\u0026#34;: 0.36532214, \u0026#34;avg_daily_trips\u0026#34;: 396, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734134400000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.019627139, \u0026#34;acc_rate\u0026#34;: 0.528229, \u0026#34;avg_daily_trips\u0026#34;: 833, \u0026#34;created\u0026#34;: 1735399228266000} Check the schema.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 $ parquet schema driver_stats.parquet { \u0026#34;type\u0026#34; : \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;schema\u0026#34;, \u0026#34;fields\u0026#34; : [ { \u0026#34;name\u0026#34; : \u0026#34;event_timestamp\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, \u0026#34;long\u0026#34; ], \u0026#34;default\u0026#34; : null }, { \u0026#34;name\u0026#34; : \u0026#34;driver_id\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, \u0026#34;long\u0026#34; ], \u0026#34;default\u0026#34; : null }, { \u0026#34;name\u0026#34; : \u0026#34;conv_rate\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, \u0026#34;float\u0026#34; ], \u0026#34;default\u0026#34; : null }, { \u0026#34;name\u0026#34; : \u0026#34;acc_rate\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, \u0026#34;float\u0026#34; ], \u0026#34;default\u0026#34; : null }, { \u0026#34;name\u0026#34; : \u0026#34;avg_daily_trips\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, \u0026#34;int\u0026#34; ], \u0026#34;default\u0026#34; : null }, { \u0026#34;name\u0026#34; : \u0026#34;created\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, { \u0026#34;type\u0026#34; : \u0026#34;long\u0026#34;, \u0026#34;logicalType\u0026#34; : \u0026#34;local-timestamp-micros\u0026#34; } ], \u0026#34;default\u0026#34; : null } ] } Conclusion This article explained how to easily read Parquet files on macOS.\n","date":"2024-09-15T16:52:04+09:00","permalink":"https://bossagyu.com/en/blog/031-read-parquet-file/","title":"How to Read Parquet Files on macOS"},{"content":"Overview This guide introduces how to quickly set up a Go development environment on Mac and run a Hello World program.\nInstalling Go Install Go using Homebrew.\n1 \u0026gt; brew install go Check the installed version.\n1 2 \u0026gt; go version go version go1.21.3 darwin/arm64 Running Hello World Save the following code as main.go.\n1 2 3 4 5 6 7 package main import \u0026#34;fmt\u0026#34; func main() { fmt.Printf(\u0026#34;Hello World\\n\u0026#34;) } Run the code.\n1 2 \u0026gt; go run hello.go Hello World Build the binary and execute it.\n1 2 3 4 5 6 7 \u0026gt; go build hello.go \u0026gt; ls hello* hello.go \u0026gt; ./hello Hello World Conclusion That\u0026rsquo;s how you can set up a Go development environment on Mac and quickly run a Hello World program.\n","date":"2024-09-15T16:52:04+09:00","permalink":"https://bossagyu.com/en/blog/030-go-environment-construction/","title":"Setting Up a Go Development Environment on Mac and Running Hello World"},{"content":"What is gRPC? gRPC is a protocol developed by Google to implement RPC. It uses Protocol Buffers to serialize data, enabling high-speed communication. API specifications are pre-defined in a .proto file using IDL, from which source code for both the server and client sides is generated. Differences between REST and gRPC\nREST is resource-oriented, whereas RPC focuses on method invocation, with data being a byproduct. Advantages and Disadvantages Advantages High performance with HTTP/2 Data transfer via Protocol Buffers Development follows a schema-first approach due to the use of IDL Flexible streaming modes Disadvantages Non-compliance with HTTP/2 Limited browser support Variability in feature implementation depending on the programming language Serialized binary data is not human-readable REST is also sufficiently fast .proto Files gRPC uses Protocol Buffers as its serialization format.\nSchema definitions are made in files with the .proto extension, and code for various languages is generated using the protoc command.\nIn Protocol Buffers, all values have types, which can be divided into scalar types and message types.\nScalar Types Numeric, string, boolean, byte array Message Types Message types with multiple fields Multiple message types can be defined in a single .proto file 1 2 3 4 5 message Person { int32 id = 1; string name = 2; string email = 3; } Conducting the gRPC Quick Start This time, we will perform the gRPC Quick Start using a Python environment.\nhttps://grpc.io/docs/languages/python/quickstart/\nSet up the necessary Python environment.\n1 2 python -m pip install grpcio python -m pip install grpcio-tools Download the sample code.\n1 2 git clone -b v1.64.0 --depth 1 --shallow-submodules https://github.com/grpc/grpc cd grpc/examples/python/helloworld Start the server.\n1 2 3 4 python greeter_server.py # Output Server started, listening on 50051 Open another terminal and start the client.\n1 2 3 4 5 python greeter_client.py ## Response Will try to greet world ... Greeter client received: Hello, you! Communication between the gRPC client and server was successfully established.\nModifying the .proto File This time, we will modify the helloworld.proto file to add a new method.\nMove to the directory containing the helloworld.proto file.\n1 cd grpc/examples/protos Modify the file as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 syntax = \u0026#34;proto3\u0026#34;; option java_multiple_files = true; option java_package = \u0026#34;io.grpc.examples.helloworld\u0026#34;; option java_outer_classname = \u0026#34;HelloWorldProto\u0026#34;; option objc_class_prefix = \u0026#34;HLW\u0026#34;; package helloworld; // The greeting service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} // Add the following line rpc SayHelloAgain (HelloRequest) returns (HelloReply) {} rpc SayHelloStreamReply (HelloRequest) returns (stream HelloReply) {} rpc SayHelloBidiStream (stream HelloRequest) returns (stream HelloReply) {} } // The request message containing the user\u0026#39;s name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } Generate the gRPC code.\n1 2 3 cd examples/python/helloworld python -m grpc_tools.protoc -I../../protos --python_out=. --pyi_out=. --grpc_python_out=. ../../protos/helloworld.proto The following files are regenerated.\n1 2 3 4 5 ls -l -rw-r--r--@ 1 xx xx 1823 9 1 18:12 helloworld_pb2.py -rw-r--r--@ 1 xx xx 578 9 1 18:12 helloworld_pb2.pyi -rw-r--r--@ 1 xx xx 7018 9 1 18:12 helloworld_pb2_grpc.py The _pd file that is updated contains the auto-generated Protocol Buffers definition classes and is generally not manually modified.\nUpdate the greeter_server.py file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from concurrent import futures import logging import grpc import helloworld_pb2 import helloworld_pb2_grpc class Greeter(helloworld_pb2_grpc.GreeterServicer): def SayHello(self, request, context): return helloworld_pb2.HelloReply(message=\u0026#34;Hello, %s!\u0026#34; % request.name) # Add the following function def SayHelloAgain(self, request, context): return helloworld_pb2.HelloReply(message=\u0026#34;Hello Again, %s!\u0026#34; % request.name) def serve(): port = \u0026#34;50051\u0026#34; server = grpc.server(futures.ThreadPoolExecutor(max_workers=10)) helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(), server) server.add_insecure_port(\u0026#34;[::]:\u0026#34; + port) server.start() print(\u0026#34;Server started, listening on \u0026#34; + port) server.wait_for_termination() if __name__ == \u0026#34;__main__\u0026#34;: logging.basicConfig() serve() Update the greeter_client.py file.\n1 2 3 4 5 6 7 8 9 10 11 def run(): # NOTE(gRPC Python Team): .close() is possible on a channel and should be # used in circumstances in which the with statement does not fit the needs # of the code. print(\u0026#34;Will try to greet world ...\u0026#34;) with grpc.insecure_channel(\u0026#34;localhost:50051\u0026#34;) as channel: stub = helloworld_pb2_grpc.GreeterStub(channel) response = stub.SayHello(helloworld_pb2.HelloRequest(name=\u0026#34;you\u0026#34;)) print(\u0026#34;Greeter client received: \u0026#34; + response.message) response = stub.SayHelloAgain(helloworld_pb2.HelloRequest(name=\u0026#34;you\u0026#34;)) print(\u0026#34;Greeter client received: \u0026#34; + response.message) Restart the server and run the client.\n1 2 3 4 5 6 python greeter_server.py python greeter_client.py # Output Greeter client received: Hello, you! Greeter client received: Hello Again, you! The newly added method is confirmed to be functioning correctly.\nSummary This time, we researched gRPC and followed the official documentation tutorial.\ngRPC allows for schema-first development and enables high-performance communication via HTTP/2. Recently, it has been gaining attention as an alternative to REST APIs, making it a technology worth familiarizing yourself with.\n","date":"2024-09-01T17:53:57+09:00","permalink":"https://bossagyu.com/en/blog/029-grpc/","title":"Research on gRPC"},{"content":"Introduction of GPT-4o On May 13, 2024, OpenAI announced a new GPT model, ChatGPT-4o.\nGPT-4o has the following improvements compared to previous models:\nRealization of natural conversation Faster response time Enhanced multilingual support Higher reliability Notably, the improvements in inference speed and quality allow it to perform exceptionally well in real-time dialogue systems, enabling smoother and more natural communication.\nReal-Time Dialogue Applications The following video demonstrates real-time interaction with ChatGPT-4o using a smartphone.\nAs you can see in the video, ChatGPT-4o can respond at a level close to human conversation.\nThis allows users to enjoy very natural conversations.\nPreviously, the ChatGPT series converted voice to text and then inputted it into GPT-4 for dialogue.\nThis meant that emotional information such as tone was lost in the process.\nHowever, ChatGPT-4o is trained directly from voice, taking into account information such as tone, leading to more natural conversations.\nAdditionally, it returns responses directly from voice, eliminating the text interpretation step and allowing for faster responses.\nEnhanced Multilingual Support GPT-4o also enhances multilingual support.\nPreviously, questions asked in English received high-accuracy responses, but questions in Japanese resulted in lower accuracy.\nWith the enhanced multilingual support, high-accuracy responses can now be obtained even in Japanese.\nSummary This time, we introduced ChatGPT-4o, announced by OpenAI.\nThe accuracy of the ChatGPT model has remarkably improved, feeling more and more like human conversation.\nI believe that how well engineers can utilize generative AI will greatly affect productivity, so I will continue to monitor trends closely.\nRelated Information Hello ChatGPT-4o ","date":"2024-05-14T23:22:39+09:00","permalink":"https://bossagyu.com/en/blog/027-chatgpt-4o/","title":"Introduction to ChatGPT 4o"},{"content":"Overview As of April 26, 2024, Evernote\u0026rsquo;s Japanese corporation was dissolved, which does not mean Evernote services are ending, but it certainly raises concerns about its future. Given the many restrictions on Evernote\u0026rsquo;s free plan and the cost associated with its premium plan, I considered switching to another note-taking app.\nAfter some deliberation, I chose Notion for the following reasons:\nIt possesses all essential features required for a note-taking app and is a viable alternative to Evernote. Notion\u0026rsquo;s free plan is less restrictive and adequate for my needs, allowing me to cut costs on premium subscriptions. Notion offers an import tool specifically for Evernote, facilitating a low-cost migration. This article details the process of migrating from Evernote to Notion.\nMigration Process Notion provides an Evernote import feature, making the migration straightforward:\nAccessing Import Settings Navigate to Settings in the Notion app.\nClick Settings to access the Import option.\nAfter clicking on Import, select Evernote as the import source.\nImporting Notebooks Once linked, you can choose the notebooks you wish to import. Here\u0026rsquo;s a critical tip: While it seems possible to select multiple notebooks for import, doing so may lead to excessively long processing times or even errors. Therefore, it is advisable to import one notebook at a time.\nEven when importing one notebook at a time, be prepared for the process to take several hours, depending on the size of the notebook.\nPost-Import Review After the import, the formatting, images, links, and labels were preserved well, allowing for a seamless transition to Notion.\nConclusion This guide outlines the steps to migrate from Evernote to Notion. With Notion\u0026rsquo;s robust free features and straightforward migration process, it\u0026rsquo;s worth considering if you\u0026rsquo;re looking to move away from Evernote.\n","date":"2024-04-29T19:32:38+09:00","permalink":"https://bossagyu.com/en/blog/026-evernote-to-notion/","title":"Migrating from Evernote to Notion"},{"content":"Overview Adding a .gitignore file to a project allows you to exclude certain files and directories from being tracked by git. However, it can be tedious to repeatedly add default directories like .idea generated by IDEs to .gitignore for each new project. This article explains how to apply .gitignore settings globally across all projects.\nApplying gitignore Globally Git by default checks for ignore settings in ~/.config/git/ignore. Therefore, by placing your ignore configurations in this file, you can have them applied to all your projects.\nWhile it\u0026rsquo;s common to create a .gitignore_global and register it under core.excludesfile, this method requires unnecessary configuration in .gitconfig, which is why the approach described here is recommended.\nSteps to Apply gitignore Globally Create a Directory for Ignore File Create a directory to store your ignore file, if it doesn\u0026rsquo;t already exist:\n1 mkdir -p ~/.config/git/ Create and Configure the Ignore File Create the ignore file and write the patterns for files and directories you want to ignore globally:\n1 vim ~/.config/git/ignore Example of what to include:\n1 2 3 .idea/ *.log node_modules/ Applying the Ignore Settings These settings will now automatically apply to all projects managed under your user account. If there are already tracked files you wish to ignore, use git rm --cached to stop tracking them without deleting the files.\nReference Git - gitignore By following these steps, you can efficiently manage your ignore settings across all your git projects without having to replicate configurations in each project repository.\n","date":"2024-04-16T23:16:25+09:00","permalink":"https://bossagyu.com/en/blog/025-git-ignore/","title":"Setting Up a Global gitignore for All Projects"},{"content":"Overview Bluesky is a decentralized social network initiated by Jack Dorsey, former CEO of Twitter.\nBuilt on the ATProtocol, it can be seen as a decentralized version of Twitter, where there is no central authority. The move towards decentralization in social networking might be following the trend seen in finance, from centralized to decentralized cryptocurrencies.\nThis article compiles methods for executing Bluesky\u0026rsquo;s API using Python.\nSteps to Use Bluesky API Generating an API Password Setting Up Python Environment Script Creation and Execution Generating an API Password To execute the API, you need to generate an API password using your account name.\nFirst, verify your account name displayed on your Bluesky profile, minus the initial @. For instance, if my account is bossagyu.bsky.social, that\u0026rsquo;s the account name I\u0026rsquo;ll use.\nNext, generate the API execution password.\nNavigate to Settings → App Passwords to create the password.\nClick Add App Password.\nAfter clicking the add button, name your password for management purposes. The name itself is not the password but helps with organization.\nOnce the password is generated, ensure to copy it as it won\u0026rsquo;t be displayed again. If you forget to copy, simply generate a new one.\nSetting Up the Python Environment Prepare your Python environment. For setup using venv, refer to this guide.\nAccording to the official documentation, Python version 3.7.1 or higher is required.\nAfter setting up your environment, install the ATProtocol library to interact with it.\n1 $ pip install atproto Verify installation:\n1 2 3 $ pip list | grep atproto atproto 0.0.46 With this, preparation is complete.\nScript Creation and Execution Create a script to post on Bluesky.\n1 2 3 4 5 6 7 8 9 from atproto import Client client = Client() user_name = \u0026#34;bossagyu.bsky.social\u0026#34; password = \u0026#34;*******\u0026#34; # Enter the generated API password client.login(user_name, password) client.send_post(text=\u0026#39;Posting via API.\u0026#39;) That\u0026rsquo;s all for the script. You can now post to Bluesky using the API.\nLet\u0026rsquo;s run it:\n1 $ python post_bluesky.py Upon execution, the post successfully appears on Bluesky.\nConclusion This article covered how to execute Bluesky\u0026rsquo;s API using Python.\nWhile Bluesky is still under development, unlike Twitter, which has limitations and charges for API use, Bluesky offers a free API, making it an excellent option for those looking to experiment with social networking APIs at no cost.\n","date":"2024-04-07T23:52:09+09:00","permalink":"https://bossagyu.com/en/blog/024-bluesky-api/","title":"Using Bluesky API for Automated Posting with Python"},{"content":"Overview Unlike image generation models like Stable Diffusion, ChatGPT can also generate images. This guide explains how to do so.\nFor those using the ChatGPT paid plan, images can be generated without any additional fees, allowing for the creation of commercially usable images with minimal effort.\nThis tutorial utilizes DALL-E, a feature of ChatGPT Plus, to generate images. For more information on DALL-E3, refer to OpenAI\u0026rsquo;s official page.\nHow to Generate Images Select \u0026ldquo;Explore GPTs\u0026rdquo; from the sidebar.\nType DALL-E in the search bar and perform a search.\nClick on \u0026ldquo;Start Chat\u0026rdquo; to begin generating images.\nThen, simply enter a description of the image you wish to generate, and the image will be created.\nGenerating an Example Image Let\u0026rsquo;s try generating an image of a dog wearing pink sunglasses, as used in this blog.\nFor a start, we\u0026rsquo;ll input the prompt \u0026ldquo;a dog wearing pink sunglasses\u0026rdquo;.\nAdditionally, like using regular ChatGPT, you can refine the generated image by entering additional prompts.\nThis time, let\u0026rsquo;s input \u0026ldquo;Make it anime-style\u0026rdquo; as an additional prompt.\nYou can see it has an anime-style appearance. By issuing further orders, you can gradually get closer to your desired image.\nConclusion This guide has explained how to generate images using ChatGPT.\nIt\u0026rsquo;s incredibly convenient to be able to generate images easily, but just like with Stable Diffusion, it\u0026rsquo;s quite challenging to get the exact image you want without adjusting the prompts. The need for prompt refinement seems less critical than with Stable Diffusion, suggesting a possible difference in model performance.\nNote that if you overuse this feature, you may receive a message asking you to wait, indicating there may be a limit on the number of generations. For those with access to a machine source, running Stable Diffusion locally on a Mac might still be the best option.\n","date":"2024-03-31T17:35:07+09:00","permalink":"https://bossagyu.com/en/blog/023-chatgpt-create-image/","title":"Generating Images with ChatGPT"},{"content":"Overview This article explains how to use Enums in TypeScript.\nWhat are Enums? Enums (enumerated types) represent a collection of related values. While many languages implement Enums, JavaScript does not. However, TypeScript supports Enums, enriching the JavaScript experience.\nHow to Use Enums Enums can be defined as follows:\n1 2 3 4 5 6 7 enum Status { zero, one, two } console.log(Status.zero); // 0 By default, Enums are assigned numerical values, starting from 0. The compiled JavaScript code would look like this:\n1 2 3 4 5 6 7 var Status; (function (Status) { Status[Status[\u0026#34;zero\u0026#34;] = 0] = \u0026#34;zero\u0026#34;; Status[Status[\u0026#34;one\u0026#34;] = 1] = \u0026#34;one\u0026#34;; Status[Status[\u0026#34;two\u0026#34;] = 2] = \u0026#34;two\u0026#34;; })(Status || (Status = {})); console.log(Status.zero); // 0 You can also assign string values to Enums:\n1 2 3 4 5 6 7 enum Status { zero = \u0026#39;zero\u0026#39;, one = \u0026#39;one\u0026#39;, two = \u0026#39;two\u0026#39; } console.log(Status.zero); // \u0026#39;zero\u0026#39; When comparing string values, you can write as follows:\n1 2 3 4 5 6 7 8 const stringZero: String = \u0026#39;zero\u0026#39;; const value = stringZero as StringStatus; if (value === StringStatus.zero) { console.log(\u0026#39;value is zero\u0026#39;); } else { console.log(\u0026#39;value is not zero\u0026#39;); } Summary This article explained how to use Enums in TypeScript. By utilizing Enums, you can improve the readability and maintainability of your code.\n","date":"2024-03-23T13:11:13+09:00","permalink":"https://bossagyu.com/en/blog/022-typescript-enum/","title":"Using Enums in TypeScript"},{"content":"Overview This article explains how to easily set up a TypeScript development environment using Volta, with MacOS as the target platform.\nWhat is Volta? Volta is a version management tool for Node.js, featuring the following characteristics as presented on Volta\u0026rsquo;s official site:\nFast: Built in Rust, enabling swift Node.js version switching. Reliable: Ensures everyone on a project uses the same tools. Universal: Can be used across package managers, node runtimes, and OSes. While nodebrew has been commonly used, the use of Volta seems to be increasing lately.\nInstalling Volta and Node.js Installing Volta is as simple as running the following command:\n1 curl https://get.volta.sh | bash If you\u0026rsquo;re using zsh and the path hasn\u0026rsquo;t been automatically set, use the following commands:\n1 2 3 echo \u0026#39;VOLTA_HOME=$HOME/.volta\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo \u0026#39;export PATH=$VOLTA_HOME/bin:$PATH\u0026#39; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc Verify the installation. If a version is displayed, Volta has been successfully installed:\n1 volta -v Next, use Volta to install Node.js. If no version is specified, the latest LTS version will be installed:\n1 volta install node Installing yarn and Creating a TypeScript Project Differences between npm and yarn Both npm and yarn are package managers for Node.js. Their features include:\nnpm:\nReleased a year after Node.js (2010). Stands for Node Package Manager. Automatically generates a package-lock.json file. Installed automatically with Node.js. yarn:\nReleased in 2016. Developed by Facebook, Google, Exponent, and Tilde. Compatible with npm (can use the same package.json). More strict in locking module versions than npm. Faster installation than npm. While yarn appears superior, recent updates to npm have minimized the differences. Here, we will use yarn to create a TypeScript project.\nInstalling yarn Install yarn using Volta:\n1 volta install yarn Confirm the installation by checking if yarn is listed:\n1 volta list Creating a TypeScript Project Initialize yarn:\n1 yarn init -y Install Node.js:\n1 volta pin node@20.0.0 Install TypeScript:\n1 yarn add typescript Install ts-node:\n1 yarn add --dev ts-node Create a tsconfig.json file. This is a TypeScript configuration file that details compilation settings. In this case, you can leave it at the default settings generated:\n1 yarn tsc --init Test a sample program:\n1 2 3 4 echo \u0026#34;console.log(\u0026#39;Hello, TypeScript!\u0026#39;);\u0026#34; \u0026gt; hello.ts yarn ts-node hello.ts # If \u0026#34;Hello, TypeScript!\u0026#34; is displayed, it\u0026#39;s a success. The test script ran successfully. This completes the setup of the TypeScript development environment.\nSummary This article described how to set up a TypeScript development environment easily using Volta. By using Volta, managing Node.js versions becomes straightforward, making the development environment setup smoother. Additionally, specifying the Node.js version with Volta in your project\u0026rsquo;s package.json can help eliminate version discrepancies among developers, adding another layer of convenience to your workflow.\n","date":"2024-03-10T13:11:13+09:00","permalink":"https://bossagyu.com/en/blog/021-typescript-setup/","title":"Setting Up TypeScript Development Environment Easily with Volta"},{"content":"Overview This article explains capacity and performance management as outlined in ITIL v4. I will also apply what I have understood to my own experiences, explaining the process of capacity and performance management.\nWhat are Capacity and Performance Management? Capacity and Performance Management involves managing the performance of services and the resources supporting them. The goal is to optimize the performance of services and ensure that the capacity of services is appropriately maintained.\nProcesses in Capacity and Performance Management The processes in capacity and performance management include:\nEstablishing Capacity and Performance Control Analyzing and Improving Service Capacity and Performance Establishing Capacity and Performance Control Establishing capacity and performance control involves agreeing with stakeholders on the usage and performance standards for the IT resources used by the service, and deciding on the timing, baseline, and format for evaluation.\nIt is realized through the following steps:\nIdentifying service capacity and performance requirements Agreeing on service capacity and performance requirements Deciding on capacity and performance requirements Designing capacity and performance evaluation metrics and reports Applying my experiences to these processes, I understood them as follows:\nIdentifying service capacity and performance requirements As an internal platform provider, I identified latency performance requirements (99%ile in Nms) demanded by internal users. Based on these thresholds, we conducted performance verification and measured throughput per instance. We calculated the cost based on the throughput measurements. Agreeing on service capacity and performance requirements We agreed on latency performance and throughput with stakeholders. Deciding on capacity and performance requirements This remained unchanged from what was agreed upon. Designing capacity and performance evaluation metrics and reports We used a tracing tool called Dynatrace to measure and report performance. Analyzing and Improving Service Capacity and Performance Analyze issues in usage and performance conditions from service output logs and incident information.\nIt is realized through the following steps:\nAnalyzing capacity and performance Reporting capacity and performance Planning and designing capacity and performance Applying my experiences to these activities, I understood them as follows:\nAnalyzing capacity and performance We used Dynatrace to analyze latency performance and throughput. We identified performance issues from incident information. Reporting capacity and performance We visualized latency performance and throughput using Dynatrace\u0026rsquo;s dashboard. Planning and designing capacity and performance Anticipating growth in users, we forecasted that the current capacity would be insufficient. We made plans to increase capacity based on demand forecasts and executed them. Summary In this article, I explained capacity and performance management based on what I have learned and applied it to my own experiences. I understood that capacity and performance management involves managing aspects similar to availability management, but from the perspective of capacity and performance.\nIn my experience, discussions about performance and availability often go hand in hand, and I rarely deal with them independently.\n","date":"2024-02-27T08:53:36+09:00","permalink":"https://bossagyu.com/en/blog/020-itilv4-capacity-and-performance-management/","title":"Explaining Capacity and Performance Management in ITIL v4"},{"content":"Overview This article will guide you through installing Stable Diffusion Web UI on Mac and using it locally.\nWhat is Stable Diffusion? Stable Diffusion is a type of image processing technology using AI. By inputting text, it can generate images corresponding to that text.\nWays to Use Stable Diffusion There are two main ways to use Stable Diffusion:\nUsing web applications like Hugging Face or Dream Studio Using Stable Diffusion Web UI locally This article focuses on using Stable Diffusion Web UI locally. While web applications are easy for trial purposes, they might have limitations or costs for generating a significant number of images. Thus, local usage is recommended for more extensive needs.\nHow to Use Stable Diffusion Web UI Locally We will use stable-diffusion-web-ui published by AUTOMATIC1111 for this purpose.\nPrepare the environment Install stable-diffusion-web-ui Place the model files Start stable-diffusion-web-ui and generate images 1. Preparing the Environment First, we need to install Python and other necessary libraries using homebrew.\nInstall homebrew:\n1 /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; Set the homebrew path:\n1 export PATH=\u0026#34;$PATH:/opt/homebrew/bin/\u0026#34; Install related libraries:\n1 brew install cmake protobuf rust pyenv git wget Set up the Python environment using pyenv. This allows you to use multiple versions of Python. For building the Python environment, refer to this article.\n1 2 pyenv install 3.10.6 pyenv local 3.10.6 2. Installing stable-diffusion-web-ui Clone the repository using git clone:\n1 2 git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git cd stable-diffusion-webui Set up a virtual environment using venv to keep the environment clean:\n1 2 python -m venv venv source venv/bin/activate Now the environment is ready.\n3. Placing the Model Files Next, download the model files and place them in the stable-diffusion-webui/models/Stable-diffusion/ directory. Model files can be downloaded from sites like:\nCivitai Hugging Face For this example, we\u0026rsquo;ll download the blue_pencil model from Civitai.\nSearch for blue_pencil in Civitai\u0026rsquo;s search bar, select blue_pencil from the search results, and click the Download button.\nMove the downloaded model to the directory:\n1 mv ~/Downloads/bluePencilXL_v401.safetensors models/Stable-diffusion/ 4. Starting stable-diffusion-web-ui and Generating Images Finally, start stable-diffusion-web-ui and generate images:\n1 ./webui.sh After launching, input the text in the prompt and generate images:\nSelect the downloaded blue_pencil model in Stable Diffusion checkpoint. Enter elements you want in the image in the prompt. Enter elements you don\u0026rsquo;t want in the image in the Negative prompt. Click Generate. By inputting \u0026ldquo;a dog wearing pink sunglasses,\u0026rdquo; the output matched the input. You can generate various images by inputting different texts.\nSummary This article introduced how to install and use Stable Diffusion Web UI on Mac locally. Using it locally allows you to generate images freely without the limitations or costs associated with web applications.\n","date":"2024-02-12T11:24:59+09:00","permalink":"https://bossagyu.com/en/blog/019-stable-diffusion/","title":"How to Use Stable Diffusion Web UI on Mac"},{"content":"Overview This article consolidates my learning and understanding of business analysis as per ITIL v4. I will also apply my own experiences to explain the business analysis process.\nWhat is Business Analysis? Business analysis involves analyzing a business or any other component to propose solutions for meeting those needs or solving business challenges. It is important to note that the term \u0026ldquo;business\u0026rdquo; analysis does not solely pertain to business entities.\nExamples of Business Analysis Techniques Examples of business analysis techniques include:\nSWOT Analysis User Stories For specific methods, I defer to other sites as they are not the main focus here. Personally, I often use Customer Journey Maps, although it is not listed as an example here.\nThe Business Analysis Process The business analysis process includes two main processes:\nDesigning and Maintaining Business Analysis Approaches Identifying Business Analysis and Solutions Designing and Maintaining Business Analysis Approaches This process focuses on establishing a consistent and effective approach to business analysis by addressing the current and anticipated needs of the organization. It is executed as follows:\nAnalyzing the organization and requirements Reviewing business analysis approach methodologies Implementing the business analysis approach In my organization, we rarely conduct analysis at the business layer level. Generally, this process involves analyzing specific requirements using certain methodologies.\nIdentifying Business Analysis and Solutions This process emphasizes analyzing stakeholders\u0026rsquo; needs and requirements. It includes identifying and proposing solutions to address the stakeholders\u0026rsquo; needs and requirements. It is executed as follows:\nCollecting and analyzing information from stakeholders Defining solution options and identifying recommended solutions Providing support to the solution delivery team Evaluating and assessing the performance of the solution Applying my own experiences to these activities, I understood them as follows:\nFor 1,2 Collecting information from stakeholders and conducting analysis. Identifying the issues to be solved (why) from the analysis results and determining what solutions are available (what). For 3 Considering how to solve the identified why and what with the product team. Deciding on the resolution method and evaluation criteria to determine how success will be measured. For 4 Regularly evaluating how well the solution meets the established criteria. I recommend automating data collection and visualizing it with tools like Grafana for visibility. Summary This article explained business analysis based on my learning and experiences. Personally, I\u0026rsquo;ve understood that business analysis does not only target businesses and includes processes beyond analysis.\nReference Business analysis management: ITIL 4 Practice Guide ","date":"2024-02-09T09:00:56+09:00","permalink":"https://bossagyu.com/en/blog/018-itilv4-business-analysis/","title":"Explaining Business Analysis in ITIL v4"},{"content":"Overview This article explains how to set up and use GitHub Copilot in VSCode. A GitHub Copilot account is required as a prerequisite.\nGetting Started with GitHub Copilot in VSCode Installing the Extension First, you need to install the extension in VSCode. Open VSCode, click on the icon with four squares in the left menu, enter \u0026ldquo;copilot\u0026rdquo; in the search text input, and click \u0026ldquo;install\u0026rdquo; to start the installation.\nLinking with GitHub After clicking install and completing the installation, the following screen will appear. Click on \u0026ldquo;Sign in to GitHub.\u0026rdquo;\nYou will be asked to allow access to your GitHub account. Click \u0026ldquo;Allow\u0026rdquo; to grant permission.\nClick \u0026ldquo;Authorize Visual Studio Code\u0026rdquo; to give permission.\nThis completes the integration of GitHub Copilot with VSCode, and you\u0026rsquo;re now ready to use it.\nHow to Use Basically, as you write code, suggestions will automatically appear. Use the following commands to write code efficiently with the suggested completions.\nCheat Sheet Function Key Accept suggestion Tab Reject suggestion Esc Open Copilot Ctrl + Enter Next suggestion Alt/Option + ] Previous suggestion Alt/Option + [ Trigger inline Copilot Alt/Option + \\ Summary This article covered how to set up and use GitHub Copilot in VSCode.\nGitHub Copilot not only provides code completions but also assists with writing text.\nThis blog was also written with the assistance of GitHub Copilot, significantly improving efficiency.\nGive it a try and see how it can help you.\n","date":"2024-02-04T22:34:51+09:00","permalink":"https://bossagyu.com/en/blog/017-vscode-copilot/","title":"Setting Up and Using GitHub Copilot in VSCode"},{"content":"Overview This article explains availability management as defined in ITIL v4. I will also relate my own experiences to explain the processes involved in availability management.\nWhat is Availability Management? Availability management involves activities to ensure the availability of a service. The objective of availability management is to ensure that the service delivers an agreed level of availability to meet the needs of customers and users.\nProcesses in Availability Management There are two main processes in availability management:\nEstablishing Service Availability Control Analyzing and Improving Service Availability Establishing Service Availability Control Establishing service availability control ensures the availability of a service. It is realized through the following steps:\nIdentifying service availability requirements Agreeing on service availability requirements Deciding on availability measurement requirements Designing availability metrics and reporting Applying my own experiences to these processes, I understood them as follows:\nIdentifying service availability requirements Identifying the types of users and the business risks of service downtime. For my service, which is an internal platform, I identified the impacts on various services using the platform. Agreeing on service availability requirements Agreeing on the service\u0026rsquo;s availability (e.g., 99% uptime) in the form of an SLA. We also clarified downtime criteria and exceptions for uptime calculations. Deciding on availability measurement requirements As the availability requirements were agreed upon earlier, there were no specific measurement requirements. Designing availability metrics and reporting Primarily designed around the \u0026lsquo;downtime/uptime\u0026rsquo; formula. For reporting, we created a dashboard to visualize the availability metrics. Analyzing and Improving Service Availability This process, as the name suggests, involves analyzing and improving service availability. It is realized through the following steps:\nAnalyzing service availability Reporting on service availability Planning and designing for service availability Applying my own experiences to these processes, I understood them as follows:\nAnalyzing service availability Confirming that service availability is being achieved and compiling the data. Reporting on service availability Reflecting availability on a dashboard that is accessible to everyone. Planning and designing for service availability In case of incidents that affect availability, we formulated plans for prevention of recurrence. Reference Availability management: ITIL 4 Practice Guide ","date":"2024-01-30T20:34:58+09:00","permalink":"https://bossagyu.com/en/blog/016-itilv4-availability-management/","title":"Understanding Availability Management in ITIL v4"},{"content":"Overview This page explains how to check for the existence of an object in S3 using Python.\nMethod Using boto3 To check using boto3.resource, you can use the following code:\n1 2 3 4 5 6 7 8 9 10 s3 = boto3.resource(\u0026#39;s3\u0026#39;) try: s3.Object(\u0026#39;bucket_name\u0026#39;, \u0026#39;object_name\u0026#39;).load() print(\u0026#34;True\u0026#34;) except ClientError as e: error_code = e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] if error_code == \u0026#39;404\u0026#39;: print(\u0026#34;Object does not exist.\u0026#34;) else: print(f\u0026#34;An error occurred: {e}\u0026#34;) If you\u0026rsquo;re using boto3.client, you can check with this code:\n1 2 3 4 5 6 7 8 9 10 s3 = boto3.client(\u0026#39;s3\u0026#39;) try: s3.head_object(Bucket=\u0026#39;bucket_name\u0026#39;, Key=\u0026#39;object_name\u0026#39;) print(\u0026#34;True\u0026#34;) except ClientError as e: error_code = e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] if error_code == \u0026#39;404\u0026#39;: print(\u0026#34;Object does not exist.\u0026#34;) else: print(f\u0026#34;An error occurred: {e}\u0026#34;) Reference check if a file exists in s3 bucket using boto3 ","date":"2024-01-27T21:41:37+09:00","permalink":"https://bossagyu.com/en/blog/015-s3-object-check/","title":"Checking for the Existence of an Object in S3"},{"content":"Overview Integrating AWS API Gateway with Lambda enables you to call Lambda functions from API Gateway. This article introduces how to integrate AWS API Gateway with Lambda.\nPrerequisites It is assumed that the Lambda function has already been created. If not, please refer to the following article for creation:\nCreating an AWS Lambda function Deciding How to Integrate API Gateway with Lambda When integrating API Gateway with Lambda, you need to consider the following two points:\nThe request format for API Gateway Whether to use Proxy Integration or Non-Proxy Integration Request Format for API Gateway You can choose from the following formats:\nREST API HTTP API WebSocket API If you choose to use the REST API format, you will need to decide between REST API and HTTP API. While REST API has more features, it is more expensive than HTTP API. HTTP API is a good choice for simpler requirements.\nFor a detailed comparison, please refer to the official documentation.\nProxy vs. Non-Proxy Integration Using Proxy Integration standardizes the format of the response returned from Lambda. It is generally recommended to use Proxy Integration.\nSetting up Lambda Proxy Integrations in API Gateway Configuration After creating the Lambda function, select \u0026ldquo;Add Trigger.\u0026rdquo;\nChoose API Gateway.\nConfigure the trigger addition as shown below. Once configured successfully, the screen should look like this.\nAccess the API endpoint listed with a tool like curl to execute the Lambda function.\n1 2 3 $ curl https://xxxxxxxxx.execute-api.ap-northeast-1.amazonaws.com/default/apigateway-get-sample \u0026#34;Hello from Lambda!\u0026#34;% Summary This article introduced how to integrate AWS API Gateway with Lambda. By integrating with API Gateway, you can externally invoke Lambda functions at any time.\n","date":"2024-01-13T18:06:52+09:00","permalink":"https://bossagyu.com/en/blog/014-aws-apigateway-lambda/","title":"Integrating AWS API Gateway with Lambda"},{"content":"Overview This article summarizes how to create a good product strategy, based on the book \u0026ldquo;Good Strategy Bad Strategy.\u0026rdquo;\nBackground As a product owner in my job, I took over a product from the previous owner. The product lacked a clear strategy and direction, prompting me to develop a new product strategy.\nWhile developing the strategy, I realized that the term \u0026ldquo;strategy\u0026rdquo; is interpreted in various ways by different people, often used loosely in many contexts. To understand what constitutes a good strategy, I referred to \u0026ldquo;Good Strategy Bad Strategy.\u0026rdquo;\nWhat is a Good Strategy? A good strategy identifies critical points where concerted efforts can significantly boost the effect of one’s actions. A strategy should show the direction for an organization to move forward.\nA good strategy has three basic structures:\nDiagnosis Guiding Policy Actions Diagnosis Diagnosis involves assessing the situation to identify the key challenges to address. A good diagnosis separates crucial issues from the complex mix of problems and simplifies them.\nMost of the strategy work lies in figuring out what is happening. Gathering information is crucial. Although the book criticizes consultants\u0026rsquo; frameworks, I find them useful in organizing information after thorough collection.\nIn creating my product strategy, I used SWOT analysis and Impact Mapping to organize the current situation and the impact of existing strategies.\nFor more on these methods, refer to the following resources:\nSWOT Analysis Impact Mapping Guiding Policy The guiding policy outlines how to approach the challenges identified in the diagnosis. A good policy is not about goals or visions but about how to face challenges and exclude other options. It focuses efforts on a decisive point to achieve a significant effect.\nA good strategy clearly shows where resources will be allocated according to the strategy.\nIn my case, I defined the product direction based on the current situation and vision, focusing on specific target segments and values to offer.\nActions Actions are a coherent set of steps designed to execute the guiding policy. A strategy coordinates all actions to implement the policy effectively.\nA good strategy includes guidelines for implementing actions.\nMy Strategy Based on the above, I developed the following strategy (partially obscured as it was for my company):\n1 Reduce the new user cost of using xx function. This simple strategy meets the three basic structures of a good strategy:\nDiagnosis Identified increasing new users of xx as critical for the company\u0026rsquo;s benefit. Guiding Policy Decided to lower the onboarding cost to acquire new users. Actions Prioritized several approaches to realize the policy. Although it could have included more action-oriented words, I decided on the above. What is a Bad Strategy Finally, let\u0026rsquo;s touch on common patterns of bad strategies:\nCharacteristics of a Bad Strategy:\nVague Uses jargon or industry terms to obscure simple facts. Avoids Significant Problems Strategy should overcome difficult challenges and obstacles. Strategies focusing only on attainability are bad. Confuses Goals with Strategy For instance, a 10% revenue increase is a goal, not a strategy. Sets Wrong Strategic Objectives Strategies set without sufficient investigation of causes and surroundings. Summary This article summarized how to create a good product strategy based on \u0026ldquo;Good Strategy Bad Strategy.\u0026rdquo; Creating a good strategy might not happen at once. However, not having a strategy is like running blindly, unable to judge whether actions are successful or not.\nIt\u0026rsquo;s important to start with a basic strategy, constantly observe surroundings, and update the strategy to define the direction for your product and organization.\n","date":"2024-01-08T21:55:15+09:00","permalink":"https://bossagyu.com/en/blog/013-good-strategy-bad-strategy/","title":"How to Create a Product Strategy"},{"content":"Overview This article explains how to set up Twitter Social Cards for a blog created with Hugo.\nWhat is a Twitter Social Card? A Twitter Social Card is an image that is displayed when an article is shared on Twitter. The image below is an example of a Twitter Social Card.\nThere are several types of Twitter Social Cards, including:\nSummary Card Summary Card with Large Image App Card Player Card For sharing blog posts, the most commonly used types are Summary Card and Summary Card with Large Image.\nFor more details on each card type, refer to Twitter\u0026rsquo;s official documentation.\nHow to Set Up Twitter Social Cards There are two main methods to set up Twitter Social Cards:\nSetting up through the theme Setting up independently of the theme Setting Up Through the Theme Some themes allow you to set up Twitter Social Cards directly.\nFor this example, I\u0026rsquo;ll use the Stack theme that I\u0026rsquo;m using. In Stack, you can configure Twitter Social Cards in config.toml as follows:\n1 2 3 4 5 6 7 8 [opengraph.twitter] site = \u0026#34;\u0026#34; card = \u0026#34;summary\u0026#34; # summary or summary_large_image [defaultImage.opengraph] enabled = true local = false src = \u0026#34;/images/share.webp\u0026#34; # Path to the default image you want to set Setting Up Independently of the Theme If your theme doesn’t support Twitter Social Card settings, you will need to implement it yourself.\nHugo’s official template for implementation is available, which you can use for an easy setup.\nTroubleshooting If the settings don’t seem to work, it might be due to incorrect implementation or the meta tags not being properly set. In such cases, use the debugging tool provided by Twitter to check if the settings have been correctly applied.\nSummary This article explained how to set up Twitter Social Cards for a blog created with Hugo. Setting up Social Cards can enhance the visibility of your shared articles on Twitter, potentially attracting more readers, so it\u0026rsquo;s definitely worth doing.\n","date":"2024-01-06T21:45:12+09:00","permalink":"https://bossagyu.com/en/blog/012-social-card/","title":"Setting Up Twitter Social Cards"},{"content":"Overview This article explains how to use ChatGPT to make a blog created with Hugo multilingual.\nTranslating Articles into English with ChatGPT You can translate articles written in Markdown into English using ChatGPT. When doing so, use the following prompt to ensure that the format remains intact:\n1 2 3 Please translate this Markdown into English without altering its format. Ensure that no extraneous output is included. Present the translated content in a format that can be easily copied. Paste your article in Japanese, and ChatGPT will output the translated Markdown. You can copy the output directly by clicking the copy button at the bottom left of the output.\nIt is highly recommended to use GPT-4 instead of GPT-3.5, despite the subscription cost. GPT-4 significantly outperforms GPT-3.5 in providing accurate responses, making it useful for purposes beyond just translation.\nMaking Hugo Multilingual Here are the steps to make your Hugo site multilingual.\nCreating Configuration Files Add the following settings to your config.toml:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Set the default language, without this setting English is assumed default. defaultContentLanguage = \u0026#34;jp\u0026#34; [languages] # Set configurations for each language [languages.jp] title = \u0026#34;Bossagyu Blog\u0026#34; languageName = \u0026#34;ja-jp 🇯🇵\u0026#34; LanguageCode = \u0026#34;ja-jp\u0026#34; contentDir = \u0026#34;content\u0026#34; # Directory for Japanese blog articles [languages.jp.params] [languages.en] title = \u0026#34;Bossagyu Blog\u0026#34; languageName = \u0026#34;en-US 🇺🇸\u0026#34; LanguageCode = \u0026#34;en-US\u0026#34; contentDir = \u0026#34;content.en\u0026#34; # Directory for English blog articles [languages.en.params] With the above settings, you can write Japanese articles in the content directory and English articles in the content.en directory to support multiple languages.\nThe final directory structure will look like this:\n1 2 3 4 5 6 7 8 9 10 11 project/ ├── content/ │ ├── index.md │ └── blog/ │ ├── index.md │ └── article1.md └── content.en/ ├── index.md └── blog/ ├── index.md └── article1.en.md In the English directory, naming files as article-name.en.md identifies them as English versions of the default language articles, and language switch icons will be added to the articles. For the articles, just copy and paste the translations from ChatGPT.\nSummary This article explained how to make a Hugo blog multilingual using ChatGPT. Utilizing ChatGPT simplifies the translation process. Multilingual support can help reach audiences outside Japan, increasing readership.\nSince it\u0026rsquo;s low-cost and efficient, it\u0026rsquo;s worth giving it a try.\n","date":"2023-12-31T20:46:36+09:00","permalink":"https://bossagyu.com/en/blog/011-hugo-multilingul-support/","title":"Using ChatGPT to Make a Hugo Blog Multilingual"},{"content":"Overview This article explains a simple method to create a favicon. It also covers how to display a favicon in Hugo.\nWhat is a Favicon? A favicon is an icon that appears in bookmarks, tabs, and home screens for websites. Google has published guidelines for favicons that appear in search results. Adhering to these guidelines can help your favicon appear in search results.\nCreating a Favicon To create a favicon, use the following site:\nFavicon.ico \u0026amp; App Icon Generator When you visit the site, you\u0026rsquo;ll see a screen like this.\nEnter the URL of the site you want a favicon for and click on Generate Favicon. Then, on the displayed screen, click the \u0026lsquo;Download the generated favicon\u0026rsquo; link to download the favicon.\nDisplaying a Favicon in Hugo To display a favicon in Hugo, the process varies depending on the theme. For the bearcub theme, you can simply set it in the toml like this:\n1 2 [params] favicon = \u0026#34;images/favicon.ico\u0026#34; Summary This article explained how to create a favicon and display it in Hugo. Favicons are displayed in bookmarks, tabs, and home screens, so it\u0026rsquo;s a good idea to create one for your blog.\n","date":"2023-12-24T22:14:39+09:00","permalink":"https://bossagyu.com/en/blog/010-favicon/","title":"Creating and Displaying a Favicon with Hugo"},{"content":"Overview This article explains how to measure your blog\u0026rsquo;s performance using Lighthouse.\nWhat is Lighthouse? Lighthouse is a performance measurement tool for websites provided by Google. It\u0026rsquo;s available as a Google Chrome extension and can be used by installing the plugin.\nInstalling Lighthouse Install Lighthouse from the Chrome Web Store.\nOpen the site you want to analyze and click on the Lighthouse icon.\nClick on Generate report to start the analysis.\nThis time, I ran it on my blog page.\nThe results are displayed as follows, taking about 1 minute to complete.\nInterpreting the Results Performance Evaluates web performance, like page loading and image display speeds. Clicking the See calculator link takes you to more details.\nAccessibility Checks whether all users can access content and navigate efficiently within the site. Scrolling down shows areas flagged by Accessibility.\nIt points out weak color contrast in the code snippets and missing descriptions in links.\nHowever, the flagged content is not from my writing but depends on the template, so to fix this, it would be necessary to override the Hugo template.\nBest Practices Tests the integrity of web pages. You can view the testing items in the results.\nSEO You can check if the page is optimized for search engine result rankings.\nProgressive Web App Checks if the loading speed of web pages on smartphones is optimized and if it\u0026rsquo;s suitable for PWAs. This wasn\u0026rsquo;t checked in this case.\nSummary The article explained how to measure the performance of a blog using Lighthouse. Especially for SEO, as it affects visibility in Google search results, it\u0026rsquo;s important to address these issues adequately.\n","date":"2023-12-22T23:08:00+09:00","permalink":"https://bossagyu.com/en/blog/009-light-house/","title":"Introduction to Using Lighthouse"},{"content":"Overview This article explains how to use AWS EventBridge to schedule Lambda functions for periodic execution.\nWhat is AWS EventBridge? AWS EventBridge is a service that facilitates the passing of events between AWS services, enabling the creation of event-driven architectures. For a detailed explanation, refer to the AWS official documentation on EventBridge.\nPrerequisites It is assumed that you have already created a Lambda function. For instructions on creating Lambda functions, please see the AWS Lambda Getting Started guide.\nSteps to Schedule a Lambda Function with EventBridge To schedule your Lambda function with EventBridge, follow these steps:\nSelect the Lambda Function: Open the AWS Management Console, navigate to the Lambda service, and select the function you want to schedule.\nAdd a Trigger: Choose \u0026lsquo;Add trigger\u0026rsquo; and select \u0026lsquo;EventBridge (CloudWatch Events)\u0026rsquo; from the list of available triggers.\nCreate a Rule: Once you select EventBridge, you will be prompted to create a new rule. Set up the rule to trigger the Lambda function at desired intervals. For this tutorial, we are using a cron expression to execute the function every 5 minutes. You can find more about cron syntax in the EventBridge Scheduler documentation.\nComplete the Setup: After configuring the rule, it will appear in the Lambda function\u0026rsquo;s diagram as an associated trigger.\nTest the Setup: As a practical application, I set up a function to send messages to LINE. The setup now sends notifications every 5 minutes, as shown in the screenshot below.\nConclusion We\u0026rsquo;ve outlined how to use AWS EventBridge to schedule Lambda functions periodically. Remember, leaving the EventBridge setup as is will incur charges, so be sure to delete it if it\u0026rsquo;s no longer needed.\n","date":"2023-12-21T23:03:13+09:00","permalink":"https://bossagyu.com/en/blog/008-aws-eventbrdge/","title":"Using AWS EventBridge to Schedule Lambda Functions"},{"content":"Overview To appear in Google searches, it\u0026rsquo;s not enough just to apply SEO strategies; your site must first be recognized by Google. This article explains how to make your own custom domain blog appear in Google search results using Google Search Console.\nSteps to Implementation Registering with Google Search Console Verifying Domain Ownership Registering the Sitemap Requesting Index Registration Summary Registering with Google Search Console Register on Google Search Console.\nChoose your domain and enter the URL.\nVerifying Domain Ownership A screen like the following will appear to verify DNS ownership.\n(The TXT record content is blacked out for privacy.)\nYou can verify ownership by adding a string specified by Google to your domain\u0026rsquo;s TXT record. Go to your domain\u0026rsquo;s DNS settings and add a TXT record.\nIn my case, I acquired the domain through Netlify, so I went to Netlify\u0026rsquo;s DNS settings. Navigate to Domains -\u0026gt; Domain Settings -\u0026gt; DNS Records and add the TXT record.\nCopy the content displayed on Google Search Console and paste it into the Value field.\n(The Value part is blacked out for privacy.)\nWait for DNS updates, which can take a few hours depending on the provider.\nYou can check DNS propagation from the command line.\n1 dig -t txt bossagyu.com Afterward, press the verify ownership button on Google Search Console.\nThis completes the verification process, and your domain will be registered with Google Search Console.\nRegistering the Sitemap Registering a sitemap informs Google about the structure of your site, facilitating the crawling process. For blogs created with Hugo, the sitemap is available at /sitemap.xml, which you should register.\nFrom the left menu of Google Search Console, select \u0026lsquo;Sitemaps\u0026rsquo; and add your sitemap.\nRequesting Index Registration Even if your site is registered in the sitemap, it can take time for Google to crawl and index it. In my case, I requested index registration after waiting several days without being indexed.\nSearch for the URL you want to register in Google Search Console, and click on \u0026lsquo;Request Indexing\u0026rsquo; found on the right side of the search result.\nThis requests index registration. It took a few hours for the index to be registered after clicking.\nSummary This article explained how to make your custom domain blog searchable using Google Search Console.\nIt\u0026rsquo;s a waste not to have your blog appear in Google searches after all the effort of creating it, so give it a try.\n","date":"2023-12-18T19:10:04+09:00","permalink":"https://bossagyu.com/en/blog/007-google-search-console/","title":"Using Google Search Console to Make Your Blog Searchable on Google"},{"content":"Overview This article explains how to efficiently develop Lambda functions using the AWS Toolkit in IntelliJ.\nSteps to Implementation Preliminary Preparation Installing AWS Toolkit Configuring AWS Toolkit Developing Lambda Executing Lambda Locally Summary Preliminary Preparation Installing Docker AWS Toolkit in IntelliJ uses Docker to run Lambda.\nPrior to proceeding, please install Docker by referring to these instructions.\nInstalling AWS CLI Install AWS CLI (SAM).\nFor installation, refer to these instructions.\nIn IntelliJ, set the path for SAM CLI executable under File -\u0026gt; Settings -\u0026gt; Tools -\u0026gt; AWS Toolkit.\nIn my case, as I installed it through brew, I set the following path.\nInstalling AWS Toolkit Install the AWS Toolkit via IntelliJ plugins. Refer to this guide for plugin installation.\nConfiguring AWS Toolkit To use AWS Toolkit, you need to set up your AWS credentials.\nSet up AWS credentials through AWS Explorer.\nObtain and configure your Access Key ID and Secret Access Key from the AWS console. Once configured, AWS resources should appear in AWS Explorer.\nNote: In this image, the region is set to us-east-1. Please adjust according to the region where you intend to create your Lambda.\nDeveloping Lambda Create a code snippet like the following.\nlambda-sample.py\n1 2 3 def lambda_handler(event, context): print(\u0026#34;Hello World\u0026#34;) return \u0026#34;Hello World!\u0026#34; Create a Lambda through AWS Explorer.\nSelect Create Lambda Function and input the necessary values.\nFor the Handler, enter \u0026lt;filename\u0026gt;.\u0026lt;function name\u0026gt; from your code snippet.\nThis completes the creation of your Lambda.\nExecuting Lambda Locally The Toolkit also allows you to execute Lambda locally. Selecting Run will execute the Lambda locally.\nSummary This article provided a guide on efficiently developing Lambda using AWS Toolkit in IntelliJ. Developing in IntelliJ and executing locally can significantly improve development efficiency.\n","date":"2023-12-12T22:40:05+09:00","permalink":"https://bossagyu.com/en/blog/006-intellij-lamda-setup/","title":"Efficient Lambda Development with AWS Toolkit in IntelliJ"},{"content":"Overview This article explains how to use GitHub Copilot in IntelliJ. Additionally, a cheat sheet of shortcuts is provided.\nSteps to Implementation Register for GitHub Copilot Configure IntelliJ Use GitHub Copilot Summary Registering for GitHub Copilot Register for GitHub Copilot through the GitHub Copilot link.\nConfiguring IntelliJ Install the GitHub Copilot plugin from IntelliJ plugins.\nOnce installed, restart IntelliJ.\nUsing GitHub Copilot When you write code in IntelliJ, GitHub Copilot will assist with code completion.\nHere is a list of shortcuts for Mac:\nShortcut Function tab Complete the code Option + ] Show the next completion suggestion Option + [ Show the previous completion suggestion Command + → Accept only the next word of the suggestion Summary This article explained how to use GitHub Copilot in IntelliJ. It\u0026rsquo;s worth noting that this article was written using GitHub Copilot, and it significantly assists with blog creation in Markdown, so those interested should give it a try.\n","date":"2023-12-11T22:45:40+09:00","permalink":"https://bossagyu.com/en/blog/005-github-copilot/","title":"How to Use GitHub Copilot in IntelliJ"},{"content":"Overview This article documents how to set up a Python environment for development on a Mac local environment.\nIn this case, we will use two systems to manage different versions of Python and virtual environments:\npyenv Used to handle multiple versions of Python. venv Used to separate environments for each project. For explanations on the differences and the necessity of each, this article is a helpful reference.\nInstalling Python First, install Pyenv on your local environment to use a specific version of Python.\nInstall pyenv.\n1 brew install pyenv Check the installed version of pyenv.\n1 2 pyenv --version pyenv 2.3.35 Add settings to zsh.\n1 2 3 echo \u0026#39;export PYENV_ROOT=\u0026#34;$HOME/.pyenv\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo \u0026#39;export PATH=\u0026#34;$PYENV_ROOT/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo -e \u0026#39;if command -v pyenv 1\u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then\\n eval \u0026#34;$(pyenv init -)\u0026#34;\\nfi\u0026#39; \u0026gt;\u0026gt; ~/.zshrc Reload .zshrc.\n1 source ~/.zshrc Display a list of installable Python versions.\n1 pyenv install --list Install the specified version.\n1 pyenv install 3.11.7 Use the specified Python version in your project folder.\n1 2 3 cd \u0026lt;created project folder\u0026gt; pyenv local 3.11.7 pyenv versions If global, it will be applied to the entire system.\n1 pyenv global 3.11.7 Check the version of Python being executed.\n1 python -V Creating a Virtual Environment with venv Create a virtual environment in the project directory.\n1 2 # python -m venv \u0026lt;virtual environment name\u0026gt; python -m venv venv Activate the virtual environment.\n1 source venv/bin/activate To deactivate, execute the following command.\n1 deactivate This completes the setup of the local environment.\n","date":"2023-12-10T23:19:33+09:00","permalink":"https://bossagyu.com/en/blog/004-paython-setup/","title":"Setting Up a Local Environment Using Pyenv and venv"},{"content":"Overview This article briefly explains how to set up Google Analytics with Hugo.\nSteps to Implementation Register with Google Analytics Obtain the Tracking ID Add the Tracking ID to Hugo\u0026rsquo;s configuration Registering with Google Analytics Follow the instructions on Setting up a new website or app with GA4 to register.\nWhen you add a data stream, you will get a Tracking ID, so make a note of it. ※ The Tracking ID may be displayed as \u0026ldquo;Measurement ID\u0026rdquo; due to translation.\nAdding Tracking ID to Hugo\u0026rsquo;s Configuration Add settings in toml Add googleAnalytics = Tracking ID to your config.toml.\n1 2 3 4 5 6 baseURL = \u0026#39;https://bossagyu.com\u0026#39; languageCode = \u0026#39;ja-jp\u0026#39; title = \u0026#39;Bossagyu Blog\u0026#39; theme = \u0026#39;hugo-bearcub\u0026#39; googleAnalytics = \u0026#34;G-1234ABCDEF\u0026#34; # ↑ Add this line, replace the Tracking ID with your own. Embedding the Tracking Code Some templates might read settings from the toml file, but the bearcub template I use does not support this, so I added the tracking code to the header myself.\nFor the code snippet, I referred to Makumaku Hugo Notes.\nCreate layouts/partials/analytics.html to load the tracking code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 {{ if not .Site.IsServer }} {{ with .Site.GoogleAnalytics }} \u0026lt;!-- Google tag (gtag.js) --\u0026gt; \u0026lt;script async src=\u0026#34;https://www.googletagmanager.com/gtag/js?id={{ . }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;js\u0026#39;, new Date()); gtag(\u0026#39;config\u0026#39;, \u0026#39;{{ . }}\u0026#39;); \u0026lt;/script\u0026gt; {{ end }} {{ end }} Make the page header load analytics.html.\n1 2 3 # Copy the content of the template to override it cp themes/hugo-bearcub/layouts/_default/baseof.html layouts/_default/baseof.html vim layouts/_default/baseof.html Add {{- partial \u0026quot;analytics\u0026quot; . -}} to baseof.html.\n1 2 3 4 5 6 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;{{ with .Site.LanguageCode }}{{ . }}{{ else }}en-US{{ end }}\u0026#34;\u0026gt; \u0026lt;head\u0026gt; {{- partial \u0026#34;analytics\u0026#34; . -}} \u0026lt;meta http-equiv=\u0026#34;X-Clacks-Overhead\u0026#34; content=\u0026#34;GNU Terry Pratchett\u0026#34; / After adding the above source code and rebuilding, data will be sent to Google Analytics.\nTips If it seems like data is not being transmitted to Google Analytics despite these steps, it\u0026rsquo;s possible that the tags have not been added correctly.\nTo troubleshoot, first check if the tracking is included in the HTML by using Google Developer Tools.\n","date":"2023-12-09T18:09:42+09:00","permalink":"https://bossagyu.com/en/blog/003-google-analytics/","title":"How to Set Up Google Analytics with Hugo"},{"content":"Overview I thought of creating an application using LINE\u0026rsquo;s Bot, so first, I will make the Bot usable.\nThis page introduces how to register for the LINE Messaging API and how to send messages from the command line using curl.\nUsing the Messaging API Log in to LINE Developers and create a provider.\nA provider is (Explanation)\n1 On the LINE Developers site, a service provider refers to individuals, companies, or organizations that provide services and obtain user information (service proprietor in LINE Mini Apps). So, you can enter any string you like.\nThen, create a new channel. Clicking the create button will establish a new channel.\nPosting from the Command Line Add friends by reading the QR code in the Messaging API settings.\nObtain the \u0026lsquo;Channel Access Token (Long-lived)\u0026rsquo; from the Messaging API settings. Get \u0026lsquo;Your User ID\u0026rsquo; from the channel basic settings.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 TOKEN=\u0026#34;\u0026lt;Channel Access Token (Long-lived)\u0026gt;\u0026#34; ID=\u0026#34;\u0026lt;Your User ID\u0026gt;\u0026#34; UUID=$(uuidgen | tr \u0026#34;[:upper:]\u0026#34; \u0026#34;[:lower:]\u0026#34;) curl -v -X POST https://api.line.me/v2/bot/message/push \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ -H \u0026#34;X-Line-Retry-Key: \u0026#34; \\ -d \u0026#34;{ \\\u0026#34;to\\\u0026#34;: \\\u0026#34;${ID}\\\u0026#34;, \\\u0026#34;messages\\\u0026#34;:[ { \\\u0026#34;type\\\u0026#34;:\\\u0026#34;text\\\u0026#34;, \\\u0026#34;text\\\u0026#34;:\\\u0026#34;Hello, world1\\\u0026#34; } ] }\u0026#34; If a response is returned and you see a post from the Bot in your LINE chat, it\u0026rsquo;s a success!\n","date":"2023-12-07T09:37:00+09:00","permalink":"https://bossagyu.com/en/blog/002-line-messaging-api/","title":"Registering and Using the LINE Messaging API"},{"content":"Overview This document describes the steps to create a site with Hugo, manage it with Github, and build it with Netlify from scratch.\nWith this method, you can easily publish by just pushing your Markdown-written blog to Github.\nProcess Generate a site with Hugo Push to Github Deploy with Netlify Generating a Static Site with Hugo First, install Hugo.\n1 brew install hugo Create a template for the blog.\n1 hugo new site my-blog Add a theme suitable for the blog as a submodule.\n1 2 3 4 5 cd my-blog git init # Add the theme as a submodule from Github git submodule add https://github.com/theNewDynamic/gohugo-theme-ananke.git themes/ananke Apply the theme by adding it to hugo.toml.\n1 echo \u0026#34;theme = \u0026#39;ananke\u0026#39;\u0026#34; \u0026gt;\u0026gt; config.toml Start the server.\n1 hugo server Access the URL like http://localhost:51517/ shown in the startup log Web Server is available at http://localhost:51517/ (bind address 127.0.0.1) to view the locally launched static site.\nTips If you want to change the Hugo theme, please choose your favorite one from Hugo Themes. It\u0026rsquo;s recommended to run through until you build with Netlify first, as this can be changed later. The way to write Toml files is described in Configure Hugo. Push to Github Create a repository on Github.\nAfter creation, execute the following commands to push your site.\n1 2 3 4 5 6 7 8 9 10 cd my-blog echo .hugo_build.lock \u0026gt;\u0026gt; .gitignore git add . git commit -m \u0026#34;first commit\u0026#34; git branch -M main # Replace \u0026lt;user name\u0026gt; with your own username. # This is an example of creating a repository called my-blog. git remote add origin git@github.com:\u0026lt;user name\u0026gt;/my-blog git push -u origin main Once the push is complete, the source code becomes viewable on the Github UI.\nDeploy with Netlify Access Netlify and perform deployment.\nThere are instructions on Hugo\u0026rsquo;s official website, so refer to them for integration.\nFollow the instructions to complete the deployment, and the result of the Deploy will be shown as published.\nClick on the URL displayed on the site to access the deployed site. This completes the deployment process. After this, any changes made and pushed to main will automatically trigger deployment, updating the site content.\n","date":"2023-12-02T00:59:37+09:00","permalink":"https://bossagyu.com/en/blog/001-hugo-netlify-build/","title":"Publishing a Blog with Hugo + Netlify + Github"}]