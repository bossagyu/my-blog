[{"content":"概要 FastAPIは、Pythonで高性能なAPIを構築するためのWebフレームワークです。以下の特徴があります。\n型ヒントベース: Pythonの型ヒントを活用し、リクエスト・レスポンスの自動バリデーションを実現 自動ドキュメント: Swagger UI（OpenAPI）が自動生成され、ブラウザからAPIをテスト可能 高速: Starlette + Pydanticをベースにしており、Node.jsやGoに匹敵するパフォーマンス この記事では、FastAPIを使ってTODOアプリのCRUD APIを作成する手順を解説します。\n前提条件 Python 3.11以上 uvがインストール済み（MacでUVを用いてPythonの開発環境を構築するを参照） 環境構築 まず、プロジェクトを作成し、必要なパッケージをインストールします。\n1 2 3 uv init fastapi-todo cd fastapi-todo uv add fastapi uvicorn fastapiがWebフレームワーク本体、uvicornがASGIサーバーです。\nHello World まずは最小構成でFastAPIの動作を確認します。main.pyを作成してください。\n1 2 3 4 5 6 7 8 from fastapi import FastAPI app = FastAPI() @app.get(\u0026#34;/\u0026#34;) def read_root(): return {\u0026#34;message\u0026#34;: \u0026#34;Hello, FastAPI!\u0026#34;} サーバーを起動します。\n1 uv run uvicorn main:app --reload --reloadオプションを付けると、コードの変更時に自動でリロードされます。\nブラウザで http://localhost:8000 にアクセスすると、以下のレスポンスが返ります。\n1 {\u0026#34;message\u0026#34;: \u0026#34;Hello, FastAPI!\u0026#34;} Swagger UIの確認 FastAPIの大きな魅力は、自動生成されるAPIドキュメントです。http://localhost:8000/docs にアクセスすると、Swagger UIが表示されます。\nここからAPIを直接テストできるため、開発中はcurlやPostmanなしでも動作確認が可能です。\nCRUD APIの実装 TODOアプリのCRUD APIを実装していきます。\nモデルの定義 まず、PydanticモデルでTODOのデータ構造を定義します。main.pyを以下のように書き換えてください。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from fastapi import FastAPI, HTTPException from pydantic import BaseModel app = FastAPI() class TodoCreate(BaseModel): title: str completed: bool = False class TodoResponse(BaseModel): id: int title: str completed: bool todos: dict[int, dict] = {} next_id: int = 1 TodoCreate: 新規作成・更新時のリクエストボディ TodoResponse: レスポンスの型定義 todos: インメモリのデータストア（dict） next_id: IDの自動採番用カウンター 作成（POST） 1 2 3 4 5 6 7 @app.post(\u0026#34;/todos\u0026#34;, response_model=TodoResponse) def create_todo(todo: TodoCreate): global next_id todo_data = {\u0026#34;id\u0026#34;: next_id, \u0026#34;title\u0026#34;: todo.title, \u0026#34;completed\u0026#34;: todo.completed} todos[next_id] = todo_data next_id += 1 return todo_data response_model=TodoResponseを指定することで、レスポンスの型がSwagger UIに反映されます。\n一覧取得（GET） 1 2 3 @app.get(\u0026#34;/todos\u0026#34;, response_model=list[TodoResponse]) def list_todos(): return list(todos.values()) 個別取得（GET） 1 2 3 4 5 @app.get(\u0026#34;/todos/{todo_id}\u0026#34;, response_model=TodoResponse) def get_todo(todo_id: int): if todo_id not in todos: raise HTTPException(status_code=404, detail=\u0026#34;Todo not found\u0026#34;) return todos[todo_id] 存在しないIDの場合はHTTPExceptionで404エラーを返します。\n更新（PUT） 1 2 3 4 5 6 7 @app.put(\u0026#34;/todos/{todo_id}\u0026#34;, response_model=TodoResponse) def update_todo(todo_id: int, todo: TodoCreate): if todo_id not in todos: raise HTTPException(status_code=404, detail=\u0026#34;Todo not found\u0026#34;) todo_data = {\u0026#34;id\u0026#34;: todo_id, \u0026#34;title\u0026#34;: todo.title, \u0026#34;completed\u0026#34;: todo.completed} todos[todo_id] = todo_data return todo_data 削除（DELETE） 1 2 3 4 5 6 @app.delete(\u0026#34;/todos/{todo_id}\u0026#34;) def delete_todo(todo_id: int): if todo_id not in todos: raise HTTPException(status_code=404, detail=\u0026#34;Todo not found\u0026#34;) del todos[todo_id] return {\u0026#34;detail\u0026#34;: \u0026#34;Todo deleted\u0026#34;} 完成したコード 上記をすべてまとめたmain.pyの全体像は以下のとおりです。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 from fastapi import FastAPI, HTTPException from pydantic import BaseModel app = FastAPI() class TodoCreate(BaseModel): title: str completed: bool = False class TodoResponse(BaseModel): id: int title: str completed: bool todos: dict[int, dict] = {} next_id: int = 1 @app.post(\u0026#34;/todos\u0026#34;, response_model=TodoResponse) def create_todo(todo: TodoCreate): global next_id todo_data = {\u0026#34;id\u0026#34;: next_id, \u0026#34;title\u0026#34;: todo.title, \u0026#34;completed\u0026#34;: todo.completed} todos[next_id] = todo_data next_id += 1 return todo_data @app.get(\u0026#34;/todos\u0026#34;, response_model=list[TodoResponse]) def list_todos(): return list(todos.values()) @app.get(\u0026#34;/todos/{todo_id}\u0026#34;, response_model=TodoResponse) def get_todo(todo_id: int): if todo_id not in todos: raise HTTPException(status_code=404, detail=\u0026#34;Todo not found\u0026#34;) return todos[todo_id] @app.put(\u0026#34;/todos/{todo_id}\u0026#34;, response_model=TodoResponse) def update_todo(todo_id: int, todo: TodoCreate): if todo_id not in todos: raise HTTPException(status_code=404, detail=\u0026#34;Todo not found\u0026#34;) todo_data = {\u0026#34;id\u0026#34;: todo_id, \u0026#34;title\u0026#34;: todo.title, \u0026#34;completed\u0026#34;: todo.completed} todos[todo_id] = todo_data return todo_data @app.delete(\u0026#34;/todos/{todo_id}\u0026#34;) def delete_todo(todo_id: int): if todo_id not in todos: raise HTTPException(status_code=404, detail=\u0026#34;Todo not found\u0026#34;) del todos[todo_id] return {\u0026#34;detail\u0026#34;: \u0026#34;Todo deleted\u0026#34;} 動作確認 サーバーを起動して、Swagger UIから動作確認を行います。\n1 uv run uvicorn main:app --reload http://localhost:8000/docs にアクセスし、以下の順にテストしてください。\n1. TODOの作成 POST /todosを開き、「Try it out」をクリックして以下のJSONを入力します。\n1 2 3 4 { \u0026#34;title\u0026#34;: \u0026#34;買い物に行く\u0026#34;, \u0026#34;completed\u0026#34;: false } 「Execute」をクリックすると、以下のようなレスポンスが返ります。\n1 2 3 4 5 { \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;買い物に行く\u0026#34;, \u0026#34;completed\u0026#34;: false } 2. 一覧取得 GET /todosを実行すると、作成したTODOの一覧が返ります。\n3. 更新 PUT /todos/1でcompletedをtrueに変更してみましょう。\n1 2 3 4 { \u0026#34;title\u0026#34;: \u0026#34;買い物に行く\u0026#34;, \u0026#34;completed\u0026#34;: true } 4. 削除 DELETE /todos/1を実行すると、TODOが削除されます。GET /todosで空の配列が返ることを確認してください。\nまとめ この記事では、FastAPIを使ってTODO管理のCRUD APIを作成しました。FastAPIの主な利点をまとめます。\nPydanticモデルによるリクエスト・レスポンスの自動バリデーション Swagger UIによるインタラクティブなAPIドキュメント Pythonの型ヒントだけでAPIの仕様が定義できるシンプルさ 今回はインメモリのデータストアを使いましたが、実際のアプリケーションではSQLAlchemyなどのORMを使ってデータベースと連携することになります。\n関連記事 MacでUVを用いてPythonの開発環境を構築する - Python環境構築 GCSとGemini File Search APIを自動同期するイベント駆動バックエンドの設計 - FastAPIの実践例 ","date":"2026-02-28T00:00:00+09:00","permalink":"https://bossagyu.com/blog/049-fastapi-crud/","title":"FastAPIでCRUD APIを作る入門チュートリアル"},{"content":"概要 デジタル庁主催の「法令×デジタル」ハッカソン第三弾に参加し、法令横断検索プロダクト「Lawve」を開発しました。Lawveは、e-Gov法令データやユーザーがアップロードしたドキュメントを横断的に自然言語検索できるプロダクトです。\nこの記事では、筆者が担当したバックエンドのアーキテクチャ設計について解説します。バックエンドの主な責務は「Google Cloud Storage（GCS）にファイルが配置・削除されたとき、Gemini File Search APIの状態を自動的に同期する」ことです。\nシステム全体像 Lawveのシステム構成は以下の通りです。\nflowchart TB subgraph ユーザー U[ブラウザ] end subgraph GCP subgraph フロントエンド FE[Cloud RunNext.js] end subgraph バックエンド CR[Cloud RunFastAPI] end subgraph ストレージ・データ GCS[(Cloud Storage)] FS[(Firestore)] end subgraph イベント基盤 EA[Eventarc] end subgraph AI GEMINI[Gemini FileSearch API] end end subgraph 外部API EGOV[e-Gov法令API] end U --\u003e FE FE --\u003e GEMINI FE --\u003e FS FE --\u003e EGOV FE --\u003e|ファイルアップロード| GCS GCS --\u003e|イベント通知| EA EA --\u003e|CloudEvents| CR CR --\u003e|登録・削除| GEMINI CR --\u003e|ダウンロード| GCS クリックで拡大 各コンポーネントの役割 コンポーネント 役割 Cloud Run（Next.js） フロントエンド。検索UI、ファイルアップロード、検索結果の表示 Cloud Run（FastAPI） バックエンド。GCSのイベントを受けてGemini File Search APIを同期 Cloud Storage ドキュメントの保管場所。Single Source of Truth Eventarc GCSのファイル変更イベントをCloud Runにルーティング Gemini File Search API ドキュメントの全文検索・セマンティック検索を提供 Firestore ドキュメントメタデータ、コメント情報の管理 e-Gov法令API 法令データの取得 この記事では、バックエンドの Cloud Run（FastAPI）の設計に焦点を当てます。\nイベント駆動アーキテクチャの設計 なぜイベント駆動を選んだか 法令検索プロダクトでは、ドキュメントがGCSに配置されたタイミングでGemini File Search APIに自動登録される必要があります。フロントエンドからの同期APIを作る方法もありますが、以下の理由でイベント駆動アーキテクチャを採用しました。\n疎結合: フロントエンドはGCSにファイルを配置するだけでよく、バックエンドの存在を意識しない 信頼性: GCSのイベントをEventarcが確実に検知し、Cloud Runに配信する 運用ツールとの親和性: CLIやスクリプトからGCSにファイルを配置しても同じように同期される Eventarcによるイベントルーティング Eventarcは2種類のGCSイベントを監視し、Cloud Runのエンドポイントにルーティングします。\nイベント トリガー条件 google.cloud.storage.object.v1.finalized ファイルがGCSにアップロードされたとき google.cloud.storage.object.v1.deleted ファイルがGCSから削除されたとき Cloud RunのエンドポイントはCloudEventsフォーマットでイベントを受信します。\n1 2 3 4 5 6 7 8 @app.post(\u0026#34;/\u0026#34;) async def handle_storage_event(request: Request): headers = dict(request.headers) body = await request.body() event = from_http(headers, body) result = event_handler.process(event) return result ファイルアップロード時の処理フロー ファイルがGCSに配置されると、以下のフローで処理されます。\nイベント受信・パスフィルタリング: file-search/archive/ プレフィックスを持つファイルのみ処理対象とする メタデータ抽出: GCSパスから law_id と source_type を自動抽出 既存ドキュメント削除: 同じ source_path を持つドキュメントがあれば削除（重複防止） ファイルダウンロード: GCSからローカルの一時ファイルにダウンロード File Search Storeに登録: メタデータを付与してGemini File Search APIに登録 1 2 3 4 5 6 class EventHandler: def _handle_event_by_type(self, event_type, bucket_name, file_name): if event_type == \u0026#34;google.cloud.storage.object.v1.finalized\u0026#34;: self._handle_file_upload(bucket_name, file_name) elif event_type == \u0026#34;google.cloud.storage.object.v1.deleted\u0026#34;: self._handle_file_delete(bucket_name, file_name) ファイル削除時の処理フロー ファイルがGCSから削除されると、対応するドキュメントをGemini File Search APIからも削除します。\nイベント受信・パスフィルタリング メタデータでFile Search Store内を検索: source_path メタデータで一致するドキュメントを特定 ドキュメント削除: File Search Storeから該当ドキュメントを削除 Gemini File Search APIとの統合 GCPエコシステムでの統一 Lawveではインフラ全体をGCPで統一する方針を取りました。RAGサービスとしてOpenAIやPineconeなど他の選択肢もありますが、Cloud Run、GCS、EventarcとシームレスにつながるGemini File Search APIを採用しています。\nFile Search Storeの概要 Gemini File Search Storeは、ドキュメントを登録するとベクトル化・インデックス化を行い、セマンティック検索を提供するマネージドサービスです。バックエンドでは以下の操作を行います。\nドキュメント登録: ファイルとメタデータをアップロード ドキュメント検索: メタデータをキーに既存ドキュメントを検索 ドキュメント削除: 不要になったドキュメントを削除 メタデータを活用した管理 各ドキュメントには以下の3つのメタデータを付与し、管理に活用しています。\nメタデータ 用途 例 law_id 法令ID。法令の一意識別に使用 323AC0000000205 source_type ドキュメントの分類 user, doc, admin source_path GCSの完全パス。ドキュメントの一意識別に使用 gs://bucket/file-search/archive/user/323AC0000000205/医療法.txt source_path はGCSパスそのものであるため、GCSのファイルとFile Search Store内のドキュメントを一意に対応付けることができます。\n1 2 3 4 5 6 7 8 9 10 metadata = { \u0026#34;law_id\u0026#34;: law_id, \u0026#34;source_path\u0026#34;: source_path, \u0026#34;source_type\u0026#34;: source_type } custom_metadata = [ {\u0026#34;key\u0026#34;: key, \u0026#34;string_value\u0026#34;: value} for key, value in metadata.items() ] 汎用性を持たせた設計の工夫 パスベースのメタデータ抽出 バックエンドは法令データに限定せず、GCSの特定パスに配置されたあらゆるファイルを処理できるよう設計しています。パスの規約は以下の通りです。\n1 gs://\u0026lt;bucket\u0026gt;/file-search/archive/\u0026lt;source_type\u0026gt;/\u0026lt;law_id\u0026gt;/\u0026lt;file_name\u0026gt; パスから source_type と law_id を正規表現で自動抽出します。\n1 2 3 4 5 6 7 8 # パスパターン GCS_PATH_PATTERN = r\u0026#39;file-search/archive/([^/]+)/([^/]+)/.+\u0026#39; def parse_gcs_path(gcs_path: str) -\u0026gt; Tuple[Optional[str], Optional[str]]: match = re.search(GCS_PATH_PATTERN, gcs_path) if match: return match.group(1), match.group(2) return None, None この設計により、法令データだけでなく社内文書や技術ドキュメントなど、任意のドキュメントを同じ仕組みで管理できます。source_type を変えるだけで分類を追加でき、パス規約さえ守ればバックエンドのコード変更は不要です。\nGCSとGemini File Search APIの状態同期の保証 GCSを Single Source of Truth（信頼できる唯一の情報源）とし、File Search Storeの状態を常にGCSと一致させることを設計原則としています。\nアップロード時の冪等性: 同じファイルを再度アップロードした場合、既存のドキュメントを削除してから再登録します。これにより重複を防ぎつつ、内容の更新にも対応できます。\n1 2 3 4 5 6 7 8 9 10 11 12 def process_file_upload(self, bucket_name, file_path): source_path = f\u0026#34;gs://{bucket_name}/{file_path}\u0026#34; # 既存ドキュメントの確認・削除 self._delete_existing_documents(source_path) # 新規アップロード law_id = extract_law_id(file_path) local_file_path = self.gcs_client.download_to_temp_file(bucket_name, file_path) # ... success = self.store_client.upload_file(local_file_path, file_path, metadata) return success 削除時の連動: GCSからファイルが削除されると、File Search Storeからも対応するドキュメントが自動的に削除されます。source_path メタデータをキーに検索し、一致するドキュメントを削除します。\n1 2 3 4 5 6 7 8 def process_file_delete(self, bucket_name, file_path): source_path = f\u0026#34;gs://{bucket_name}/{file_path}\u0026#34; documents = self.store_client.find_documents_by_source_path(source_path) if not documents: return False return self.store_client.delete_document(documents[0]) ローカルダウンロード経由の設計 GCSからGemini File Search APIへの登録は、一度ローカルの一時ファイルにダウンロードしてから行う設計にしています。\n1 2 3 4 5 6 7 8 9 10 def download_to_temp_file(self, bucket_name, file_path): _, ext = os.path.splitext(file_path) if not ext: ext = \u0026#34;.txt\u0026#34; temp_fd, temp_path = tempfile.mkstemp(suffix=ext) os.close(temp_fd) blob.download_to_filename(temp_path) return temp_path 直接アップロードする方がシンプルですが、ローカルに一度保存することで以下のメリットがあります。\n拡張性: 将来ExcelをCSVに変換するなど、アップロード前のファイル加工処理を挟める MIME判定: ファイル拡張子を保持することで正確なMIMEタイプ判定が可能 デバッグ: 問題発生時にローカルファイルの内容を確認できる エラーハンドリングと運用設計 常に200 OKを返す設計 Cloud RunのエンドポイントはEventarcからのリクエストに対し、エラーが発生しても常に200 OKを返します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @app.post(\u0026#34;/\u0026#34;) async def handle_storage_event(request: Request): try: headers = dict(request.headers) body = await request.body() event = from_http(headers, body) result = event_handler.process(event) return result except Exception as e: return { \u0026#34;status\u0026#34;: \u0026#34;error\u0026#34;, \u0026#34;message\u0026#34;: f\u0026#34;Event processing failed: {str(e)}\u0026#34;, \u0026#34;note\u0026#34;: \u0026#34;Error logged and notified via Slack\u0026#34; } これはEventarcのリトライを防ぐための設計です。エラー時に4xx/5xxを返すとEventarcがイベントを再送し、同じエラーが繰り返し発生します。結果としてSlack通知が重複し、ログが汚染されます。エラーはSlack通知とログで把握し、200 OKで受け取り済みとすることで運用上の問題を防いでいます。\nSlack通知によるエラー監視 File Search Storeへのアップロード失敗時にはSlack Webhookでエラー通知を送信します。通知にはGCSパスとエラー詳細を含め、運用者が問題箇所を素早く特定できるようにしています。\nこのSlack通知による監視はハッカソンの開発規模では十分ですが、実運用ではCloud Monitoringによるアラート設定やCloud Loggingとの連携、Dead Letter Queueによる失敗イベントの再処理など、より堅牢な監視・リカバリの仕組みが必要になるでしょう。\n遅延初期化によるクライアント管理 Gemini APIクライアントやGCSクライアントは遅延初期化（Lazy Initialization）を採用しています。\n1 2 3 4 5 @property def client(self) -\u0026gt; Optional[genai.Client]: if self._client is None and self.api_key: self._client = genai.Client(api_key=self.api_key) return self._client APIキーが未設定の場合でもアプリケーションが起動でき、テスト環境やローカル開発時にすべての環境変数を揃えなくても動作します。\nまとめ Lawveのバックエンドは、以下の設計原則に基づいて構築しました。\nイベント駆動による疎結合な自動同期: Eventarcを活用し、フロントエンドとバックエンドを疎結合に保ちつつGCSの変更を自動的にGemini File Search APIに反映 メタデータ駆動の汎用設計: パス規約からメタデータを自動抽出し、法令以外のドキュメントにも対応可能な拡張性を確保 GCSをSingle Source of Truthとした状態管理: 冪等なアップロードと連動削除により、GCSとFile Search Storeの状態を常に一致させる イベント駆動アーキテクチャとGCPのマネージドサービスを組み合わせることで、少ないコードで信頼性の高いドキュメント同期基盤を実現できました。GCSにファイルを配置するだけで検索可能になるシンプルさは、ハッカソンの限られた時間内での開発にも大きく貢献しました。\nソースコードはGitHubで公開しています。\n関連記事 MacでUVを用いてPythonの開発環境を構築する（Python環境構築） OpenAI Response APIの使い方まとめ（OpenAI APIの利用） ","date":"2026-02-15T00:00:00+09:00","permalink":"https://bossagyu.com/blog/048-lawve-backend/","title":"GCSとGemini File Search APIを自動同期するイベント駆動バックエンドの設計"},{"content":"このチュートリアルで学べること 掃除リマインダーBotの基本的な使い方を、5つのステップで学びます。\n友だち追加 タスクを追加する 通知を設定する 通知を受け取る タスクを完了する 所要時間は約5分です。一緒にやってみましょう！\nStep 1: 友だち追加 まずはBotを友だちに追加します。\n以下のQRコードをLINEで読み取るか、リンクをタップしてください。\n友だち追加はこちら\n友だち追加が完了すると、トーク画面が開きます。\nStep 2: タスクを追加する 次に、管理したい掃除タスクを登録します。\nトーク画面で以下のように入力して送信してください。\n1 追加 掃除機 7 これは「掃除機を7日ごとにリマインドする」という意味です。\nBotから「掃除機を追加しました」と返信が来れば成功です。\nポイント 「追加」の後にスペースを入れる タスク名は自由（トイレ、お風呂、換気扇など） 日数は何日ごとにリマインドするかを指定 Step 3: 通知を設定する タスクを追加したら、通知を受け取る曜日と時間を設定しましょう。\n以下のように入力して送信してください。\n1 通知設定 月水金 7 これは「月・水・金の7時に通知する」という意味です。\n「通知設定を更新しました」と返信が来れば設定完了です。\nポイント 曜日は続けて入力（月水金、毎日なら月火水木金土日） 時間は0〜23の数字で指定 Step 4: 通知を受け取る 設定した曜日・時間になると、期限切れのタスクがあれば通知が届きます。\n通知には以下の情報が含まれます。\nタスク名 何日超過しているか（例: +3日） 超過日数が多いほど、優先的に掃除すべきタスクです。\nStep 5: タスクを完了する 掃除が終わったら、Botに報告しましょう。\n以下のように入力して送信してください。\n1 完了 掃除機 「掃除機を完了しました」と返信が来れば完了です。\nタスクの期限がリセットされ、次は7日後に再び通知されます。\nポイント 完了のタスクはスペース区切りで複数複数指定可能です。 完了 掃除機 トイレ まとめ お疲れさまでした！これで基本的な使い方は完了です。\n覚えておきたいコマンド やりたいこと コマンド タスクを追加 追加 [タスク名] [日数] タスクを完了 完了 [タスク名] 通知を設定 通知設定 [曜日] [時間] 残りタスクを確認 残り 次のステップ より詳しい機能については、以下の記事をご覧ください。\n掃除リマインダーBotの全機能紹介 ぜひ活用して、掃除を習慣化しましょう！\n関連記事 掃除を忘れない！掃除リマインダーLINE Bot（全機能の紹介） LINE Messaging APIの登録と使い方（LINE Bot開発の基礎） ","date":"2026-02-07T20:00:00+09:00","permalink":"https://bossagyu.com/blog/047-clean-bot-tutorial/","title":"【初心者向け】掃除リマインダーBotの使い方チュートリアル"},{"content":"概要 家族で使える掃除リマインダーLINE Botを作りました。この記事では、その技術的な実装について解説します。\n完成したBotはこちら: 掃除リマインダーBot\nアーキテクチャ flowchart LR subgraph ユーザー操作 A[LINE User] end subgraph AWS B[API Gateway] C[Lambdaprocess_user_message] D[(S3JSON)] E[Lambdapush_message_periodically] F[EventBridge毎時実行] end A --\u003e|メッセージ送信| B B --\u003e C C \u003c--\u003e D E \u003c--\u003e D F --\u003e|トリガー| E E --\u003e|通知| A クリックで拡大 使用サービス サービス 用途 API Gateway (HTTP API) LINEのWebhook受信 Lambda メッセージ処理、定期通知 S3 ユーザーごとのタスクデータ保存 EventBridge 定期実行スケジュール なぜこの構成か サーバーレス: 使った分だけ課金、運用負荷ゼロ S3: RDSより安価、JSONでシンプルに管理 SAM: Infrastructure as Codeでデプロイ管理 プロジェクト構成 1 2 3 4 5 6 7 8 9 10 clean-bot/ ├── lib/ # コアモジュール │ ├── clean_task.py # タスク状態管理 │ ├── message.py # コマンドパーサー │ ├── line.py # LINE API │ └── s3_client.py # S3操作 ├── test/ # テスト ├── line_clean_bot.py # Lambdaエントリーポイント ├── template.yaml # SAMテンプレート └── Makefile # 開発コマンド 実装のポイント 1. Lambdaエントリーポイント 2つのLambda関数を用意しています。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # line_clean_bot.py def process_user_message(event, context): \u0026#34;\u0026#34;\u0026#34;LINEメッセージのWebhookハンドラ\u0026#34;\u0026#34;\u0026#34; body = json.loads(event.get(\u0026#39;body\u0026#39;)) # ユーザーID or グループIDを取得 if body[\u0026#39;events\u0026#39;][0][\u0026#39;source\u0026#39;][\u0026#39;type\u0026#39;] == \u0026#39;user\u0026#39;: line_id = body[\u0026#39;events\u0026#39;][0][\u0026#39;source\u0026#39;][\u0026#39;userId\u0026#39;] else: line_id = body[\u0026#39;events\u0026#39;][0][\u0026#39;source\u0026#39;][\u0026#39;groupId\u0026#39;] message = body[\u0026#39;events\u0026#39;][0][\u0026#39;message\u0026#39;][\u0026#39;text\u0026#39;] # S3からユーザーデータを取得 s3client = S3client(BUCKET_NAME) obj_key = line_id + \u0026#39;.json\u0026#39; if not s3client.check_exist_object(obj_key): s3client.update_object(obj_key, \u0026#39;{\u0026#34;tasks\u0026#34;: []}\u0026#39;) # メッセージを処理して返信 # ... def push_message_periodically(event, context): \u0026#34;\u0026#34;\u0026#34;定期実行でリマインド通知を送信\u0026#34;\u0026#34;\u0026#34; current_time = datetime.now() + timedelta(hours=9) # JST s3client = S3client(BUCKET_NAME) for obj_list in s3client.list_objects(): clean_task = CleanTask(s3client.get_object_body(obj_list.key)) # 通知条件を満たすか判定 if clean_task.should_notify(current_time): # メッセージ送信 # ... 2. タスクの期限管理 「前回から何日経過したか」でタスクの期限を判定します。\n1 2 3 4 5 6 7 8 9 10 11 12 # lib/clean_task.py def __evaluate_cleanup_timing(self, task): \u0026#34;\u0026#34;\u0026#34;タスクが期限切れか判定\u0026#34;\u0026#34;\u0026#34; task_time = datetime.strptime(task[\u0026#39;updated_at\u0026#39;], self.date_format) return (self.now - task_time).days \u0026gt;= int(task[\u0026#39;duration\u0026#39;]) def get_todo_tasks(self): \u0026#34;\u0026#34;\u0026#34;期限切れのタスク一覧を取得\u0026#34;\u0026#34;\u0026#34; return [task for task in self.tasks if not task.get(\u0026#39;paused\u0026#39;, False) and self.__evaluate_cleanup_timing(task)] 3. 通知判定ロジック ユーザーが設定した曜日・時間にのみ通知します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def should_notify(self, current_time): \u0026#34;\u0026#34;\u0026#34;通知すべきか判定\u0026#34;\u0026#34;\u0026#34; if not self.notification[\u0026#39;enabled\u0026#39;]: return False # 曜日チェック weekday_map = {0: \u0026#39;月\u0026#39;, 1: \u0026#39;火\u0026#39;, 2: \u0026#39;水\u0026#39;, 3: \u0026#39;木\u0026#39;, 4: \u0026#39;金\u0026#39;, 5: \u0026#39;土\u0026#39;, 6: \u0026#39;日\u0026#39;} current_weekday = weekday_map[current_time.weekday()] if current_weekday not in self.notification[\u0026#39;days\u0026#39;]: return False # 時刻チェック if current_time.hour \u0026lt; self.notification[\u0026#39;hour\u0026#39;]: return False # 今日すでに通知済みかチェック # ... return True 4. コマンドパーサー シンプルな文字列分割でコマンドを解析します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # lib/message.py def get_return_message(self, message, s3client): task_operation = self.__get_task_operation_name(message) # 最初の単語 task_name = self.__get_task_name(message) # 2番目の単語 if task_operation == \u0026#39;完了\u0026#39;: for task_name in self.__get_all_task_name(message): self.clean_task.update_task_updated_at(task_name) s3client.update_object(self.object_keyname, self.clean_task.get_json()) return \u0026#34;完了しました\u0026#34; if task_operation == \u0026#39;追加\u0026#39;: duration = self.__get_duration(message) # 3番目の単語 self.clean_task.add_task(task_name, duration) # ... 5. データ構造（JSON） ユーザーごとに1つのJSONファイルをS3に保存します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \u0026#34;tasks\u0026#34;: [ { \u0026#34;task_name\u0026#34;: \u0026#34;掃除機\u0026#34;, \u0026#34;updated_at\u0026#34;: \u0026#34;2024-01-01 12:00:00\u0026#34;, \u0026#34;duration\u0026#34;: 7, \u0026#34;paused\u0026#34;: false } ], \u0026#34;notification\u0026#34;: { \u0026#34;enabled\u0026#34;: true, \u0026#34;days\u0026#34;: [\u0026#34;月\u0026#34;, \u0026#34;水\u0026#34;, \u0026#34;金\u0026#34;], \u0026#34;hour\u0026#34;: 7, \u0026#34;last_notified_at\u0026#34;: \u0026#34;2024-01-01 07:00:00\u0026#34; } } SAMテンプレート AWS SAMを使ってインフラを定義します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 # template.yaml AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; Transform: AWS::Serverless-2016-10-31 Parameters: ChannelAccessToken: Type: String NoEcho: true BucketName: Type: String Default: your-bucket-name Globals: Function: Runtime: python3.11 Timeout: 300 Environment: Variables: CHANNEL_ACCESS_TOKEN: !Ref ChannelAccessToken BUCKET_NAME: !Ref BucketName Resources: # メッセージ処理Lambda ProcessUserMessageFunction: Type: AWS::Serverless::Function Properties: Handler: line_clean_bot.process_user_message CodeUri: . Policies: - S3CrudPolicy: BucketName: !Ref BucketName Events: ApiEvent: Type: HttpApi Properties: Path: /process_user_message Method: ANY # 定期通知Lambda PushMessagePeriodicallyFunction: Type: AWS::Serverless::Function Properties: Handler: line_clean_bot.push_message_periodically CodeUri: . Policies: - S3CrudPolicy: BucketName: !Ref BucketName Events: ScheduleEvent: Type: Schedule Properties: Schedule: cron(0 * * * ? *) # 毎時0分 デプロイ方法 1. 事前準備 AWS CLIの設定 SAM CLIのインストール LINE Messaging APIのチャンネルアクセストークン取得 2. ビルド \u0026amp; デプロイ 1 2 3 4 5 # ビルド sam build # デプロイ sam deploy --parameter-overrides ChannelAccessToken=your_token 3. LINE Webhook設定 デプロイ後に出力されるAPIエンドポイントを、LINE DevelopersコンソールでWebhook URLとして設定します。\nローカル開発 1 2 3 4 5 6 7 # ローカルAPIサーバー起動 sam local start-api --env-vars env.json # テスト実行 curl -X POST http://localhost:3000/process_user_message \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d @events/line_message.json コスト 月間コストの目安（100ユーザー想定）:\nサービス コスト Lambda ほぼ無料（無料枠内） API Gateway ほぼ無料（無料枠内） S3 数円/月 合計 数円〜数十円/月 まとめ サーバーレス構成で運用コストを最小化 S3 + JSONでシンプルなデータ管理 SAMでインフラをコード管理 「日数経過」という掃除に適した期限管理 ソースコードはGitHubで公開しています。\n関連記事 掃除リマインダーBotの使い方 LINE Messaging APIの登録と使い方 AWS EventBridgeを用いてLambdaを定期実行する方法 AWS API GatewayとLambdaを連携させる方法 S3のオブジェクトの存在確認をする方法【Python boto3】 ","date":"2026-02-07T19:00:00+09:00","permalink":"https://bossagyu.com/blog/046-clean-bot-technical/","title":"AWS Lambda + LINE Botで掃除リマインダーを作る"},{"content":"こんな悩みはありませんか？ 「掃除機、最後にかけたのいつだっけ\u0026hellip;」 「トイレ掃除、そろそろやらないと」 「換気扇の掃除、忘れてた！」 掃除は「やらなきゃ」と思っていても、つい忘れがち。特に週1回、月1回といった定期的な掃除は、気づいたら期限を過ぎていることが多いですよね。\n掃除リマインダーBotを作りました そんな悩みを解決するために、掃除リマインダーLINE Botを作りました。\nこのBotは、掃除タスクごとに「何日ごとにやるか」を設定でき、期限が来たらLINEで通知してくれます。\n主な機能 タスク登録: 「掃除機 7日」「トイレ 3日」など、タスクと間隔を設定 完了報告: 掃除が終わったらLINEで報告するだけ リマインド通知: 設定した曜日・時間に期限切れタスクを通知 一時停止: 旅行中などはタスクを停止できる 使い方 1. 友だち追加 以下のQRコードまたはリンクから友だち追加してください。\n友だち追加はこちら\n2. タスクを追加する 1 追加 掃除機 7 これで「7日ごとに掃除機」というタスクが登録されます。\n3. 掃除が終わったら報告 1 完了 掃除機 タスクの期限がリセットされ、次の通知まで7日間になります。\n4. 残りのタスクを確認 1 残り 期限切れのタスクが一覧で表示されます。何日超過しているかも分かります。\n1 2 3 タスクは以下の通りです。 掃除機 (+3日) トイレ (+1日) 5. 通知設定 好きな曜日と時間に通知を受け取れます。\n1 通知設定 月水金 7 これで月・水・金の7時に通知が届きます。\nコマンド一覧 コマンド 説明 例 追加 [タスク名] [日数] タスクを追加 追加 掃除機 7 完了 [タスク名] タスクを完了 完了 掃除機 削除 [タスク名] タスクを削除 削除 掃除機 停止 [タスク名] 一時停止 停止 掃除機 再開 [タスク名] 再開 再開 掃除機 残り 期限切れタスク表示 確認 全タスク詳細表示 通知設定 [曜日] [時間] 通知設定 通知設定 月水金 7 使い方 ヘルプ表示 なぜ「日数経過」で管理するのか 一般的なTODOアプリは「期日」を設定しますが、掃除は「前回からどれくらい経ったか」で判断することが多いです。\n掃除機は「1週間に1回くらい」 トイレは「3日に1回」 換気扇は「月に1回」 このBotは、完了報告をするたびに自動で次の期限が設定されるので、毎回日付を入力する手間がありません。\n無料で使えます このBotは完全無料で利用できます。ぜひお試しください！\n友だち追加はこちら\n関連記事 【初心者向け】掃除リマインダーBotの使い方チュートリアル（初心者向け使い方ガイド） LINE Messaging APIの登録と使い方（LINE Bot開発の基礎） ","date":"2026-02-07T18:30:00+09:00","permalink":"https://bossagyu.com/blog/045-clean-bot/","title":"掃除を忘れない！掃除リマインダーLINE Bot"},{"content":"概要 OpenAIのResponse APIを使ってテキスト生成を行う手順をまとめました。 公式ドキュメントの場所、APIキーの取得方法、Pythonからの最小サンプルまでを確認できます。\nResponse APIとは Response APIは、テキスト生成やツール呼び出しなどを統一的に扱えるOpenAIのAPIです。 単発の問い合わせから、会話形式のやり取りまで幅広く利用できます。\n公式ドキュメント 最新の仕様は公式ドキュメントを参照するのが確実です。\nResponse API リファレンス: https://platform.openai.com/docs/api-reference/responses APIキーの取得方法 Response APIを利用するにはOpenAIのAPIキーが必要です。 以下の手順で取得できます。\nOpenAIのダッシュボードにサインインする API Keysページ（https://platform.openai.com/api-keys）を開く 「Create new secret key」から新しいキーを発行する 取得したキーは環境変数に設定しておくと安全です。\n1 export OPENAI_API_KEY=\u0026#34;\u0026lt;YOUR_API_KEY\u0026gt;\u0026#34; Pythonのサンプルコード Python SDKを使う場合は、まずuvでパッケージをインストールします。\n1 uv pip install openai 以下はResponse APIでテキストを生成する最小サンプルです。\n1 2 3 4 5 6 7 8 9 10 from openai import OpenAI client = OpenAI() response = client.responses.create( model=\u0026#34;gpt-4.1-mini\u0026#34;, input=\u0026#34;Response APIの特徴を3行で教えてください。\u0026#34;, ) print(response.output_text) 実行イメージ 1 2 3 Response APIは単発の生成にも対話にも使える統一APIです。 シンプルなリクエストでモデル出力を取得できます。 公式ドキュメントを確認しながら用途に合わせて拡張できます。 まとめ Response APIはシンプルな入力でテキスト生成を行えるOpenAIのAPIです。 公式ドキュメントを確認し、APIキーを準備してからPythonで試すとスムーズに導入できます。\n関連記事 ChatGPT 4oの紹介（ChatGPTの最新機能紹介） GCSとGemini File Search APIを自動同期するイベント駆動バックエンドの設計（API統合パターンの実践例） ","date":"2025-12-24T10:00:00+09:00","permalink":"https://bossagyu.com/blog/044-openai-response-api/","title":"OpenAI Response APIの使い方まとめ"},{"content":"概要 Codex CLIを使ってローカル環境からコマンドライン経由でCodexにアクセスする手順をまとめました。 インストールからサインイン、代表的なコマンドの使い方まで一通り確認できます。\nCodex CLIのインストール方法 Node.jsが入っている環境ならnpm経由で簡単に導入できます。\n1 npm install -g codex-cli Homebrewを利用している場合は以下でもOKです。\n1 2 brew tap codexcli/tap brew install codex インストール後、バージョンが表示されれば準備完了です。\n1 codex --version サインインの方法 Codex CLIは初回実行時に認証が必要です。用途に応じてAPIキーかChatGPTアカウントでサインインできます。\nAPIキーでサインインする 1 codex login --token \u0026lt;YOUR_API_KEY\u0026gt; 環境変数に設定しておくと毎回入力する手間を省けます。\n1 export CODEX_API_KEY=\u0026lt;YOUR_API_KEY\u0026gt; ChatGPTのアカウントでサインインする ブラウザでの認証フローを利用したい場合は、トークン指定を省いて codex login を実行します。\n1 codex login 上記を実行するとブラウザが開き、ChatGPTのサインイン画面が表示されます。メールアドレスやGoogle/Appleアカウントでサインインし、許可を与えるとCLIに戻ってトークンが保存されます。\nサインイン後は共通で以下を実行し、アカウント情報が表示されれば認証成功です。\n1 codex whoami Codexの実行方法とサンプル 単発でコード生成を行う 1 codex run \u0026#34;PythonでFizzBuzzを書いて\u0026#34; ファイルをコンテキストに含めて要約する 1 codex run \u0026#34;このファイルを要約して\u0026#34; --file README.md 対話モードで試す 1 codex chat チャットモードでは、プロンプトを入力すると逐次回答が返ってきます。Ctrl + D で終了できます。\nチャット内のスラッシュコマンド 対話モードでは先頭に / を付けて補助操作を行えます。\nコマンド 役割 /help 利用可能なコマンド一覧を表示する /new 会話履歴をリセットして新しいスレッドを開始する /file \u0026lt;path\u0026gt; 指定したファイルをコンテキストに添付する /exit チャットを終了する コマンドの使い方 主なコマンドは以下の通りです。\nコマンド 説明 codex login --token \u0026lt;API_KEY\u0026gt; APIキーでサインインする codex login ChatGPTのアカウントでブラウザ認証を開始する codex whoami 現在サインインしているアカウントを確認する codex run \u0026quot;\u0026lt;prompt\u0026gt;\u0026quot; 単発のプロンプトを投げて結果を取得する codex run \u0026quot;\u0026lt;prompt\u0026gt;\u0026quot; --file \u0026lt;path\u0026gt; ファイルをコンテキストに含めて実行する codex chat 対話モードで利用する codex history 直近の実行履歴を確認する まとめ Codex CLIはインストール後にサインインを済ませるだけで、コマンドラインからすぐにCodexを活用できます。 よく使うコマンドを覚えておくと、スクリプト生成やファイル要約が素早く行えるようになります。\n関連記事 VSCodeでGithub Copilotを使いこなす完全ガイド Github CopilotをChat toolを使って便利に使う方法 OpenAI Response APIの使い方まとめ ","date":"2025-12-12T10:00:00+09:00","permalink":"https://bossagyu.com/blog/043-codex/","title":"Codex CLIの使い方まとめ"},{"content":"概要 この記事では、GoでRedisを使う方法について説明します。\nRedisの起動方法 Redisの説明やDockerでの起動方法については、こちらの記事を参照してください。\nRedisを起動する。\n1 2 3 docker run -d --name my-redis \\ -p 6379:6379 \\ redis:7.4.5-alpine redis-cliで接続確認。\n1 docker exec -it my-redis redis-cli ping PONGと返ってくればOK。\nGoのプロジェクトの作成 Goのプロジェクトを作成する。\n1 2 3 mkdir redis-go cd redis-go go mod init redis-go GoでRedisを使う Redisに接続して、書き込み・読み込みを行うサンプルコード。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/redis/go-redis/v9\u0026#34; ) func main() { // パスワードなし、ローカルの6379に接続 rdb := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;127.0.0.1:6379\u0026#34;, Password: \u0026#34;\u0026#34;, // なし DB: 0, DialTimeout: 3 * time.Second, ReadTimeout: 1 * time.Second, WriteTimeout: 1 * time.Second, }) defer rdb.Close() ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() // 接続確認 if err := rdb.Ping(ctx).Err(); err != nil { log.Fatalf(\u0026#34;redis ping failed: %v\u0026#34;, err) } log.Println(\u0026#34;connected to redis ✅\u0026#34;) // 基本操作（SET/GET） if err := rdb.Set(ctx, \u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, 10*time.Minute).Err(); err != nil { log.Fatalf(\u0026#34;SET error: %v\u0026#34;, err) } val, err := rdb.Get(ctx, \u0026#34;hello\u0026#34;).Result() if err != nil { log.Fatalf(\u0026#34;GET error: %v\u0026#34;, err) } fmt.Println(\u0026#34;hello =\u0026#34;, val) // Set型の例 key := \u0026#34;team:dev\u0026#34; if err := rdb.SAdd(ctx, key, \u0026#34;alice\u0026#34;, \u0026#34;bob\u0026#34;, \u0026#34;alice\u0026#34;).Err(); err != nil { log.Fatalf(\u0026#34;SADD error: %v\u0026#34;, err) } members, _ := rdb.SMembers(ctx, key).Result() fmt.Println(\u0026#34;members =\u0026#34;, members) } パッケージのインストールを行う。\n1 go mod tidy 実行する。\n1 go run main.go 以下のように表示されればOK。\n1 2 hello = world members = [alice bob] まとめ GoでRedisを使う方法について説明しました。 GoでRedisを使う場合は、go-redisを使用します。\n関連記事 RedisをDockerで起動する方法（Redisの起動方法） macでGoの開発環境を構築して最速でHello Worldする（Go環境構築） ","date":"2025-10-05T20:25:02+09:00","permalink":"https://bossagyu.com/blog/041-redis-go/","title":"RedisをGoで使う方法"},{"content":"概要 この記事では、RedisをDockerで起動する方法について説明します。\nRedisとは Redisは、オープンソースのインメモリデータ構造ストアであり、データベース、キャッシュ、メッセージブローカーとして使用されます。高速な読み書き性能を持ち、様々なデータ構造（文字列、リスト、セット、ハッシュなど）をサポートしています。\nDockerでのRedis起動 1. Dockerのインストール まず、Dockerがインストールされていることを確認します。インストールされ ていない場合は、Dockerの公式サイトからインストールしてください。\n2. Redisコンテナの起動 以下のコマンドを実行して、Redisコンテナを起動します。\n1 docker run --name redis -d -p 6379:6379 redis:7.4.5-alpine コンテナにログインする\n1 docker exec -it redis sh 3. Redisの動作確認 Redisコンテナ内で以下のコマンドを実行して、Redisクライアントを起動します。\n1 redis-cli データの設定と取得を試してみます。\n1 2 3 set name \u0026#34;hoge\u0026#34; get name \u0026#34;hoge\u0026#34; これで、Redisが正常に動作していることが確認できました。\n4. Redisコンテナの停止と削除 Redisコンテナを停止するには、以下のコマンドを実行します。\n1 docker stop redis Redisコンテナを削除するには、以下のコマンドを実行します。\n1 docker rm redis まとめ この記事では、Dockerを使用してRedisを起動する方法について説明しました。 Redisは高速なデータベースとして広く利用されており、Dockerを使用することで簡単にセットアップできます。 ぜひ、Redisを活用してみてください。\n関連記事 RedisをGoで使う方法（GoでRedisを操作する実践例） ","date":"2025-10-03T07:29:31+09:00","permalink":"https://bossagyu.com/blog/040-redis-local/","title":"RedisをDockerで起動する方法"},{"content":"概要 この記事では、Visual Studio Code (VSCode) で AWS Toolkit を使用する方法について説明します。\nAWS Toolkitのインストール VSCodeを起動します。 左側のサイドバーから拡張機能アイコンをクリックします。 検索バーに「AWS Toolkit」と入力し、表示されたリストから「AWS Toolkit for Visual Studio Code」を選択します。 「インストール」ボタンをクリックして、インストールを開始します。 Asia Pacific Tokyoリージョンを設定する方法 EXPLORERからリージョンをして利用するのですが、デフォルトではus-east-1が選択されているため、Asia Pacific Tokyoリージョンを設定する必要があります。 少しわかりにくいので、以下の手順で設定します。\n左側のサイドバーから「AWS Explorer」から、ハンバーガーメニューをクリックします。 Show or Hide Regionsをクリックします。 表示されたリストから「Asia Pacific (Tokyo) ap-northeast-1」を選択します。 これで、AWS ToolkitがAsia Pacific Tokyoリージョンで使用できるようになります。\n","date":"2025-08-11T20:33:30+09:00","permalink":"https://bossagyu.com/blog/039-aws-toolkit-vscode/","title":"Visual Studio CodeでAWS Toolkitを使う方法"},{"content":"概要 MacでVisual Studio Code（VSCode）のアップデート方法を説明します。 GUIからの手動アップデートに加え、コマンドラインでの更新方法や自動アップデートの設定についても解説します。\nGUIからアップデートする方法 VSCodeを起動します。 Code → 「更新の確認」を選択します。 英語の場合は Code → \u0026ldquo;Check for Updates\u0026rdquo; を選択します。 更新が始まると以下のような画面になります。\nダウンロードが完了すると「再起動して更新」が表示されます。\n再起動後「Visual Studio Codeのバージョン情報」をクリックして、バージョンが上がっていることを確認します。\nコマンドラインからアップデートする方法 Homebrewを使ってVSCodeをインストールしている場合は、ターミナルからアップデートできます。\n現在のバージョンを確認 1 code --version Homebrewでアップデート 1 brew upgrade --cask visual-studio-code Homebrewでインストールしていない場合は、既存のVSCodeをアプリケーションフォルダから削除した上で、以下のコマンドでHomebrewによる管理に切り替えられます。 設定やインストール済みの拡張機能はアカウント同期で復元できるため、削除しても問題ありません。\n1 brew install --cask visual-studio-code 以降は brew upgrade --cask visual-studio-code でアップデートが可能になります。\n自動アップデートの設定 VSCodeにはアップデートの挙動を制御する update.mode 設定があります。 設定画面を開き（Cmd + ,）、update mode で検索すると変更できます。\n設定値 挙動 default バックグラウンドで自動的にアップデートをダウンロード・インストールします manual アップデートの確認は行いますが、インストールは手動で行います start VSCodeの起動時のみアップデートを確認します none 自動アップデートを完全に無効化します settings.jsonで直接設定する場合は以下のように記述します。\n1 2 3 { \u0026#34;update.mode\u0026#34;: \u0026#34;default\u0026#34; } 特別な理由がない限り default のままで問題ありません。\nトラブルシューティング アップデートが「更新の確認」に表示されない 自動アップデートが無効になっている可能性があります。 設定の update.mode が none になっていないか確認してください。\nアップデートのダウンロードが失敗する プロキシやファイアウォールの設定が原因の場合があります。 以下の設定を確認してください。\n1 2 3 4 { \u0026#34;http.proxy\u0026#34;: \u0026#34;http://proxy.example.com:8080\u0026#34;, \u0026#34;http.proxyStrictSSL\u0026#34;: false } アップデート後に拡張機能が動作しなくなった 拡張機能の互換性の問題が考えられます。以下の手順を試してください。\nコマンドパレット（Cmd + Shift + P）を開く 「Extensions: Disable All Installed Extensions」を実行 VSCodeを再起動 拡張機能を1つずつ有効化して問題のある拡張機能を特定する それでも解決しない場合 VSCodeを再インストールすることで解決する場合があります。\n1 2 # Homebrewの場合 brew reinstall --cask visual-studio-code 設定やインストール済みの拡張機能はGitHubアカウントやMicrosoftアカウントで同期しておくと、再インストール後に自動で復元されます。\nまとめ VSCodeのアップデート方法について解説しました。\nGUI: Code → 「更新の確認」から数クリックで完了 コマンドライン: brew upgrade --cask visual-studio-code で一括更新 自動アップデート: update.mode 設定で挙動を制御可能 定期的にアップデートを行うことで、新機能やセキュリティパッチを利用できます。 アップデート後は、必ずバージョン情報を確認して、正しくアップデートが行われたかをチェックしましょう。\n","date":"2025-07-27T18:12:16+09:00","permalink":"https://bossagyu.com/blog/038-update-vscode/","title":"Visual Studio Codeのアップデート方法"},{"content":"概要 Visual Studio Code (VSCode) では、インデントガイドの色をカスタマイズすることができます。 この記事では、インデントガイドの色を変更する方法を説明します。 indent-rainbowのような拡張機能はインデントガイドが途中で切れるなどの問題がありましたが、VSCodeの設定ファイルを編集するだけで実現できます。\nインデントガイドの色を変更する方法 設定ファイルを開く VSCodeの設定ファイルを開きます。 Command Palette (Ctrl + Shift + P) を開き、Preferences: Open Settings (JSON) を選択します。\n設定を追加する 以下を参考に設定を追加します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \u0026#34;workbench.colorCustomizations\u0026#34;: { \u0026#34;editorIndentGuide.background1\u0026#34;: \u0026#34;#006400\u0026#34;, \u0026#34;editorIndentGuide.background2\u0026#34;: \u0026#34;#008000\u0026#34;, \u0026#34;editorIndentGuide.background3\u0026#34;: \u0026#34;#00a000\u0026#34;, \u0026#34;editorIndentGuide.background4\u0026#34;: \u0026#34;#006400\u0026#34;, \u0026#34;editorIndentGuide.background5\u0026#34;: \u0026#34;#008000\u0026#34;, \u0026#34;editorIndentGuide.background6\u0026#34;: \u0026#34;#00a000\u0026#34;, \u0026#34;editorIndentGuide.activeBackground1\u0026#34;: \u0026#34;#00ff00\u0026#34;, \u0026#34;editorIndentGuide.activeBackground2\u0026#34;: \u0026#34;#00ff00\u0026#34;, \u0026#34;editorIndentGuide.activeBackground3\u0026#34;: \u0026#34;#00ff00\u0026#34;, \u0026#34;editorIndentGuide.activeBackground4\u0026#34;: \u0026#34;#00ff00\u0026#34;, \u0026#34;editorIndentGuide.activeBackground5\u0026#34;: \u0026#34;#00ff00\u0026#34;, \u0026#34;editorIndentGuide.activeBackground6\u0026#34;: \u0026#34;#00ff00\u0026#34;, \u0026#34;editorWhitespace.foreground\u0026#34;: \u0026#34;#393A3D\u0026#34; }, } 設定内容 説明 editorIndentGuide.background\u0026lt;number\u0026gt; インデントガイドの背景色を指定します。\n\u0026lt;number\u0026gt; は インデントの深さに対応します。 editorIndentGuide.activeBackground\u0026lt;number\u0026gt; アクティブなインデントガイドの背景色を指定します。\n\u0026lt;number\u0026gt; はインデントの深さに対応します。 editorWhitespace.foreground 空白文字の色を指定します。 実際に設定を追加した結果以下の画像のようにインデントガイドの色が変更されます。 まとめ VSCodeでは、インデントガイドの色を簡単にカスタマイズできます。 設定ファイルに必要な項目を追加するだけで、好みの色に変更できます。 これにより、コードの可読性が向上し、インデントの深さを視覚的に把握しやすくなります。 ぜひ試してみてください。\n","date":"2025-07-22T21:01:13+09:00","permalink":"https://bossagyu.com/blog/036-vscode-indent/","title":"vscodeのインデントガイドをカスタマイズする方法"},{"content":"概要 IntelliJやPyCharmなどの JetBrains製品のメジャーバージョンを上げる際にサイドダウンロードするが必要になります。 設定を引き継ぎつつなるべく簡単にアップデートする方法について説明します。\nJetBrains Toolbox Appのインストール JetBrains製品のメジャーバージョンを上げる際は、JetBrains Toolbox Appを利用することをおすすめします。\n上記リンクよりJetBrains Toolbox Appをダウンロードし、インストールします。\nアップデート JetBrains Toolbox Appを起動し、アップデートしたい製品を選択します。\n今回はPyCharmをの更新を行うのでクリックすると、アップデートが開始されます。\nアップデートが完了すると、以下のように新しいバージョンがインストールされます。\nまとめ JetBrains製品のメジャーバージョンを上げる際は、JetBrains Toolbox Appを利用することで簡単にアップデートが可能です。\n","date":"2025-06-12T20:46:53+09:00","permalink":"https://bossagyu.com/blog/034-jetbrains-update/","title":"JetBrains製品のメジャーバージョンを上げる方法"},{"content":"概要 今回はFeastのチュートリアルを参考に、Macで実行してみます。\n事前準備 MacでUVを用いてPythonの開発環境を構築するを参考にUVを用いて開発できる環境を整えてください。\nuvで構築した環境にfeastをインストールする。\nFeastのインストールからUIの起動まで 1 \u0026gt; uv pip install feast==0.40.1 2025/01/13現在、Feastにバグがあり最新版をインストールするとUIが起動しないので注意してください。(issue)\nFeature Repositoryを作成する。\n1 2 3 \u0026gt; feast init my_feature_repo Creating a new Feast repository in /Users/kouhei/Program/ML/feast/my_feature_repo. 以下のようなリポジトリが作成される。\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; tree . └── my_feature_repo ├── README.md ├── __init__.py └── feature_repo ├── __init__.py ├── data │ └── driver_stats.parquet ├── example_repo.py ├── feature_store.yaml └── test_workflow.py Feastのチュートリアルの設定を反映する。\n1 2 cd my_feature_repo/feature_repo feast apply Feastのuiを起動する。\n1 \u0026gt; feast ui http://0.0.0.0:8888/p/my_feature_repo へアクセスするとUIが表示されます。\nFeastへのデータ操作 5.Build a training datasetからは jupyter notebookを使うのでインストールしておきます。\n1 uv pip install jupyter notebookの起動\n1 jupyter notebook Jupyter Notebookで以下の内容を実行し、トレーニングに利用するデータセットを準備する。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from feast import FeatureStore import pandas as pd from datetime import datetime entity_df = pd.DataFrame.from_dict({ \u0026#34;driver_id\u0026#34;: [1001, 1002, 1003, 1004], \u0026#34;event_timestamp\u0026#34;: [ datetime(2021, 4, 12, 10, 59, 42), datetime(2021, 4, 12, 8, 12, 10), datetime(2021, 4, 12, 16, 40, 26), datetime(2021, 4, 12, 15, 1 , 12) ] }) store = FeatureStore(repo_path=\u0026#34;.\u0026#34;) training_df = store.get_historical_features( entity_df=entity_df, features = [ \u0026#39;driver_hourly_stats:conv_rate\u0026#39;, \u0026#39;driver_hourly_stats:acc_rate\u0026#39;, \u0026#39;driver_hourly_stats:avg_daily_trips\u0026#39; ], ).to_df() print(training_df.head()) # Train model # model = ml.fit(training_df) NoteBookでの実行結果は以下の通りとなります。\nオンラインストアにデータを入れる。\nサンプルで記載されている feast materialize-incremental $CURRENT_TIME ではうまく動作しなかったので、データ全体を対象とするように時刻の範囲を設定しています。\n1 2 3 4 5 6 7 8 9 10 feast materialize 1970-01-01T00:00:00Z 2025-01-04T01:24:24Z 01/04/2025 10:28:40 AM root WARNING: _list_feature_views will make breaking changes. Please use _list_batch_feature_views instead. _list_feature_views will behave like _list_all_feature_views in the future. Materializing 2 feature views from 1970-01-01 09:00:00+09:00 to 2025-01-04 10:24:24+09:00 into the sqlite online store. driver_hourly_stats_fresh: 0%| | 0/5 [00:00\u0026lt;?, ?it/s]01/04/2025 10:28:40 AM root WARNING: Cannot use sqlite_vec for vector search 100%|███████████████████████████████████████████████████████████████| 5/5 [00:00\u0026lt;00:00, 1299.11it/s] driver_hourly_stats: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:00\u0026lt;00:00, 4569.95it/s] Jupyter Notebookを用いてオンラインストアからデータを取得します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pprint import pprint from feast import FeatureStore store = FeatureStore(repo_path=\u0026#34;.\u0026#34;) feature_vector = store.get_online_features( features=[ \u0026#39;driver_hourly_stats:conv_rate\u0026#39;, \u0026#39;driver_hourly_stats:acc_rate\u0026#39;, \u0026#39;driver_hourly_stats:avg_daily_trips\u0026#39; ], entity_rows=[{\u0026#34;driver_id\u0026#34;: 1001}] ).to_dict() pprint(feature_vector) # Make prediction # model.predict(feature_vector) 出力結果\n1 2 3 4 {\u0026#39;acc_rate\u0026#39;: [0.5004482269287109], \u0026#39;avg_daily_trips\u0026#39;: [691], \u0026#39;conv_rate\u0026#39;: [0.3067885637283325], \u0026#39;driver_id\u0026#39;: [1001]} Feastのオンラインストアにマテリアラズして、オンラインストアからデータを取得することができました。\nまとめ 本記事では、Feastのチュートリアルを基に、Mac上でFeastを使用してデータを管理する方法を紹介しました。UVを使ったPython開発環境の構築から、Feastのインストール、UIの起動、そしてトレーニングデータの準備やオンラインストアへのデータマテリアライズまで、一連の操作を丁寧に解説しました。 これにより、Feastを利用してトレーニングデータと推論データの管理を効率的に行うことができ、トレーニングスキューの回避が可能になります。\n関連記事 MacでUVを用いてPythonの開発環境を構築する（Python環境構築） Pyenvとvenvを用いたローカル環境のセットアップ方法（従来のPython環境構築方法） macでparquetファイルを読む方法（Feastのサンプルデータ形式） ","date":"2025-01-13T11:49:53+09:00","permalink":"https://bossagyu.com/blog/033-feast-tutorial/","title":"FeastのチュートリアルをMacで実行する"},{"content":"概要 MacでPythonの開発環境を構築する方法を紹介します。\nuvとは 2024年中旬に発表されたばかりのパッケージ管理ツール。\nRustで書かれており、他のパッケージ管理マネージャよりも高速であることが特徴です。\n公式の説明は こちら を参照してください。\n利用方法 uvのインストール 1 curl -LsSf https://astral.sh/uv/install.sh | sh PATHを通す\n1 2 source $HOME/.local/bin/env (sh, bash, zsh) source $HOME/.local/bin/env.fish (fish) インストールできたことを確認\n1 2 uv --version uv 0.5.13 (c456bae5e 2024-12-27) 使い方 仮想環境の作成\n1 uv venv 仮想環境のアクティベート\n1 source .venv/bin/activate パッケージのインストール\n1 uv pip install \u0026lt;package name\u0026gt; uvの細かい説明は 公式のドキュメント を参照してください。\nまとめ MacでPythonの開発環境を構築する方法を紹介しました。 uvは高速かつ使いやすいので2025年1月時点ではPythonの開発環境構築において有用なツールと言えるでしょう。\n関連記事 Pyenvとvenvを用いたローカル環境のセットアップ方法（従来のPython環境構築方法） ","date":"2025-01-01T14:39:53+09:00","permalink":"https://bossagyu.com/blog/032-python-uv/","title":"MacでUVを用いてPythonの開発環境を構築する"},{"content":"概要 Scrumを採用しているチームでは、セレモニーを行います。 セレモニーはチームメンバーが集まり、スプリントの進捗や課題を共有し、次のスプリントに向けての計画を立てるための重要な場です。 しかし、Scrumを惰性で続けてしまっているとセレモニーが長引いたり、効果的な議論が行われなかったりすることがあります。\nこの記事では、セレモニーの見直しによってチームのパフォーマンスを向上させる方法について考察します。\nセレモニーの目的を明らかにする 各セレモニーの目的は以下の通りです。\nもっとも大切なことはこの目的を達成するために必要な議論がなされるアジェンダになっていることです。 目的についてはスクラムガイドを参考に記載していますが、やや筆者の意訳が含まれています。\nセレモニー 目的 デイリースクラム スプリントゴールを達成するために、チームの一日の計画を立てる。 スプリントレビュー スプリントの成果を検査し、今後の適応を決定すること。 プロダクトバックログリファインメント プロダクトバックログを整理し、プロダクトゴールを達成するための計画を立てる。 スプリントレトロスペクティブ スプリントの振り返りを行い、品質と高価を高めるための適応を計画もしくは実行する。 スプリントプランニング 次のスプリント期間中の作業計画を立てる タイムボックスの徹底 セレモニーを効率的に行うためには、タイムボックスを徹底することが重要です。 タイムボックスとは、セレモニーの時間を決めてその時間内に議論を終えるというルールのことです。\nセレモニーではタイムボックスを超えないようにし、超えてしまった場合はなぜ超えてしまったのか、超えないようにはどうしていくと良いのかを振り返ることが重要です。\nセレモニーごとの効率的な議論を行うためのポイント セレモニーのうち特に時間を取られがちなものについて、効率的な議論を行うためのポイントを以下に示します。\nデイリースクラム デイリースクラムではチームが今日何をするべきかが明確になるようにすることに主眼を置きます。\nそれ以外の議論や報告についてはデイリースクラムとは呼ばずに関係者を絞り別途時間を設けるようにします。\nチームメンバー全員の時間を使って行う価値があるかどうかを常に意識することが重要です。\nポイント 15分を超えないようにする 報告内容を絞る 議論が発生する場合は、別途時間を設け議論を行うというルールにする スプリントレビュー スプリントレビューではステークホルダーに対してフィードバックをもらい、次のスプリントに向けての計画を立てるための情報を得ることが目的です。 漫然とインクリメントを見せるだけではなく、事前にフィードバックを得たい内容を明確にしておくことが重要です。\nポイント レビューの準備を事前に行う 誰に、どのようなフィードバック得たいのか事前に明らかにしておく（仮説を立てる） プロダクトバックログリファインメント プロダクトバックログリファインメントは一般的には最大でスプリントの10%程度の時間を割くことが推奨されています。\n特に見積もりに時間がかかることが多いのである程度見切りをつけて行うことが大切です。\nポイント バックログアイテムに対する詳細な議論をしすぎない。 次スプリント以降のアイテムはすぐにやらない可能性もあるので、詳細な議論は不要です。スプリント計画のタイミングで実施しましょう。 見積もりについては相対見積もりを採用し、細かい数値にこだわらない 不確実性のコーンコーンが示す通り、見積もりは基本的にはブレるものであり、細かい数値にこだわる必要はありません。 まとめ 今回はセレモニーを効率的に進める方法について説明しました。\nセレモニーを効率的に進めることで、チームのパフォーマンスを向上させることができます。\n","date":"2024-11-18T09:36:05+09:00","permalink":"https://bossagyu.com/blog/028-scrum-ceremony/","title":"セレモニーの見直しによるチームパフォーマンス向上"},{"content":"概要 MacでGo言語の開発環境を構築して、最速で Hello World する方法を紹介します。\nGo言語のインストール brewを使ってGo言語をインストールします。\n1 \u0026gt; brew install go バージョンを確認\n1 2 \u0026gt; go version go version go1.21.3 darwin/arm64 Hello World の実行 以下のコードを main.go として保存します。\n1 2 3 4 5 6 7 package main import \u0026#34;fmt\u0026#34; func main() { fmt.Printf(\u0026#34;Hello World\\n\u0026#34;) } 実行する\n1 2 \u0026gt; go run hello.go Hello World バイナリをビルドして実行する\n1 2 3 4 5 6 7 \u0026gt; go build hello.go \u0026gt; ls hello* hello.go \u0026gt; ./hello Hello World まとめ 以上で、MacでGo言語の開発環境を構築して、最速で Hello World する方法を紹介しました。\n関連記事 gRPCについての調査（gRPCの実践） RedisをGoで使う方法（Redis + Goの実践） ","date":"2024-09-15T16:52:04+09:00","permalink":"https://bossagyu.com/blog/030-go-environment-construction/","title":"macでGoの開発環境を構築して最速でHello Worldする"},{"content":"概要 macでparquetファイルをコマンドラインで簡単に読む方法を紹介します。\nparquet-cliを使って読む 今回は Feastのサンプルで提供されているParquetファイルを読んでみます。\nparquet-cliをbrewでmacにインストールする。\n1 brew install parquet-cli meta情報を確認する。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ parquet meta driver_stats.parquet File path: driver_stats.parquet Created by: parquet-cpp-arrow version 18.1.0 Properties: pandas: {\u0026#34;index_columns\u0026#34;: [{\u0026#34;kind\u0026#34;: \u0026#34;range\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;start\u0026#34;: 0, \u0026#34;stop\u0026#34;: 1807, \u0026#34;step\u0026#34;: 1}], \u0026#34;column_indexes\u0026#34;: [{\u0026#34;name\u0026#34;: null, \u0026#34;field_name\u0026#34;: null, \u0026#34;pandas_type\u0026#34;: \u0026#34;unicode\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;metadata\u0026#34;: {\u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;}}], \u0026#34;columns\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;event_timestamp\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;event_timestamp\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;datetimetz\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;datetime64[ns]\u0026#34;, \u0026#34;metadata\u0026#34;: {\u0026#34;timezone\u0026#34;: \u0026#34;UTC\u0026#34;}}, {\u0026#34;name\u0026#34;: \u0026#34;driver_id\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;driver_id\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;int64\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;int64\u0026#34;, \u0026#34;metadata\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;conv_rate\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;conv_rate\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;metadata\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;acc_rate\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;acc_rate\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;metadata\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;avg_daily_trips\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;avg_daily_trips\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;int32\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;int32\u0026#34;, \u0026#34;metadata\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;datetime\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;datetime64[us]\u0026#34;, \u0026#34;metadata\u0026#34;: null}], \u0026#34;creator\u0026#34;: {\u0026#34;library\u0026#34;: \u0026#34;pyarrow\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;18.1.0\u0026#34;}, \u0026#34;pandas_version\u0026#34;: \u0026#34;2.2.3\u0026#34;} ARROW:schema: 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 Schema: message schema { optional int64 event_timestamp (TIMESTAMP(NANOS,true)); optional int64 driver_id; optional float conv_rate; optional float acc_rate; optional int32 avg_daily_trips; optional int64 created (TIMESTAMP(MICROS,false)); } Row group 0: count: 1807 16.88 B records start: 4 total(compressed): 29.796 kB total(uncompressed):29.760 kB -------------------------------------------------------------------------------- type encodings count avg size nulls min / max event_timestamp INT64 S _ R 1807 2.78 B 0 \u0026#34;2021-04-12T07:00:00.00000...\u0026#34; / \u0026#34;2024-12-28T14:00:00.00000...\u0026#34; driver_id INT64 S _ R 1807 0.07 B 0 \u0026#34;1001\u0026#34; / \u0026#34;1005\u0026#34; conv_rate FLOAT S _ R 1807 5.42 B 0 \u0026#34;1.9221554E-4\u0026#34; / \u0026#34;0.9998668\u0026#34; acc_rate FLOAT S _ R 1807 5.42 B 0 \u0026#34;2.1329636E-4\u0026#34; / \u0026#34;0.99993944\u0026#34; avg_daily_trips INT32 S _ R 1807 3.14 B 0 \u0026#34;0\u0026#34; / \u0026#34;999\u0026#34; created INT64 S _ R 1807 0.05 B 0 \u0026#34;2024-12-28T15:20:28.266000\u0026#34; / \u0026#34;2024-12-28T15:20:28.266000\u0026#34; headで中を見る。\n1 2 3 4 5 6 7 8 9 10 11 12 $ parquet head driver_stats.parquet {\u0026#34;event_timestamp\u0026#34;: 1734102000000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.27734742, \u0026#34;acc_rate\u0026#34;: 0.7152132, \u0026#34;avg_daily_trips\u0026#34;: 823, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734105600000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.57354224, \u0026#34;acc_rate\u0026#34;: 0.9831811, \u0026#34;avg_daily_trips\u0026#34;: 851, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734109200000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.3287562, \u0026#34;acc_rate\u0026#34;: 0.6172164, \u0026#34;avg_daily_trips\u0026#34;: 116, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734112800000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.045716193, \u0026#34;acc_rate\u0026#34;: 0.032996926, \u0026#34;avg_daily_trips\u0026#34;: 741, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734116400000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.12863782, \u0026#34;acc_rate\u0026#34;: 0.8951942, \u0026#34;avg_daily_trips\u0026#34;: 534, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734120000000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.9555806, \u0026#34;acc_rate\u0026#34;: 0.62216556, \u0026#34;avg_daily_trips\u0026#34;: 216, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734123600000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.75297666, \u0026#34;acc_rate\u0026#34;: 0.37602386, \u0026#34;avg_daily_trips\u0026#34;: 954, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734127200000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.46957988, \u0026#34;acc_rate\u0026#34;: 0.6454945, \u0026#34;avg_daily_trips\u0026#34;: 360, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734130800000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.6702387, \u0026#34;acc_rate\u0026#34;: 0.36532214, \u0026#34;avg_daily_trips\u0026#34;: 396, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734134400000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.019627139, \u0026#34;acc_rate\u0026#34;: 0.528229, \u0026#34;avg_daily_trips\u0026#34;: 833, \u0026#34;created\u0026#34;: 1735399228266000} スキーマの確認。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 $ parquet schema driver_stats.parquet { \u0026#34;type\u0026#34; : \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;schema\u0026#34;, \u0026#34;fields\u0026#34; : [ { \u0026#34;name\u0026#34; : \u0026#34;event_timestamp\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, \u0026#34;long\u0026#34; ], \u0026#34;default\u0026#34; : null }, { \u0026#34;name\u0026#34; : \u0026#34;driver_id\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, \u0026#34;long\u0026#34; ], \u0026#34;default\u0026#34; : null }, { \u0026#34;name\u0026#34; : \u0026#34;conv_rate\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, \u0026#34;float\u0026#34; ], \u0026#34;default\u0026#34; : null }, { \u0026#34;name\u0026#34; : \u0026#34;acc_rate\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, \u0026#34;float\u0026#34; ], \u0026#34;default\u0026#34; : null }, { \u0026#34;name\u0026#34; : \u0026#34;avg_daily_trips\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, \u0026#34;int\u0026#34; ], \u0026#34;default\u0026#34; : null }, { \u0026#34;name\u0026#34; : \u0026#34;created\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, { \u0026#34;type\u0026#34; : \u0026#34;long\u0026#34;, \u0026#34;logicalType\u0026#34; : \u0026#34;local-timestamp-micros\u0026#34; } ], \u0026#34;default\u0026#34; : null } ] } まとめ macでparquetファイルを簡単に読む方法を紹介しました。\n関連記事 FeastのチュートリアルをMacで実行する（Parquetファイルを使ったFeastの活用） ","date":"2024-09-15T16:52:04+09:00","permalink":"https://bossagyu.com/blog/031-read-parquet-file/","title":"macでparquetファイルを読む方法"},{"content":"共分散とは 共分散は、2つの変数がどのように連動して変化するかを示す統計量です。\n正の値なら同じ方向に変化しやすく、負の値なら逆方向に変化しやすいことを意味します。\n計算式 共分散は次の式で計算されます：\nポイント 共分散が 0 に近い場合、2つの変数に強い関係はないと考えられます。 値がスケールに依存するため、相関係数を使うことが一般的です。 Pythonでの例 以下は、Pythonを使った共分散の計算例です。\n1 2 3 4 5 6 7 8 9 10 11 import numpy as np # サンプルデータ X = [2.1, 2.5, 4.0, 3.6] Y = [8, 10, 12, 14] # 共分散の計算 cov_matrix = np.cov(X, Y, bias=True) covariance = cov_matrix[0][1] print(f\u0026#34;共分散: {covariance}\u0026#34;) 実行結果 1 共分散: 1.53 共分散と相関係数の違い 特徴 共分散 相関係数 スケール依存 あり なし 値の範囲 -∞ から ∞ -1 から 1\\ まとめ 共分散は、2つの変数の関係性を理解するための基本的な指標です。\nただし、スケール依存性があるため、相関係数と併用することでより深い分析が可能です。\n","date":"2024-09-02T10:00:00+09:00","permalink":"https://bossagyu.com/blog/030-covariance/","title":"共分散についての調査"},{"content":"gRPCとは PRCを実現するためにGoogleが開発したプロトコルの一つ Protocol Bufferを使ってデータをシリアライズし、高速な通信を実現できる点が特徴 IDLを使ってあらかじめAPIの仕様を.protoファイルとして定義し、そこから、サーバ側\u0026amp;クライアント側に必要なソースコードを生成する。 REST と gRPCの違い\nRESTはリソース志向、RPCはメソッドの呼び出しが起点となり、データは副産物であるという考え方。 利点と欠点 利点 HTTP/2による高パフォーマンス Protocol Buffersによるデータ転送 IDLを書くことになるので、スキーマファーストで開発することになる 柔軟なストリーミング方式 欠点 HTTP/2非対応 ブラウザの対応状況が不十分 言語によって機能の実装増強にばらつきがある バイナリにシリアライズされると人間が読めない RESTでも十分早い .protファイル gRPCではシリアライズフォーマットとしてProtocol Buffersを利用する。\n.proto を拡張子として持つファイル上にスキーマ定義を行い、 protoc コマンド絵各言語用のコードを生成する。\nProtocol Buffersでは全ての値が型を持つ。型はスカラー型とメッセージ型に分けることができる。\nスカラー型 数値、文字列、真偽値、バイト配列 メッセージ型 複数のフィールドを持ったメッセージ型 メッセージ型は一つの .proto ファイルに複数定義することができる 1 2 3 4 5 message Person { int32 id = 1; string name = 2; string email =3; } gRPCのQuick Startを実施する 今回はPythonの環境を用いて、gRPCのQuick Startを実施する。\nhttps://grpc.io/docs/languages/python/quickstart/\n起動に必要なPythonの環境を整える。\n1 2 python -m pip install grpcio python -m pip install grpcio-tools サンプルコードのダウンロード\n1 2 git clone -b v1.64.0 --depth 1 --shallow-submodules https://github.com/grpc/grpc cd grpc/examples/python/helloworld サーバを起動する。\n1 2 3 4 python greeter_server.py # 出力 Server started, listening on 50051 別のターミナルを起動し、クライアントを起動する。\n1 2 3 4 5 python greeter_client.py ## レスポンス Will try to greet world ... Greeter client received: Hello, you! gRPCのクライアントとサーバを用いて通信を行うことができました。\n.proto ファイルを変更してみる 今回は、helloworld.proto ファイルを変更して、新しいメソッドを追加してみます。\nhelloworld.prot ファイルが格納されている\n1 cd grpc/examples/protos 以下のように修正する\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 syntax = \u0026#34;proto3\u0026#34;; option java_multiple_files = true; option java_package = \u0026#34;io.grpc.examples.helloworld\u0026#34;; option java_outer_classname = \u0026#34;HelloWorldProto\u0026#34;; option objc_class_prefix = \u0026#34;HLW\u0026#34;; package helloworld; // The greeting service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} // 以下の1行を追加 rpc SayHelloAgain (HelloRequest) returns (HelloReply) {} rpc SayHelloStreamReply (HelloRequest) returns (stream HelloReply) {} rpc SayHelloBidiStream (stream HelloRequest) returns (stream HelloReply) {} } // The request message containing the user\u0026#39;s name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } grpcのコードを生成する\n1 2 3 cd examples/python/helloworld python -m grpc_tools.protoc -I../../protos --python_out=. --pyi_out=. --grpc_python_out=. ../../protos/helloworld.proto 以下のファイルが再作成されている。\n1 2 3 4 5 ls -l -rw-r--r--@ 1 xx xx 1823 9 1 18:12 helloworld_pb2.py -rw-r--r--@ 1 xx xx 578 9 1 18:12 helloworld_pb2.pyi -rw-r--r--@ 1 xx xx 7018 9 1 18:12 helloworld_pb2_grpc.py 更新される _pd ファイルとは、protocol Buuffersの定義クラスが自動で生成されており基本的にはさわらない。\ngreeter_server.py を更新する。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from concurrent import futures import logging import grpc import helloworld_pb2 import helloworld_pb2_grpc class Greeter(helloworld_pb2_grpc.GreeterServicer): def SayHello(self, request, context): return helloworld_pb2.HelloReply(message=\u0026#34;Hello, %s!\u0026#34; % request.name) # 以下の関数を追加 def SayHelloAgain(self, request, context): return helloworld_pb2.HelloReply(message=\u0026#34;Hello Again, %s!\u0026#34; % request.name) def serve(): port = \u0026#34;50051\u0026#34; server = grpc.server(futures.ThreadPoolExecutor(max_workers=10)) helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(), server) server.add_insecure_port(\u0026#34;[::]:\u0026#34; + port) server.start() print(\u0026#34;Server started, listening on \u0026#34; + port) server.wait_for_termination() if __name__ == \u0026#34;__main__\u0026#34;: logging.basicConfig() serve() greeter_client.py を更新する。\n1 2 3 4 5 6 7 8 9 10 11 def run(): # NOTE(gRPC Python Team): .close() is possible on a channel and should be # used in circumstances in which the with statement does not fit the needs # of the code. print(\u0026#34;Will try to greet world ...\u0026#34;) with grpc.insecure_channel(\u0026#34;localhost:50051\u0026#34;) as channel: stub = helloworld_pb2_grpc.GreeterStub(channel) response = stub.SayHello(helloworld_pb2.HelloRequest(name=\u0026#34;you\u0026#34;)) print(\u0026#34;Greeter client received: \u0026#34; + response.message) response = stub.SayHelloAgain(helloworld_pb2.HelloRequest(name=\u0026#34;you\u0026#34;)) print(\u0026#34;Greeter client received: \u0026#34; + response.message) serverを再起動し、clientを実行する。\n1 2 3 4 5 6 python greeter_server.py python greeter_client.py # 出力 Greeter client received: Hello, you! Greeter client received: Hello Again, you! 追加したメソッドが正常に動作していることが確認できました。\nまとめ 今回はgRPCについて調査の調査と公式ドキュメントのチュートリアルを行いました。 gRPCはスキーマファーストで開発することができ、HTTP/2による高パフォーマンス通信が可能であることら、最近ではREST APIの代替手段として注目されている技術ですので、ぜひ抑えておきたいです。\n関連記事 macでGoの開発環境を構築して最速でHello Worldする（Go環境構築） ","date":"2024-09-01T17:53:57+09:00","permalink":"https://bossagyu.com/blog/029-grpc/","title":"gRPCについての調査"},{"content":"GPT-4oの登場 2024年5月13日に、OpenAI社より新しいGPTモデル、ChatGPT-4oが発表されました。\nGPT-4oは従来のモデルと比較して以下の内容が向上しています。\n自然な対話の実現 より高速な応答 多言語対応の強化 より高い信頼性 特に注目すべきは、推論速度と質の向上です。これにより、リアルタイムの対話システムにおいても高い性能を発揮し、よりスムーズで自然なコミュニケーションが可能となります。\nリアルタイム対話の応用 以下の動画では、スマートフォンを利用してChatGPT-4oとのリアルタイム対話のデモンストレーションが紹介されています。\n実際にご覧いただけるとわかるように、ChatGPT-4oは人間との対話に近いレベルで応答を返すことができます。\nこれにより、ユーザーは非常に自然な対話を楽しむことができます。\n今までのChatGPTシリーズでは音声を文字列へ変換し、その後GPT-4へ入力することで対話を行っていました。\nこれは、音声に含まれる声色などの感情の情報が失われることを意味しています。\nしかし、ChatGPT-4oでは音声からモデルのトレーニングを行っているため、声色などの情報も考慮されており、より自然な対話が実現されています。\nまた、音声から直接音声を返却するので、テキストを解釈するステップがない分より高速に応答が返ってくるようになっています。\n多言語対応の強化 GPT-4oは、多言語対応の強化も図っています。\n今までは英語で質問をすると、精度が高い回答が得られるが、日本語で質問をすると精度が落ちるという問題がありました。\n今回の多言語対応の強化により、日本語でも高い精度で回答が得られるようになりました。\nまとめ 今回は、OpenAI社が発表したChatGPT-4oについて紹介しました。\nChatGPTのモデルの精度の向上はめざましく、どんどん人間の対話に近づいていると感じます。\n今後エンジニアは生成AIをどれだけうまく使えるかで、生産性が大きく変わってくると感じているので引き続き動向を注視していきたいと思います。\n関連情報 Hello ChatGPT-4o 関連記事 chatGPTで画像を生成する方法（ChatGPTによる画像生成） OpenAI Response APIの使い方まとめ（OpenAI API活用） ","date":"2024-05-14T23:22:39+09:00","permalink":"https://bossagyu.com/blog/027-chatgpt-4o/","title":"ChatGPT 4oの紹介"},{"content":"概要 2024年4月26日にEvernoteの日本法人が解散するなど、Evernoteのサービスは終了しないもののいよいよEvernoteの雲行きが怪しくなってきました。\nEvernoteは無料プランに制限が多いため、有料プランを利用しており費用がかさむこともあり、他のノートアプリへの移行を検討しました。\n以下の理由から移行先はNotionを選択しました。\nノートアプリととして基本的な機能を揃えており、Evernoteの代替として十分利用できる。 Notionは無料プランの制限がゆるく無料プランで十分使え移行することで有料プランの費用を削減できる。 Evernoteからインポートする機能がNotion側で提供されており、移行コストが非常に低い。 今回はEvernoteからNotionへの移行方法をまとめます。\n移行方法 NotionにはEvernoteからのインポート機能が提供されているため、こちらを利用するだけで簡単に移行ができます。\nNotionのアプリのメニューから 設定 を選択します。\n設定をクリックすると インポート が表示されるので、これをクリックします。\nインポートをクリックすると、インポート元のアプリケーションが表示されるので、ここで Evernote を選択します。\n連携が完了するとインポートするノートブックが選択できるようになります。\nここでの注意点です！\nノートブックを複数選択すると一気にインポートできるように見えるのですが、一気にインポートするととんでもない時間がかかった挙げ句エラーが出ることがあります。 このため、ノートブックは一つづつインポートすることをおすすめします。\n一つずつインポートした場合でも、ノートの分量が多いと数時間かかったりするので、気長に対応するとよいです。\nインポート後は特に体裁が崩れることもなく、画像やリンク、ラベルも正常にインポートされるので問題なく利用ができました。\nまとめ EvernoteからNotionへの移行方法をまとめました。 無料で機能が豊富なNotionに簡単に移行できるので、ぜひ移行を検討してみてください。\n","date":"2024-04-29T19:32:38+09:00","permalink":"https://bossagyu.com/blog/026-evernote-to-notion/","title":"EvernoteからNotionへの移行方法"},{"content":"概要 .gitignore ファイルをプロジェクトに追加することでプロジェクト事にgitのトラッキング対象から外すことができます。\nしかしながら.idea などIDEがデフォルトで生成するディレクトリを毎回プロジェクト毎にgitignoreに追加するのが面倒です。\n本記事では gitignore に設定した内容をすべてのプロジェクトに 適応する方法をまとめます。\ngitignoreを全体に適応する方法 gitはデフォルトで ~/.config/git/ignore へignore設定を見に行きます。\nこのため、~/.config/git/ignore にignore設定を記述することですべてのプロジェクトにgitignoreの内容を適応できます。\nよく .gitignore_global を作成して、core.excludesfile に登録する方法が案内されていますが、この方法だと .gitconfig に無駄な設定をいれる必要があるため、こちらの方法をおすすめします。\nプロジェクト全体にgitignoreを適応する手順 ignoreファイルを格納するためのディレクトリを作成します。\n1 mkdir -p ~/.config/git/ ignoreファイルを作成し、全プロジェクトで無視したい内容を記述してください。\n1 vim ~/.config/git/ignore 記載例\n1 2 3 .idea/ *.log node_modules/ この設定を行うことで、全プロジェクトで同じignore設定を適応できます。\nすでにトラッキングしているファイルを含む場合は、一度 git rm --cached でトラッキングを解除してください。\n参考 Git - gitignore ","date":"2024-04-16T23:16:25+09:00","permalink":"https://bossagyu.com/blog/025-git-ignore/","title":"グローバルなgitignoreを設定してプロジェクト全体に適応する方法"},{"content":"概要 Bluesky とは、旧Twitter社の元CEOであるジャック・ドーシー氏が立ち上げた分散型SNSです。\nATProtocl というプロトコルを用いて構築されたSNSで、簡単に言うと中央管理者がいないTwitterのようなものです。\n昨今の中央集権である通貨から分散型である仮想通貨への流れのように、SNSも分散型への流れがあるのかなと感じます。\n今回はそんなBlueskyのAPIをPythonを用いて実行する方法をまとめます。\nBluesky APIを使うまでのステップ API実行用パスワードの生成 Python実行環境の構築 スクリプトの作成と実行 API実行用パスワードの生成 APIを実行するためにはアカウント名とAPI実行用のパスワードの発行が必要です。\nまずは、APIの実行に利用するアカウント名を確認します。\nアカウント名は、Blueskyにログインした際に画像の箇所に表示される名前となります。\nこの際先頭の @ は不要で、私のアカウントであれば bossagyu.bsky.social がアカウント名となります。\n次に、API実行用のパスワードを生成します。\nAPI実行用パスワードは 設定 → アプリパスワード から生成できます。\nその後、アプリパスワードを追加 をクリックします。\n追加ボタンを押すと、パスワードにつける名前を聞かれます。\nこれ自体はパスワードとならず管理を容易にすることが目的なので、特にこだわりがなければそのまま作成します。\nパスワードが生成されるので、これをコピーしておきます。\nちなみに二度と表示されなくなるので、コピーを忘れないようにしましょう。コピーを忘れた場合は再生成すればOKです。\nPython実行環境の構築 Pythonの実行環境をセットアップしてください。\nvenvを用いたセットアップについては、こちら にまとめています。\n公式ドキュメント によると、Pythonのバージョンは3.7.1以上を利用する必要がありますので注意しておいてください。\nPythonの実行環境が整えば、ATProtocolを利用するためにライブラリをインストールします。\n1 $ pip install atproto インストールの確認\n1 2 3 $ pip list | grep atproto atproto 0.0.46 これで準備は完了です。\nスクリプトの作成と実行 Blueskyに投稿するスクリプトを作成します。\n1 2 3 4 5 6 7 8 9 from atproto import Client client = Client() user_name = \u0026#34;bossagyu.bsky.social\u0026#34; password = \u0026#34;*******\u0026#34; # 生成したAPI実行用パスワードを入力 client.login(user_name, password) client.send_post(text=\u0026#39;APIからの投稿です\u0026#39;) スクリプトはこれだけで、APIを用いてBlueskyに投稿できます。\nそれでは実行してみましょう。\n1 $ python post_bluesky.py 実行すると、Blueskyに以下のように無事投稿されました。\nまとめ 今回はPythonを用いてBlueskyのAPIを実行する方法をまとめました。 Blueskyはまだまだ開発途中のSNSですが、TwitterはAPIを課金しないと使えないなどの制約があるので、無料でAPIを使ってSNSで遊んでみたいという方にはおすすめです。\n関連記事 Pyenvとvenvを用いたローカル環境のセットアップ方法（Python環境構築） MacでUVを用いてPythonの開発環境を構築する（モダンなPython環境構築） ","date":"2024-04-07T23:52:09+09:00","permalink":"https://bossagyu.com/blog/024-bluesky-api/","title":"PythonでBluesky APIを用いて自動投稿する方法"},{"content":"概要 Stable Diffusionなどの画像生成用のモデルではなく ChatGPTでも画像が生成できるので、生成の方法を説明します。\nChatGPTの有料プランを利用している人は新たに課金などせずに利用できるので、大きな手間をかけずに商用利用可能な画像を生成できます。\n今回は、DALL-Eと呼ばれるChatGPT Plusの機能を利用して画像を生成します。 DALL-E3については OpenAI の公式ページを参照してください。\n画像生成の方法 サイドバーから 「Explore GPTs」を選択\n検索窓で DALL-E と入力し検索を行う。\nStart Chat をクリックし、画像生成を開始する。\n後は、生成したい画像の説明を入力するだけで画像が生成されます。\n実際に画像を生成してみる このブログで利用されている、ピンク色のサングラスを掛けた犬の画像を生成してみます。\nとりあえず ピンク色のサングラスをかけた犬 というプロンプトで打ってみます。\nまた、普段のChat GPTを使うように出力された画像に対して追加のプロンプトを入力することで加工できます。\n今回は アニメ調にしてください と追加のプロンプトを入力してみます。\nアニメ調になっているのがわかります。 このように追加のオーダーをすることでどんどん目的の画像に近づけつつ生成できます。\nまとめ 今回はChatGPTを利用して画像を生成する方法を説明しました。\n簡単に画像を生成できるのが非常に便利ですが、Stable Diffusion同様期待する画像を出力することはなかなか難しく、プロンプトを調整する必要があります。 Stable Diffusionを利用したときよりもプロンプトを工夫しなくても良い画像が生成されるので、このあたりはモデルの性能差なのかなと思いました。\nちなみに、やりすぎると以下のような文章が出力され、待つように言われるので生成回数には制限がついていそうです。 マシンソースがある人はやっぱりローカルmacでStable Diffusionでやるのが良さそうですね。\n関連記事 ChatGPT 4oの紹介（ChatGPT 4oの新機能紹介） ","date":"2024-03-31T17:35:07+09:00","permalink":"https://bossagyu.com/blog/023-chatgpt-create-image/","title":"chatGPTで画像を生成する方法"},{"content":"概要 この記事では、TypeScriptにおけるEnumの使い方について説明します。\nEnumとは Enum（列挙型）は、特定の値の集合を表す型です。\n多くの言語に実装されていますが、JavaScriptには存在しません。しかし、TypeScriptではEnumがサポートされています。\nEnumの使い方 以下のようにEnumを定義します。\n1 2 3 4 5 6 7 enum Status { zero, one, two } console.log(Status.zero); // 0 Enumは、デフォルトで数値を割り当てられ、0から始まります。 生成されるJavaScriptコードは以下の通りです。\n1 2 3 4 5 6 7 var Status; (function (Status) { Status[Status[\u0026#34;zero\u0026#34;] = 0] = \u0026#34;zero\u0026#34;; Status[Status[\u0026#34;one\u0026#34;] = 1] = \u0026#34;one\u0026#34;; Status[Status[\u0026#34;two\u0026#34;] = 2] = \u0026#34;two\u0026#34;; })(Status || (Status = {})); console.log(Status.zero); // 0 また、enumの値を文字列で指定することもできます。\n1 2 3 4 5 6 7 enum Status { zero = \u0026#39;zero\u0026#39;, one = \u0026#39;one\u0026#39;, two = \u0026#39;two\u0026#39; } console.log(Status.zero); // zero 文字列比較を行う場合は、以下のように記述します。\n1 2 3 4 5 6 7 8 const stringZero :String = \u0026#39;zero\u0026#39;; const value = stringZero as StringStatus; if (value === StringStatus.zero) { console.log(\u0026#39;value is zero\u0026#39;); } else { console.log(\u0026#39;value is not zero\u0026#39;); } まとめ この記事では、TypeScriptにおけるEnumの使い方について説明しました。 enumを利用することで、コードの可読性、保守性を向上させることができます。\n関連記事 Voltaを利用してTypeScriptの開発環境を簡単にセットアップする方法（TypeScript環境構築） ","date":"2024-03-23T13:11:13+09:00","permalink":"https://bossagyu.com/blog/022-typescript-enum/","title":"TyeScriptにおけるEnumの使い方"},{"content":"概要 この記事では、TypeScriptの開発環境を簡単にセットアップする方法について説明します。 本記事ではMacOSを対象にしています。\nVoltaとは VoltaはNode.jsのバージョン管理ツールです。\nVoltaの公式サイト で紹介されている通り以下の特徴を備えています。\n高速 Rustで構築されており、Node.jsのバージョン切り替えが高速です。 信頼できる プロジェクトの全員が同じツールを利用可能 ユニバーサル パッケージマネージャー、ノードランタイム、OSに依存なく利用可能。 今まではnodebrewなどを利用することが、一般的でしたが、現在はVoltaを利用するケースが増えている印象です。\nVoltaとNode.jsのインストール voltaのインストールは以下のコマンドだけで完了です。\n1 curl https://get.volta.sh | bash パスが通っていないことがあるのでzshを利用している方は以下のコマンドでパスを通してください。\n1 2 3 echo \u0026#39;VOLTA_HOME=$HOME/.volta\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo \u0026#39;export PATH=$VOLTA_HOME/bin:$PATH\u0026#39; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc 動作を確認。バージョンが表示されれば問題なくインストールができています。\n1 volta -v voltaを利用してNode.jsをインストールします。\nバージョンの指定をしない場合最新のLTSがインストールされます。\n1 volta install node yarnをインストールしてTypeScriptのプロジェクトを作成 npmとyarnの違い npm, yarnともにNode.jsのパッケージマネージャーとなります。\nそれぞれの特徴は以下の通りです。\nnpm\nNode.jsがリリースされた翌年（2010年）リリース Node Package Managerの略 package-lock.jsonファイルを自動的に生成する Node.jsをインストールすれば自動的にインストールされる yarn\n2016年リリース Facebook、Google、Exponent、Tildeによって開発された新しいJavaScriptパッケージマネージャー npｍと互換性がある 同じpackage.jsonが使える npmより厳密にモジュールのバージョンを固定できる npmよりインストールが速い yarnの方が優れているように見えますが、最近ではnpmがアップデートされて機能の差はあまりないようです。\n今回はyarnを利用してTypeScriptのプロジェクトを作成します。\nyarnのインストール voltaを利用してyarnをインストールします。\n1 volta install yarn インストールされているかを確認します。\nlistの結果にyarnが表示されれば問題なくインストールされています。\n1 volta list TypeScriptのプロジェクトを作成 yarnの初期化\n1 yarn init -y Node.jsのインストール\n1 volta pin node@20.0.0 TypeScriptのインストール\n1 yarn add typescript node-tsのインストール\n1 yarn add --dev ts-node tsconfig.jsonを作成します。\ntsconfig.jsonはTypeScriptの設定ファイルで、コンパイル時の設定を記述します。\n今回は、console.logを利用するため、今回はtargetを es2016 に設定します。特にデフォルトで生成されるものから変更する必要はありません。\n1 yarn tsc --init サンプルプログラムを実行してみる。\n1 2 3 4 5 echo \u0026#34;console.log(\u0026#39;Hello, TypeScript!\u0026#39;);\u0026#34; \u0026gt; hello.ts yarn ts-node hello.ts # 以下のように表示されれば成功 Hello, TypeScript! 無事にテストスクリプトが動きました。\n以上でTypeScriptの開発環境のセットアップが完了です。\nまとめ 本記事では、Voltaを利用してTypeScriptの開発環境を簡単にセットアップする方法について説明しました。 Voltaを利用することで、Node.jsのバージョン管理が簡単になり、開発環境のセットアップがスムーズに行えます。 また、VoltaでNode.jsのバージョンを指定すると、package.jsonにバージョンが記述され、他の開発者とのバージョンの差異を解消することができる点も魅力的ですね。\n関連記事 TyeScriptにおけるEnumの使い方（TypeScript Enumの使い方） ","date":"2024-03-10T13:11:13+09:00","permalink":"https://bossagyu.com/blog/021-typescript-setup/","title":"Voltaを利用してTypeScriptの開発環境を簡単にセットアップする方法"},{"content":"概要 この記事では、ITIL v4のキャパシティ及びパフォーマンス管理について説明します。 た、理解した内容をもとに自分の経験を当てはめキャパシティ及びパフォーマンス管理のプロセスについて説明します。\nキャパシティおよびパフォーマンス管理とは サービス及びサービスを支えるリソースのパフォーマンスを管理することです。\nキャパシティとパフォーマンスの管理活動を通じて、サービスのパフォーマンスを最適化し、サービスのキャパシティを適切に確保することが目的です。\nキャパシティ及びパフォーマンス管理のプロセス キャパシティ及びパフォーマンス管理のプロセスは以下の2つがあります。\nキャパシティとパフォーマンスコントロールの確立 サービスのキャパシティとパフォーマンスの分析と改善 キャパシティとパフォーマンスコントロールの確立 キャパシティとパフォーマンスコントロールの確立は、サービスが利用するITリソースの使用量と性能基準について、要件を利害関係者と合意し、それらを評価するタイミング・基準値・報告形式を決めることです。\n以下の流れで実現されます。\nサービスキャパシティとパフォーマンス要件の特定 サービスキャパシティとパフォーマンス要件の合意 キャパシティとパフォーマンスの要件の決定 キャパシティとパフォーマンス評価指標とレポートの設計 上記プロセスに対して筆者の経験を当てはめると以下の通りになりました。\nサービスキャパシティとパフォーマンス要件の特定 筆者は社内PFとしてAPIを提供していたため、社内の利用者から求められるレイテンシー性能（99%ile Nms）を特定しました。 上記のしきい値をベースにパフォーマンス検証を行い、1インスタンスあたりのスループットを測定しました。 スループットをベースに必要となる金額を算出しました。 サービスキャパシティとパフォーマンス要件の合意 レイテンシー性能とスループットを関係者と合意を行いました。 キャパシティとパフォーマンスの要件の決定 こちらについては合意内容と変わらず キャパシティとパフォーマンス評価指標のレポートの設計 パフォーマンスについてはDynatraceと呼ばれるトレーシングツールを利用して計測、レポートを作成しました。 サービスのキャパシティとパフォーマンスの分析と改善 サービスの出力ログ・インシデント情報から使用量と性能状況の問題点を分析する。\n以下の流れで実現されます。\nキャパシティとパフォーマンスの分析 キャパシティとパフォーマンスの報告 キャパシティとパフォーマンスの計画と設計 上記プロセスに対して筆者の経験を当てはめると以下の通りになりました。\nキャパシティとパフォーマンスの分析 パフォーマンスの分析については、Dynatraceを利用して、レイテンシー性能とスループットを分析しました。 インシデント情報から、パフォーマンスの問題点を特定しました。 キャパシティとパフォーマンスの報告 パフォーマンスの報告については、Dynatraceのダッシュボードを利用して、レイテンシー性能とスループットを可視化しました。 キャパシティとパフォーマンスの計画と設計 利用者の拡大によって現状のキャパシティでは受け入れが難しくなります。 需要予測を立て必要なキャパシティの増強計画を立てて、実行に移すようにしています。 まとめ 今回はキャパシティ及びパフォーマンス管理について学習した内容に基づき筆者の経験を当てはめ説明しました。 キャパシティ及びパフォーマンス管理では、可用性管理相当のことをキャパシティとパフォーマンスの観点から行うことが理解できました。 筆者の経験ではパフォーマンスの話と可用性の話についてはセットで行うことが多いのであまり独立して行わないと感じました。\n関連記事 ITIL v4 可用性管理について解説（関連ITIL v4記事） ITIL v4 事業分析について解説（関連ITIL v4記事） ","date":"2024-02-27T08:53:36+09:00","permalink":"https://bossagyu.com/blog/020-itilv4-capacity-and-performance-management/","title":"ITIL v4 キャパシティ及びパフォーマンス管理について解説"},{"content":"概要 この記事ではMacにStable Diffusion Web UIをインストールし、ローカルで利用する方法を紹介します。\nStable Diffusionとは Stable Diffusionは、AIを用いた画像処理技術の一つです。 テキストを入力することで、そのテキストに対応する画像を生成することができます。\n以下は、Stable Diffusionのアニメ画像を出力できるモデルで、黒髪の少女を出力した例です。\nStable Diffusionを利用する方法 Stable Diffusionを利用方法は大きく以下の2種類に分かれます。\nHugging Face, Dream Studio などのwebアプリケーションを利用する ローカルでStable Diffusion Web UIを利用する 本記事では、ローカルでStable Diffusion Web UIを利用する方法を紹介します。 試しに使って見るだけであれば、webアプリケーションを利用するのが簡単ですが、画像を大量に生成する場合は制限があったり、費用がかかったりするので、 ある程度の量を生成する場合はローカルで利用することをおすすめします。\nStable Diffusion Web UIをローカルで利用する方法 今回はAUTOMATIC1111氏が公開している、stable-diffusion-web-uiを利用します。\n動作する環境を整える stable-diffusion-web-uiをインストールする モデルファイルを配置する stable-diffusion-web-uiを起動し画像を生成する 1. 動作する環境を整える まずは、ローカルで動作させるにあたって、Pythonやその他ライブラリが必要であるため、homebrewを利用してインストールします。\nhomebrewのインストール\n1 /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; homebrewのパスを通す\n1 export PATH=\u0026#34;$PATH:/opt/homebrew/bin/\u0026#34; 関連ライブラリのインストール\n1 brew install cmake protobuf rust pyenv git wget pyenvを用いて、Python環境のセットアップ。複数のPythonのバージョンを使い分けられるようにします。 このあとにvenvも登場します。pythonの環境の構築についてはこちらの記事を参考にしてください。\n1 2 pyenv install 3.10.6 pyenv local 3.10.6 2. stable-diffusion-web-uiをインストールする git cloneでリポジトリをクローンします。\n1 2 git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git cd stable-diffusion-webui venvで仮想環境を設定、他の環境を汚さないようにします。\n1 2 python -m venv venv source venv/bin/activate これで環境構築は完了です。\n3. モデルファイルを配置する 次にモデルファイルをダウンロードしてきて、stable-diffusion-webui/models/Stable-diffusion/ ディレクトリに配置します。\nモデルファイルは以下のサイトからダウンロードできます。\nCivitai Hugging Face 今回はCivilaiから bule_pencil のモデルをダウンロードしてきて利用してみます。\nCivilaiの検索窓にbule_pencilと入力し、検索します。\n検索結果からbule_pencilを選択し、Downloadボタンをクリックします。\nダウンロードしたモデルをディレクトリに移します。\n1 mv ~/Downloads/bluePencilXL_v401.safetensors models/Stable-diffusion/ 4. stable-diffusion-web-uiを起動し画像を生成する 最後に、stable-diffusion-web-uiを起動し、画像を生成します。\n1 ./webui.sh 起動したらプロンプトにテキストを入力し、画像を生成します。\nStable Diffusion checkpointで先程ダウンロードしたbule_pencilを選択します。 promptに生成したい画像の要素を入力します Negative promptに生成してほしくない画像の要素を入れます。 Generateをクリックします。 ピンク色のサングラスをかけた犬という意味のテキストを入力してみたところ、ちゃんと出力されました。\nもちろん、他のテキストを入力することで、様々な画像を生成することができます。\nまとめ この記事ではMacにStable Diffusion Web UIをインストールし、ローカルで利用する方法を紹介しました。 ローカルで利用することで、制限があったり、費用がかかったりするwebアプリケーションを利用するよりも、自由に画像を生成することができます。\n","date":"2024-02-12T11:24:59+09:00","permalink":"https://bossagyu.com/blog/019-stable-diffusion/","title":"MacでStable Diffusion Web UIを使う方法"},{"content":"概要 ITIL v4の事業分析について、学習し理解した内容をまとめます。\nまた、理解した内容をもとに自分の経験を当てはめ、事業分析のプロセスについて説明します。\n事業分析とは 事業分析とは、事業または他の何らかの要素を分析し、それらのニーズへの対応やビジネス上の課題を解決するためのソリューションを提案するプロセスです。\n「事業」分析とされていますが、対象は事業だけでないことに注意が必要です。\n事業分析手法の例 事業分析手法の例として以下のようなものがあります。\nSWOT分析 ユーザーストーリー 具体的ない方法についてはここでは本題ではないため他のサイトに譲ります。\n私は例には挙げられていませんが、カスタマージャーニーマップをよく使います。\n事業分析のプロセス 事業分析のプロセスは以下の２つのプロセスがあります。\n事業分析アプローチの設計と維持 事業分析とソリューションの特定 事業分析アプローチの設計と維持 このプロセスの焦点は、組織の現在および予想されるニーズに対処することにより、ビジネス分析に対する一貫した効果的なアプローチを確立することです。\n以下の流れで実行されます。\n組織と要件を分析する ビジネス分析のアプローチ手法のレビュー ビジネス分析アプローチを実行する 私が所属する組織ではあまり、事業レイヤーレベルの分析を行うことはないですが、一般的に要件・要求があればそれを特定の手法で分析しましょうというお話であると理解しています。\n事業分析とソリューションの特定 このプロセスは、利害関係者のニーズと要件を分析することに重点が置かれています。 分析の結果から利害関係者のニーズと要件に対処するためのソリューションの特定と提案が含まれています。\n以下の流れで実行されます。\nステークホルダーからの情報収集と分析 ソリューションのオプションを定義し、推奨されるソリューションを特定する ソリューション提供チームへのサポートの提供 ソリューションのパフォーマンスと評価 上記活動について、私の経験を当てはめると以下のように理解しました。\n1,2について ステークホルダーから情報を収集、得た情報の分析を行う。 分析結果から解決すべき課題(why)を特定、課題に対してどのような解決策があるか(what)を決定する。 3について 1,2で特定されたwhy, whatに対して、どのような解決策(how)があるかをプロダクトチームと一緒に考える この際に解決手法とともに評価基準をきめ、どのような変化があればこのソリューションが成功したと言えるかを決定する。 またこの際に効果はどの程度の時間軸で現れるのかの認識を合わせておくとよい。 4について 定めた評価基準に対して、ソリューションがどの程度達成されているかを定期的に評価する。 数値については手動集計だと見なくなるので、個人的には自動化してGrafanaなどで可視化することをおすすめします。 まとめ 今回は事業分析について学習した内容に基づき私の経験を当てはめ説明しました。\n個人的な理解としては事業分析といいつつ事業だけを対象としないこと、分析といいつつ分析したあとのプロセスも含まれていることが理解できました。\n参考 Business analysis management: ITIL 4 Practice Guide 関連記事 ITIL v4 可用性管理について解説（関連ITIL v4記事） ITIL v4 キャパシティ及びパフォーマンス管理について解説（関連ITIL v4記事） ","date":"2024-02-09T09:00:56+09:00","permalink":"https://bossagyu.com/blog/018-itilv4-business-analysis/","title":"ITIL v4 事業分析について解説"},{"content":"概要 この記事では、VSCodeでGithub Copilotを設定して使う方法から、Markdownでの活用、Chat toolの使い方まで網羅的に説明します。 前提としてGithub Copilotのアカウントが必要です。\nVSCodeでGithub Copilotを使えるようにするまで 拡張機能をインストール まずは、VSCodeに拡張機能をインストールします。 VSCode を開き、左メニューの四角形が4つあるアイコンをクリックし、検索用テキスト入力に「copilot」と入力します。 「install」をクリックし、インストールを開始してください。\nGitHubとの連携 installをクリックし、installが完了すれば以下のような画面が表示されるので、「Sign in to GitHub」をクリックします。\nGitHubのアカウントへのアクセスを要求されるので「Allow」で許可します。\n「Authorize Visual Studio Code」をクリックし、許可します。\nこれでGithub CopilotとVSCodeの連携が完了し、使えるようになりました。\n使い方 基本的には、コードを書いていくだけで自動的に補完されるようになります。 補完内容が提案されるので以下のショートカットを使いながらコードを書いていくと良いでしょう。\nチートシート 機能 キー 提案を受け入れる Tab 提案を拒否する Esc Copilotを開く Ctrl + Enter 次の提案 Alt/Option + ] 前の提案 Alt/Option + [ インラインCopilotをトリガーする Alt/Option + \\ Markdownファイルでの補完を有効にする Github Copilotは、デフォルトではMarkdownファイルでの補完が無効になっています。 ブログ執筆などでMarkdownを使う場合は、有効化しておくと便利です。\n設定方法 VSCodeでGitHub Copilotのプラグインのページを開きます 歯車アイコンをクリックし、設定を開きます 設定画面で markdown が false になっているので true に変更します これでMarkdownファイルでもCopilotの補完が効くようになります。 このブログもGithub Copilotで補完を行いながら書いており、かなり効率化できています。\nChat toolを使ってさらに便利に Github Copilot Chatには「Chat tool」という機能があり、チャット内で様々なタスクを実行できます。\nChat toolの実行方法 基本的には #\u0026lt;command\u0026gt; \u0026lt;args\u0026gt; の形式でコマンドを入力します。\n代表的なChat toolのコマンド コマンド 機能 #codebase 現在のワークスペースを全部検索する #selection 現在のエディタで選択しているコードをコンテキストとしてプロンプトに追加する #terminal_selection 現在のターミナルで選択している部分をコンテキストとしてプロンプトに追加する。エラーが出たときにターミナルの出力を参照するのに便利です。 #fetch_webpage URLを指定してwebページからコンテンツを取得してコンテキストとしてプロンプトに追加する 他にも便利なコマンドがあるので、公式ドキュメントを参照してください。\nGithub Copilot Chat tools まとめ VSCodeでGithub Copilotを設定して使う方法について説明しました。\n基本設定: 拡張機能のインストールとGitHub連携 ショートカット: Tab/Escで提案の受け入れ・拒否 Markdown対応: 設定でmarkdownをtrueに変更 Chat tool: #codebaseや#selectionでコンテキストを追加 Github Copilotはソースコードだけではなく文章にも補完を行ってくれます。 ぜひ活用してみてください。\n関連記事 IntellijでのGithub Copilotの使い方 GitHub CopilotをMarkdownで有効にする方法 Github CopilotをChat toolを使って便利に使う方法 Codex CLIの使い方まとめ ","date":"2024-02-04T22:34:51+09:00","permalink":"https://bossagyu.com/blog/017-vscode-copilot/","title":"VSCodeでGithub Copilotを使いこなす完全ガイド"},{"content":"概要 この記事では、ITIL v4の可用性管理について説明します。 また、理解した内容をもとに自分の経験を当てはあめ、可用性管理のプロセスについて説明します。\n可用性管理とは 可用性管理とは、サービスの可用性を確保するための活動のことです。 可用性管理の目的は、サービスが顧客とユーザーのニーズを満たすために合意されたレベルの可用性を確実に提供することです。\n可用性管理のプロセス 可用性管理のプロセスは以下２つがあります。\nサービス可用性制御の確立 サービス可用性の分析と改善 サービス可用性制御の確立 サービス可用性制御の確立は、サービスの可用性を確保するための活動のことです。 以下の流れで実現されます。\nサービス可用性要件の特定 サービス可用性要件の合意 可用性測定要件の決定 可用性メトリクスと報告の設計 上記プロセスに対して自分の経験を当てはめると以下のように理解しました。\nサービス可用性要件の特定 どのような利用者がおり、サービスが停止したときの事業リスクなどの影響を特定する。 自分のサービスは社内のPFなので、各サービスがPFとして利用しておりそれぞれどのような影響があるかを特定しました。 サービス可用性要件の合意 SLAの形でサービスの可用性（稼働率99%）などを合意する 稼働率では停止判定の基準や、例外事由なども明らかにしました。 可用性測定要件の決定 測定要件については可用性要件合意の段階で何をサービス停止とするかを決めているので、測定要件は特になし。 可用性メトリクスと報告の設計 基本的には「ダウンタイム/稼働時間」で設計 報告については、可用性メトリクスを可視化するためのダッシュボードを作成しました。 サービス可用性の分析と改善 サービス可用性の分析と改善は、その名の通り可用性の分析と改善を行うプロセスです。 以下の流れで実現されます。\nサービス可用性の分析 サービス可用性の報告 サービス可用性の計画と設計 上記プロセスに対して自分の経験を当てはめると以下のように理解しました。\nサービス可用性の分析 サービス可用性が達成されていることを確認し、集計します。 サービス可用性の報告 可用性をダッシュボードに反映、誰でも見られる状態にします。 サービス可用性の計画と設計 可用性を割るような障害が発生した場合、再発防止のための計画を立てました。 参考 Availability management: ITIL 4 Practice Guide 関連記事 ITIL v4 事業分析について解説（関連ITIL v4記事） ITIL v4 キャパシティ及びパフォーマンス管理について解説（関連ITIL v4記事） ","date":"2024-01-30T20:34:58+09:00","permalink":"https://bossagyu.com/blog/016-itilv4-availability-management/","title":"ITIL v4 可用性管理について解説"},{"content":"概要 PythonでS3のオブジェクトが存在するかどうかを確認する方法を説明します。 boto3にはresourceとclientの2つのAPIがあり、それぞれの方法を紹介します。\n実際にLINE Botの開発で、ユーザーごとの設定ファイルがS3に存在するかチェックする必要があり、この方法を使いました。\nresource vs client どちらを使うべきか boto3には2つのAPIレベルがあります。\nAPI 特徴 適したユースケース resource 高レベルAPI、オブジェクト指向 シンプルな操作、可読性重視 client 低レベルAPI、AWS APIに近い 細かい制御、パフォーマンス重視 結論: 単純な存在確認ならclientのhead_objectがおすすめです。理由は以下の通りです。\nhead_objectはオブジェクトのメタデータのみを取得するため軽量 resourceのload()も内部的にはhead_objectを呼んでいる clientの方がAWS APIに近く、挙動が明確 boto3.clientを利用する方法（推奨） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import boto3 from botocore.exceptions import ClientError def check_s3_object_exists(bucket_name: str, object_key: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;S3オブジェクトの存在確認\u0026#34;\u0026#34;\u0026#34; s3 = boto3.client(\u0026#39;s3\u0026#39;) try: s3.head_object(Bucket=bucket_name, Key=object_key) return True except ClientError as e: error_code = e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] if error_code == \u0026#39;404\u0026#39;: return False # 404以外のエラー（権限不足など）は再raise raise # 使用例 if check_s3_object_exists(\u0026#39;my-bucket\u0026#39;, \u0026#39;path/to/file.json\u0026#39;): print(\u0026#34;オブジェクトが存在します\u0026#34;) else: print(\u0026#34;オブジェクトが存在しません\u0026#34;) ポイント head_objectはオブジェクトの内容をダウンロードせず、メタデータのみを取得 404エラー以外（403: アクセス拒否など）は別のエラーなので再raiseする 関数化しておくとテストしやすい boto3.resourceを利用する方法 オブジェクト指向的に書きたい場合はこちらも使えます。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import boto3 from botocore.exceptions import ClientError def check_s3_object_exists_resource(bucket_name: str, object_key: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;S3オブジェクトの存在確認（resource版）\u0026#34;\u0026#34;\u0026#34; s3 = boto3.resource(\u0026#39;s3\u0026#39;) try: s3.Object(bucket_name, object_key).load() return True except ClientError as e: error_code = e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] if error_code == \u0026#39;404\u0026#39;: return False raise よくあるハマりポイント 1. 権限不足を404と間違える S3バケットへのアクセス権限がない場合、403エラーが返ります。 404だけをキャッチして「存在しない」と判断すると、権限の問題を見逃します。\n1 2 3 4 5 6 7 8 9 # NG: 404以外のエラーを握りつぶしている except ClientError: return False # 権限エラーも「存在しない」扱いになる # OK: 404以外は再raise except ClientError as e: if e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] == \u0026#39;404\u0026#39;: return False raise # 権限エラーなどは呼び出し元で処理 2. list_objectsを使う方法は非効率 list_objectsでプレフィックス検索する方法もありますが、オブジェクト数が多いと遅くなります。 単一オブジェクトの存在確認にはhead_objectを使いましょう。\nまとめ S3オブジェクトの存在確認にはhead_objectを使う 404エラー以外は権限不足などの可能性があるので適切に処理する resourceとclientどちらでも可能だが、clientの方がシンプル 参考 boto3 S3 head_object Stack Overflow: check if a file exists in s3 bucket using boto3 関連記事 AWS Lambda + LINE Botで掃除リマインダーを作る Pyenvとvenvを用いたローカル環境のセットアップ方法（Python環境構築） MacでUVを用いてPythonの開発環境を構築する（モダンなPython環境構築） ","date":"2024-01-27T21:41:37+09:00","permalink":"https://bossagyu.com/blog/015-s3-object-check/","title":"S3のオブジェクトの存在確認をする方法【Python boto3】"},{"content":"概要 AWS API GatewayとLambdaを連携させることで、API GatewayからLambdaを呼び出すことができます。 本記事AWS API GatewayとLambdaを連携させる方法を紹介します。\n前提条件 Lambda関数については作成されていることを前提としています。 作成していない場合は、下記記事を参考に作成してください。\nAWS Lambdaを作成する方法 どの形式でAPI GatewayとLambdaを連携させるかを考える API GatewayとLambdaを連携させるに当たってどのような方式で連携させるか以下の2点について考える必要があります。\nAPI Gatewayのリクエスト形式について プロキシ統合が非プロキシ統合か API Gatewayのリクエスト形式について 以下の形式から選択できます。\nREST API HTTP API WebSocket API この内、REST APIの形式で利用する場合は、REST APIかHTTP APIのどちらかを選択することになります。\nREST APIの方が機能が多いですが、HTTP APIと比べコストがかかります。\n特に複雑なことをしないのであればHTTP APIを選択するのが良いと思います。\n詳細な比較内容については公式ドキュメントで公開されているのでそちらを参照してください。\nプロキシ統合が非プロキシ統合か プロキシ統合を利用することで、Lambdaから返される値のフォーマットが固定化されます。 基本的にはプロキシ統合を利用することをおすすめします。\nAPI Gateway で Lambda プロキシ統合を設定する 設定 Lambda関数を作成したら、トリガー追加を選択します。\nAPI Gatewayを選択します。\nトリガーを追加の画面で以下のように設定を行います。 設定がうまくいくと以下のような画面になります。\nAPI endpoint に記載のエンドポイントにcurlなどでアクセスするとLambda関数が実行されます。\n1 2 3 $ curl https://xxxxxxxxx.execute-api.ap-northeast-1.amazonaws.com/default/apigateway-get-sample \u0026#34;Hello from Lambda!\u0026#34;% まとめ 本記事ではAWS API GatewayとLambdaを連携させる方法を紹介しました。 API Gatewayと連携させることで外部から任意のタイミングでLambda関数を呼び出すことができるようになります。\n関連記事 IntellijでAWS Toolkitを使ってLambdaを効率よく開発する AWS EventBridgeを用いてLambdaを定期実行する方法 AWS Lambda + LINE Botで掃除リマインダーを作る ","date":"2024-01-13T18:06:52+09:00","permalink":"https://bossagyu.com/blog/014-aws-apigateway-lambda/","title":"AWS API GatewayとLambdaを連携させる方法"},{"content":"概要 良いプロダクト戦略の作り方について、「良い戦略、悪い戦略」という書籍をベースにまとめました。\n背景 業務でプロダクトオーナーとして、前プロダクトオーナーからプロダクトを引き継ぎました。\n引き継いだプロダクトにはプロダクト戦略がなく、プロダクトの方向性が定まっていない状態であったため、今後プロダクトをどうするかを含めて、プロダクト戦略を作成しました。\nプロダクト戦略を作成するにあたって、そもそも「戦略」というものが人によって様々な意味に解釈されており、なんでも「戦略」という言葉を使ってしまう傾向があると常々感じていました。 そこで、プロダクト戦略を作成するにあたって、どのような戦略を作成すれば良いのかを知るために、「良い戦略、悪い戦略」を読みました。\n良い戦略とは 良い戦略とは、こちらの打つ手の効果が一気に高まるようなポイントを見つけ、そこに狙いを絞って手持ちのリソースを集中させることで、効果を最大化することができる戦略です。 戦略とは組織が前に進むためにどのようにしたら良いかを示すものである必要があります。\n良い戦略は、以下の3つの基本構造を持っています。\n診断 基本方針 行動 診断 診断とは、状況を診断し取り組むべき課題を見極めることです。\n良い診断は死活的に重要な問題点をより分け、複雑に絡み合った状況を明快に整理することができます。\n戦略を立てる作業の多くは、今何が置きているのかを洗い出すことにあります。まずは情報を集めることが何よりも大切です。\n本書ではコンサルが提案するようなフレームに割り当てるだけではまともな戦略はできない。とこき下ろしていますが情報をしっかりと集めた上でフレームに割り当て整理することを診断の段階で行うことは有意義だと私は考えています。\n実際に私がプロダクト戦略を作成するにあたって、SWOT分析でプロダクトの置かれる状況を、インパクトマッピングを用いて現状の施策が一体誰にどのような影響を与えるのかの整理を行いました。\nそれぞれのやり方については、以下の記事や書籍を参照してください。\nSWOT分析 インパクトマッピング 基本方針 基本方針とは、診断で見つかった課題にどう取り組むか、大きな方向性と総合的な方針を示すことです。\n良い基本方針とは、目標でもビジョンではない、何曲に立ち向かう方法を固め、他の選択肢を排除することが基本方針である。と本書では述べられていました。\n決定的な一点に努力を集中させることによって、大きな効果を上げることができます。\nこのため良い戦略の中には、その戦略に従うことによって何に対してリソースを割くのかがはっきりと分かるようになっていることが大切です。\n私が戦略を立てた際には、インパクトマッピングで整理した内容をベースに、プロダクトの置かれた現状やビジョンをもとに勘案し、プロダクトの方向性を決めました。\nリソースを選択する意味では、どのターゲットのセグメントに対して、どのような価値を提供するのかを明確にすることを意識しました。\n行動 基本方針を実行するために設計された一貫性のある一連の行動のことです。 戦略が存在することですべての行動をコーディネートして方針を実行することができます。\nこのため、良い戦略は、行動を実行するための指針が含まれていることが大切となります。\n私が立てた戦略 上記をベースに私が立てた戦略は以下となりました。\n会社で立てた戦略ですので、一部ぼかして書いています。\n1 xx機能の利用者の新規利用コストを低減する かなりシンプルなものになりましたが、以下のように良い戦略の3つの基本構造を満たしていると考えています。\n診断 プロダクトの状況、課題をベースに考えたときにxxの新規利用者を増やすことが会社の利益につながると考えました。 基本方針 新規利用者を確保するために導入コストを下げるという基本方針を定めました。 行動 基本方針を実現するためにいくつかのアプローチを用意し優先順位をつけました。 ここについては行動を連想できるようなワードを戦略に含められればと考えましたが上記対応としました。 悪い戦略とは 最後に陥りがちな悪い戦略のパターンについて書いておきます。\n悪い戦略の特徴\n空疎である わかり切っていることを専門用語や業界用語で煙に巻くような内容 重大な問題に取り組まない 本来困難な課題を克服し、障害を乗り越えるためのものが戦略である。 達成容易性のみを考えた戦略は悪い戦略である。 目標と戦略と取り違えている 売りあげ10%向上など。それはただの目標である。 間違った戦略目標を掲げている 十分な周辺・原因の調査がなく戦略を掲げている状態。 まとめ 今回は、良いプロダクト戦略の作り方について、「良い戦略、悪い戦略」という書籍をベースにまとめました。 一回でいきなり良い戦略は立てられないと考えています。ただ、戦略のない環境はただ闇雲に走っているだけなのでそれが成功したのか失敗したのかすら判断することができない最低の状態です。\n最初は下手な戦略でも良いので、良い戦略とは何かを意識しつつ常に周りの状況を観察しながら、戦略をアップデートしプロダクトと組織の方向性を定めていくことが大切だと思います。\n","date":"2024-01-08T21:55:15+09:00","permalink":"https://bossagyu.com/blog/013-good-strategy-bad-strategy/","title":"プロダクト戦略の作り方"},{"content":"概要 この記事では、Hugoで作ったブログにTwitter Social Cardを設定する方法を説明します。\nTwitter Social Cardとは Twitter Social Cardとは、Twitterで記事をシェアした際に表示される画像のことです。 以下のような画像がTwitter Social Cardです。\nTwitter Social Cardは、以下の種類があります。\nSummary Card Summary Card with Large Image App Card Player Card この中でもブログのシェアを行う場合は、Summary Card か Summary Card with Large Image を利用することが多いです。\nそのれぞれのカードがどのようなものかについてはTwitterの公式ドキュメントを参照してください。\nTwitter Social Cardの設定方法 Twitter Social Cardの設定方法は、以下の2つの方法があります。\nテーマによる設定 テーマに依存しない設定 テーマによる設定 テーマによっては、Twitter Social Cardの設定を行うことができます。\n今回は私が採用しているStackを例に説明します。\nStackでは、config.toml 対して以下のような設定を行うことでTwitter Social Cardの設定が可能です。\n1 2 3 4 5 6 7 8 [opengraph.twitter] site = \u0026#34;\u0026#34; card = \u0026#34;summary\u0026#34; # summary or summary_large_image [defaultImage.opengraph] enabled = true local = false src = \u0026#34;/images/share.webp\u0026#34; # デフォルトで設定したいimageのパス テーマに依存しない設定 テーマによっては、Twitter Social Cardの設定を行うことができません。 自前で実装して、設定を行う必要があります。\nHugoの公式で実装のテンプレートが公開されていますので、それを利用することで容易に実装することが可能です。\nうまくいかない場合 うまくいかない場合はうまく設定が反映されていない、megaタグが正しく設定されていない可能性があります。 Twitterから提供されているデバッグツールを利用して、設定が正しく反映されているか確認してみましょう。\nまとめ この記事では、Hugoで作ったブログにTwitter Social Cardを設定する方法を説明しました。 Social Cardを設定することで、Twitterで記事をシェアした際に、より多くの人に記事を読んでもらうことができますのでぜひ設定しましょう。\n関連記事 Hugo + Netlify + Githubでブログを公開する - ブログの初期構築 Google Search Consoleでブログを検索対象にする方法 - SEO対策 ","date":"2024-01-06T21:45:12+09:00","permalink":"https://bossagyu.com/blog/012-social-card/","title":"Twitter Social Cardの設定方法"},{"content":"概要 この記事では、ChatGPTを利用してHugoで作ったブログを多言語対応する方法を説明します。\nGhatGPTを利用して記事を英語化する方法 Markdownで書かれた記事をChatGPTに英語化させます。 英語化を実施するにあたって、なるべき体裁を崩さないようにするために、以下のようなプロンプトをChatGPTへ入力します。\n1 2 3 マークダウンを体裁を崩さずに英語にしてください。 マークダウン以外の余分な出力は行わないでください。 英語化された内容をそのままコピーできる形で出力してください。 その後、日本語で書いた記事をそのまま貼り付けると、英語化されたMarkdownが出力されます。 出力結果の左下のコピーボタンから出力結果をコピーすれば、英語化は完了です。\nまた、ChatGPTを利用する際はGPT3.5ではなく、課金をしてGPT4を利用することを強くおすすめします。 GPT4は月々お金がかかりますが、GPT3.5と比べて圧倒的に正しい回答を返す確率が高いので、英語化以外の用途にも使えます。\nHugoで多言語対応する方法 Hugoで多言語化を行うための設定方法を記載します。\n設定ファイルの作成 config.tomlに以下のような設定を追加します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # デフォルトの言語を設定、設定しない場合英語がデフォルト判定されます。 defaultContentLanguage = \u0026#34;jp\u0026#34; [languages] # 言語ごとに設定を変更します [languages.jp] title = \u0026#34;Bossagyu Blog\u0026#34; languageName = \u0026#34;ja-jp 🇯🇵\u0026#34; LanguageCode = \u0026#34;ja-jp\u0026#34; contentDir = \u0026#34;content\u0026#34; # 日本語ブログ記事を格納するディレクトリ [languages.jp.params] [languages.en] title = \u0026#34;Bossagyu Blog\u0026#34; languageName = \u0026#34;en-US 🇺🇸\u0026#34; LanguageCode = \u0026#34;en-US\u0026#34; contentDir = \u0026#34;content.en\u0026#34; # 英語ブログ記事を格納するディレクトリ [languages.en.params] 上記のような設定を行うことで、contentディレクトリに日本語の記事を、content.enディレクトリに英語の記事を書くことで多言語対応が可能となります。\n最終的なディレクトリ構成は以下のような形になります。\n1 2 3 4 5 6 7 8 9 10 11 project/ ├── content/ │ ├── index.md │ └── blog/ │ ├── index.md │ └── article1.md └── content.en/ ├── index.md └── blog/ ├── index.md └── article1.en.md また英語化したディレクトリの中には 記事名.en.md の拡張子とすることで、デフォルト言語に対する英語の記事として認識され、記事に言語変換用のアイコンが出力されるようになります。 記事についてはGhatGPTを用いて英語化したものをコピー\u0026amp;ペーストするだけでOKです。\nまとめ この記事では、ChatGPTを利用してHugoで作ったブログを多言語対応する方法を説明しました。 ChatGPTを利用することで、英語化を簡単に行うことができます。 また、多言語対応することで記事を日本以外の国にもリーチでき、より多くの人に記事を読んでもらえます。\nローコストで多言語対応できるのでぜひやってみましょう。\n関連記事 Hugo + Netlify + Githubでブログを公開する - ブログの初期構築 ","date":"2023-12-31T20:46:36+09:00","permalink":"https://bossagyu.com/blog/011-hugo-multilingul-support/","title":"ChatGPTを利用してHugoで作ったブログを多言語対応する方法"},{"content":"概要 Lighthouseを用いて、ブログのパフォーマンスを計測する方法を解説します。\nLighthouseとは LighthouseはGoogleが提供している、Webサイトのパフォーマンスを計測するツールです。 Google Chromeの拡張機能として提供されており、プラグインをインストールすることで利用することができます。\nLighthouseのインストール Lighthouseをchromeウェブストアからインストールします。\n分析したいサイトを開き、Lighthouseのアイコンをクリックします。\nGenerate reportをクリックすると、分析が始まります。\n今回は私のブログのページで実行しました。\n実行すると、以下のような結果が表示されます。\n実行の完了までに約1分くらい時間がかかります。\n結果の見方 Performance ページの読み込み速度や画像の表示速度など、webサイトのパフォーマンスが評価される。 See calculatorのリンクをクリックすると詳細に飛べる。\nAccessibility すべてのユーザーがコンテンツにアクセス、サイト内を効率的に移動できるかどうかを確認する。 スクロールすると、Accessibilityで指摘されている箇所が表示される。\nコードスニペットを貼っているところの色のコントラストが弱いことと、リンクに説明がないことを指摘されています。\nただ、指摘されている内容は自分の記述ではなく、テンプレートに依存している部分なので、これを直そうと思うとHugoのテンプレートをオーバーライドする必要がありますね。。\nBest Practices ウェブページの健全性についてテストを行います。 検証項目については結果から閲覧できます。 SEO ページが検索エンジンの結果ランキング向けに最適化されているかを確認できます。 Progressive Web App スマートフォン上のウェブページの読み込み速度を高速化できているか、PWAに最適化できているかを確認できます。 今回はチェックしてません。\nまとめ Lighthouseを用いて、ブログのパフォーマンスを計測する方法を解説しました。 特にSEOについては、Googleの検索結果に表示されるかどうかに影響するので、しっかりと対応しておきましょう。\n関連記事 Hugo + Netlify + Githubでブログを公開する - ブログの初期構築 HugoでGoogle Analyticsの設定をする方法 - アクセス解析 ","date":"2023-12-22T23:08:00+09:00","permalink":"https://bossagyu.com/blog/009-light-house/","title":"Lighthouseの使い方の紹介"},{"content":"概要 AWS EventBridgeを用いてLambdaを定期実行する方法を解説します。\n私はこの仕組みを使って、LINE Botに定期的にリマインド通知を送る機能を実装しました。 たとえば「毎朝7時に掃除のリマインドを送る」といった処理を、サーバーを用意せずに実現できます。\nAWS EventBridgeとは AWS EventBridgeは、AWSのサービス間でイベントを受け渡すためのサービスです。 主な用途は以下の2つです。\n用途 説明 スケジュール実行 cron式やrate式で定期的にLambdaを実行 イベント駆動 S3へのファイルアップロードなどをトリガーにLambdaを実行 今回はスケジュール実行の方法を解説します。\n実際のユースケース EventBridge + Lambdaの定期実行は、以下のようなケースで役立ちます。\nリマインド通知: 毎日決まった時間にSlackやLINEに通知 バッチ処理: 毎時データを集計してDBに保存 ヘルスチェック: 5分ごとに外部APIの死活監視 クリーンアップ: 毎日深夜に古いログを削除 私の場合は、掃除リマインダーBotで毎時ユーザーへの通知判定を行うために使っています。\n前提 Lambda関数についてはすでに作成されていることを前提としています。 Lambda関数の作成方法については、AWS Lambda 開始方法 を参照して作成してください。\n手順 1. トリガーを追加 EventBridgeで実行する予定のLambda関数を選択し「トリガーを追加」を選択します。\n2. EventBridgeを選択 トリガーから「EventBridge (CloudWatch Events)」を選択します。\n3. スケジュールを設定 トリガーの選択を行うと、ルールの作成画面が表示されるので設定します。\n4. 設定完了 設定が完了するとLambda関数のダイアグラムのトリガーにEventBridgeが追加されます。\ncron式の書き方 EventBridgeのcron式は6つのフィールドで構成されます。\n1 cron(分 時 日 月 曜日 年) よく使うパターン 実行タイミング cron式 毎日9時（UTC） cron(0 9 * * ? *) 毎日9時（JST = UTC+9） cron(0 0 * * ? *) 毎時0分 cron(0 * * * ? *) 5分ごと cron(0/5 * * * ? *) 平日のみ毎朝9時（UTC） cron(0 9 ? * MON-FRI *) 毎月1日の0時（UTC） cron(0 0 1 * ? *) 注意: タイムゾーンはUTC EventBridgeのcron式はUTC基準です。日本時間（JST）で設定したい場合は9時間引いてください。\n例: 毎朝7時（JST）に実行 → cron(0 22 * * ? *) （前日の22時UTC）\n実際に動かしてみた結果 私はLINEにメッセージを通知するFunctionを作って動かしてみました。 5分に1回通知がくるようになりました。\nテストの際は短い間隔（5分など）で設定して動作確認し、本番では適切な間隔に変更するのがおすすめです。\nコストについて EventBridge + Lambdaの組み合わせは非常に低コストです。\nサービス 無料枠 超過時の料金 EventBridge 無料 スケジュールルールは無料 Lambda 月100万リクエスト無料 $0.20/100万リクエスト 個人開発のリマインダーBot程度なら、ほぼ無料で運用できます。\nハマりポイント 1. タイムゾーンの罠 前述の通り、EventBridgeはUTC基準です。「毎朝9時」と設定したつもりが「夕方18時」に動いていた、というのはよくある失敗です。\n2. 初回実行のタイミング cron式を設定した直後は、次のスケジュールまで待つ必要があります。 すぐにテストしたい場合は、Lambdaのテスト機能を使いましょう。\n3. 削除を忘れずに テスト用に作成したEventBridgeルールを放置すると、Lambdaが実行され続けます。 不要になったルールは必ず削除しましょう。\nまとめ AWS EventBridgeを用いてLambdaを定期実行する方法を解説しました。\nEventBridgeはスケジュール実行とイベント駆動の2つの用途がある cron式はUTC基準なので、JSTで設定したい場合は9時間引く 個人開発レベルならほぼ無料で運用可能 不要になったルールは削除を忘れずに サーバーレスで定期実行を実現できるので、リマインダーBotやバッチ処理などに活用してみてください。\n関連記事 AWS API GatewayとLambdaを連携させる方法 掃除リマインダーBotの使い方 掃除リマインダーBotの技術解説 ","date":"2023-12-21T23:03:13+09:00","permalink":"https://bossagyu.com/blog/008-aws-eventbrdge/","title":"AWS EventBridgeを用いてLambdaを定期実行する方法"},{"content":"概要 Google検索で引っかかるようにするために、SEO対策を行いましょうとありますが、まずはGoogle検索に認識されないと話になりません。 この記事では、Google Search Consoleを用いて自身が作成した独自ドメインのブログが、Google検索の対象となる方法を解説します。\n実現までの流れ Google Search Consoleの登録 ドメインの所有権の確認 サイトマップの登録 インデックス登録をリクエスト まとめ Google Search Consoleの登録 Google Search Consoleに登録します。\nドメインを選択肢、URLを入力します。\nドメインの所有権の確認 以下のような画像が表示されDNSの所有権を確認します。\n(念のためTXTレコードの内容は黒く塗りつぶしています。) ドメインのTXTにGoogleが指定した文字列を追加することで、所有権を確認することができます。 ドメインのDNSの設定画面に移動し、TXTレコードを追加します。\n私の場合はNetlifyでドメインを取得しているので、NetlifyのDNSの設定画面に移動します。 Domains -\u0026gt; Domain Settings -\u0026gt; DNS Records に移動し、TXTレコードを追加します。 レコードの内容はGoogle Search Consoleに表示されているものをコピーしてValueに貼り付けます。\n(Valueの部分は黒く塗りつぶしています。) DNSの反映を待ちます。ものによっては数時間かかる場合があります。\nDNSの反映はコマンドラインからでも確認できます。\n1 dig -t txt bossagyu.com その後、Google Search Consoleの所有権の確認を押します。\nこれで所有権の確認が完了し、Google Search Consoleにドメインが登録されます。\nサイトマップの登録 サイトマップを登録することで、Googleにサイトの構造を伝え、サイトのクロールを促進することができます。 Hugoで作成したブログの場合は、/sitemap.xmlにサイトマップが作成されているので、これを登録します。\nGoogle Search Consoleの左側のメニューから「サイトマップ」を選択し、サイトマップを追加します。 インデックス登録をリクエスト サイトマップに登録されていても、Googleがクロールして、その後インデックスが登録されるまでには時間がかかります。 私の場合は数日待ってもインデックスが登録されなかったので、インデックス登録をリクエストしました。\nGoogle Search Consoleの検索窓で登録したいURLを検索し、検索結果の右側にある「インデックス登録をリクエスト」を押します。 これでインデックス登録をリクエストできます。 クリックしてから数時間でインデックスが登録されました。\nまとめ Google Search Consoleを用いて自身が作成した独自ドメインのブログが検索に引っかかるようにする方法を解説しました。\nせっかくブログを作成したのに、Google検索に引っかからないのはもったいないので、ぜひ試してみてください。\n関連記事 Hugo + Netlify + Githubでブログを公開する - ブログの初期構築 HugoでGoogle Analyticsの設定をする方法 - アクセス解析 Lighthouseの使い方の紹介 - パフォーマンス計測 ","date":"2023-12-18T19:10:04+09:00","permalink":"https://bossagyu.com/blog/007-google-search-console/","title":"Google Search Consoleを用いてブログをGoogle検索の対象にする方法"},{"content":"概要 IntellijでAWS Toolkitを使ってLambdaを効率よく開発する方法を解説します。\n実現までの流れ 事前準備 AWS Toolkitのインストール AWS Toolkitの設定 Lambdaの開発 Lambdaをローカルで実行 まとめ 事前準備 dockerのインストール intellijで利用するAWS Toolkitでは、Lambda動作させるためにDockerを使用します。\nこのため事前に こちらを参考にDockerをインストールしておいてください。\nAWS CLIのインストール AWS CLI(SAM)をインストールします。\nインストール方法は こちら を参考にしてください。\nIntellijにSAM CLI executableのパスを File -\u0026gt; Settings -\u0026gt; Tools -\u0026gt; AWS Toolkit から設定します。\n私の環境ではbrewでインストールしたので、以下のパスを設定しました。 AWS Toolkitのインストール IntellijのプラグインからAWS Toolkitをインストールします。 プラグインのインストールは こちら を参考にしてください。\nAWS Toolkitの設定 AWS Toolkitを利用するためにはAWSの認証情報を設定する必要があります。\nAWS ExplorerからAWSの認証情報を設定します。 Access Key IDとSecret Access KeyをAWSのコンソールから取得し、設定します。 設定が完了したら、AWS ExplorerにAWSのリソースが表示されるようになります。 この画像ではリージョンが us-east-1 になっていますが、Lambdaを作成するリージョンに合わせてください。\nLambdaの開発 以下のようなコードスニペットを作成します。\nlamda-sample.py\n1 2 3 def lambda_handler(event, context): print(\u0026#34;Hello World\u0026#34;) return \u0026#34;Hello World!\u0026#34; AWS ExplorerからLambdaを作成します。\nCreate Lambda Functionを選択し、必要な値を入力します。\nHandlerにはコードスニペットの \u0026lt;ファイル名\u0026gt;.\u0026lt;関数名\u0026gt; を入力してください。\nこれでLambdaの作成が完了しました。\nLambdaをローカルで実行 またToolkitを利用すると、Lambdaをローカルで実行することができます。 Runを選択すると、Lambdaがローカルで実行されます。\nまとめ IntellijでAWS Toolkitを使ってLambdaを効率よく開発する方法を解説しました。 Intellijで開発してローカルで実行できるので、開発効率がかなり上がります。\n関連記事 AWS API GatewayとLambdaを連携させる方法 AWS Lambda + LINE Botで掃除リマインダーを作る ","date":"2023-12-12T22:40:05+09:00","permalink":"https://bossagyu.com/blog/006-intellij-lamda-setup/","title":"IntellijでAWS Toolkitを使ってLambdaを効率よく開発する"},{"content":"概要 Github CopilotをIntellijで使う方法を解説します。 合わせてショートカットのチートシートを記載します。\n実現までの流れ Github Copilotの登録 Intellijの設定 Github Copilotの利用 まとめ Github Copilotの登録 Github Copilot のリンクからGithub Copilotに登録します。\nIntellijの設定 IntellijのプラグインからGithub Copilotをインストールします。\nインストールが完了したら、Intellijを再起動します。\nGithub Copilotの利用 ショートカット一覧 Intellijでコードを書いていると、Github Copilotがコードを補完してくれます。\nmacのショートカットの一覧は以下のとおりです。\nショートカット 機能 tab コードを補完する Option + ] 次の補完候補を表示する Option + [ 前の補完候補を表示する Command + → 提案の次の単語のみ受け入れる コメントによるコード補完 Github Copilotはコメントによるコード補完も行うことができます。\n例えば、以下のようなコメントを書くと、コメントの内容に応じてコードを補完してくれます。\n1 2 3 4 // このメソッドは、引数の値を2倍にして返す ← 書いたコメント public int double(int value) { // ← 生成されたコード return value * 2; } まとめ IntellijでGithub Copilotを利用する方法を解説しました。 この記事はGithub Copilotを利用して書いており、Markdownでのブログ作成でもかなり補完してくれるので、気になる人は試してみてください。\n関連記事 VSCodeでGithub Copilotを使いこなす完全ガイド Github CopilotをChat toolを使って便利に使う方法 ","date":"2023-12-11T22:45:40+09:00","permalink":"https://bossagyu.com/blog/005-github-copilot/","title":"IntellijでのGithub Copilotの使い方"},{"content":"概要 macのローカル環境で開発を行う際のpythonの環境構築の方法について記載します。\n今回は以下の２つの仕組みを利用して、pythonのバージョン管理と仮想環境の管理を行います。\npyenv 複数のpythonのバージョンを扱うために利用します。 venv プロジェクトごとに環境を分けるために利用します。 それぞれの違いや必要性の解説はこちらの記事が参考になります。\nPythonのインストール まずは、ローカル環境にPyenvをインストールし、任意のPythonバージョンを利用できるようにします。\npyenvをインストールします。\n1 brew install pyenv インストールしたpyenvのバージョンを確認します。\n1 2 pyenv --version pyenv 2.3.35 zshに設定を追加します。\n1 2 3 echo \u0026#39;export PYENV_ROOT=\u0026#34;$HOME/.pyenv\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo \u0026#39;export PATH=\u0026#34;$PYENV_ROOT/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo -e \u0026#39;if command -v pyenv 1\u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then\\n eval \u0026#34;$(pyenv init -)\u0026#34;\\nfi\u0026#39; \u0026gt;\u0026gt; ~/.zshrc .zshrcの内容を読み込みます。\n1 source ~/.zshrc インストール可能なPythonのバージョン一覧を表示します。\n1 pyenv install --list 指定したバージョンをインストールします。\n1 pyenv install 3.11.7 プロジェクトフォルダに指定したPythonのバージョンを利用します。\n1 2 3 cd \u0026lt;作成したプロジェクトフォルダ\u0026gt; pyenv local 3.11.7 pyenv versions globalの場合は全体に反映されます。\n1 pyenv global 3.11.7 実行されているpythonのバージョンを確認します。\n1 python -V venvで仮想環境の作成 プロジェクトのディレクトリに仮想環境を作成します。\n1 2 # python -m venv \u0026lt;仮想環境名\u0026gt; python -m venv venv 仮想環境を有効化します。\n1 source venv/bin/activate ディアクティベートは以下のコマンドで実行できます。\n1 deactivate venvのアップデート方法 依存パッケージを最新化する場合は、仮想環境を有効化した上で以下を実行します。requirements.txt を利用しているなら、ファイルを更新した後に再インストールするとよいでしょう。\n1 2 3 source venv/bin/activate pip install --upgrade pip pip install -U -r requirements.txt Pythonのマイナーバージョンを上げるときは、pyenv で新しいバージョンを入れ直したあと、仮想環境を作り直すとライブラリの整合性が保ちやすいです。\n1 2 3 4 5 6 pyenv install 3.12.2 pyenv local 3.12.2 rm -rf venv python -m venv venv source venv/bin/activate pip install -U -r requirements.txt venvの削除方法 不要になった仮想環境はディレクトリごと削除すればよいです。作業ディレクトリを確認した上で実行してください。\n1 rm -rf venv 以上でローカル環境の構築が完了です。\n関連記事 MacでUVを用いてPythonの開発環境を構築する（モダンな代替ツールとして） ","date":"2023-12-10T23:19:33+09:00","permalink":"https://bossagyu.com/blog/004-python-setup/","title":"Pyenvとvenvを用いたローカル環境のセットアップ方法"},{"content":"概要 HugoをでGoogle Analyticsを設定する方法をサクッと解説します。\n実現までの流れ Google Analyticsへの登録 トラッキングIDの取得 Hugoの設定にトラッキングIDを追加 Google Analyticsの登録 [GA4] アナリティクスで新しいウェブサイトまたはアプリのセットアップを行う に従い登録を行います。\nデータストリームを追加すると、トラッキングIDが取得できるのでメモしておきます。 ※ トラッキングIDは日本語訳の影響か、測定IDという表示になっています。\nHugoの設定にトラッキングIDを追加 tomlに設定を追加 config.tomlにgoogleAnalytics = トラッキングIDを追加します。\n1 2 3 4 5 6 baseURL = \u0026#39;https://bossagyu.com\u0026#39; languageCode = \u0026#39;ja-jp\u0026#39; title = \u0026#39;Bossagyu Blog\u0026#39; theme = \u0026#39;hugo-bearcub\u0026#39; googleAnalytics = \u0026#34;G-1234ABCDEF\u0026#34; # ↑ この行を追加、トラッキングIDは自分のものに変更してください。 トラッキングコードを埋め込む テンプレートによってはtomlの設定を入れるだけで読めるものもあるみたいですが、 私の使用しているbearcubのテンプレートは対応してなかったので、 自分でヘッダにトラッキングコードを読み込むように追加します。\nコードスニペットについてはまくまく Hugo ノートを参考にさせていただきました。\nトラッキングコードを読むために layouts/partials/analytics.html を作成する。\n1 2 3 4 5 6 7 8 9 10 11 12 13 {{ if not .Site.IsServer }} {{ with .Site.GoogleAnalytics }} \u0026lt;!-- Google tag (gtag.js) --\u0026gt; \u0026lt;script async src=\u0026#34;https://www.googletagmanager.com/gtag/js?id={{ . }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;js\u0026#39;, new Date()); gtag(\u0026#39;config\u0026#39;, \u0026#39;{{ . }}\u0026#39;); \u0026lt;/script\u0026gt; {{ end }} {{ end }} ページヘッダで analytics.html を読み込むようにする。\n1 2 3 # テンプレートの内容をコピーしてきてオーバーライドする cp themes/hugo-bearcub/layouts/_default/baseof.html layouts/_default/baseof.html vim layouts/_default/baseof.html baseof.htmlに{{- partial \u0026quot;analytics\u0026quot; . -}} を追加する。\n1 2 3 4 5 6 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;{{ with .Site.LanguageCode }}{{ . }}{{ else }}en-US{{ end }}\u0026#34;\u0026gt; \u0026lt;head\u0026gt; {{- partial \u0026#34;analytics\u0026#34; . -}} \u0026lt;meta http-equiv=\u0026#34;X-Clacks-Overhead\u0026#34; content=\u0026#34;GNU Terry Pratchett\u0026#34; / 上記ソースコードを追加した状態で、再度ビルドするとGoogle Analyticsにデータが送信されるようになります。\ntips こちらの対応をしても、Google Analyticsにデータが連携されていないように見える場合は、タグの追加がうまくいっていない可能性があります。\nまずはタグがちゃんと入っているかの切り分けをするために、googleデベロッパーツールを開きトラッキングがhtml内に含まれているか確認してみるとよいです。\n関連記事 Hugo + Netlify + Githubでブログを公開する - ブログの初期構築 Google Search Consoleでブログを検索対象にする方法 - SEO対策 ","date":"2023-12-09T18:09:42+09:00","permalink":"https://bossagyu.com/blog/003-google-analytics/","title":"HugoでGoogle Analyticsの設定をする方法"},{"content":"概要 LINEのBotを利用してアプリケーションを作ってみようと考えたのでまずはBotを利用できる状態にする。\n本ページでは一番最初にLINE Message APIの登録の方法とcurlでコマンドラインからメッセージを送る方法を紹介します。\nMessaging APIを利用する LINE Developers にログインしてプロバイダーを作成する。\nプロバイダーとは(説明)\n1 2 LINE Developersサイトでは、サービスを提供し、ユーザーの情報を取得する開発者個人、 企業、または団体等をサービス提供者（LINEミニアプリではサービス事業主）と呼びます。 なので好きな文字列を入れる。\nそのまま新規チャンネルを作成する。 このまま作成ボタンを押すと新規チャンネルが作成される\nコマンドラインからポストをする Messaging API設定からQRコードを読みこんで友達追加する。\nMessaging API設定から「チャンネルアクセストークン(長期)」を取得 チャンネル基本設定から「あなたのユーザーID」取得\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 TOKEN=\u0026#34;\u0026lt;チャンネルアクセストークン(長期)\u0026gt;\u0026#34; ID=\u0026#34;\u0026lt;あなたのユーザーID\u0026gt;\u0026#34; UUID=$(uuidgen | tr \u0026#34;[:upper:]\u0026#34; \u0026#34;[:lower:]\u0026#34;) curl -v -X POST https://api.line.me/v2/bot/message/push \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ -H \u0026#34;X-Line-Retry-Key: \u0026#34; \\ -d \u0026#34;{ \\\u0026#34;to\\\u0026#34;: \\\u0026#34;${ID}\\\u0026#34;, \\\u0026#34;messages\\\u0026#34;:[ { \\\u0026#34;type\\\u0026#34;:\\\u0026#34;text\\\u0026#34;, \\\u0026#34;text\\\u0026#34;:\\\u0026#34;Hello, world1\\\u0026#34; } ] }\u0026#34; レスポンスが帰ってきて、LINEのトーク画面でBotからの投稿が行われていれば成功！\n関連記事 掃除を忘れない！掃除リマインダーLINE Bot（LINE Botの実践プロジェクト） ","date":"2023-12-07T09:37:00+09:00","permalink":"https://bossagyu.com/blog/002-line-messaging-api/","title":"LINE Messaging APIの登録と使い方"},{"content":"概要 Hugoで作ったサイトをGithubで管理、Netlifyでビルドした手順を0から作れるよう記載します。 この方式にすると手元でMarkDownで書いたブログをGithubにPushするだけで簡単に公開できるようになります。\nFaviconの設定方法も合わせて解説します。\n流れ Hugoでサイトを生成 Githubにプッシュ Netlifyでデプロイ Faviconを設定 Hugoで静的サイトを生成 まずはHugoをインストールします。\n1 brew install hugo blogの雛形を作成します。\n1 hugo new site my-blog ブログに適応するテーマをsubmoduleとして追加します。\n1 2 3 4 5 cd my-blog git init # テーマをgithubのsubmoduleとして追加 git submodule add https://github.com/theNewDynamic/gohugo-theme-ananke.git themes/ananke hugo.tomlに記載することでテーマを適応する。\n1 echo \u0026#34;theme = \u0026#39;ananke\u0026#39;\u0026#34; \u0026gt;\u0026gt; config.toml サーバを起動させます。\n1 hugo server 起動ログの Web Server is available at http://localhost:51517/ (bind address 127.0.0.1) のような記述の http://localhost:51517/ にアクセスすればローカルに起動した静的サイトが閲覧できます。\nTips Hugoのテーマを変えたい場合は、Hugo Themas から好きなものを選んで変えてください。 これは後からでも変えられるのでとりあえずNetlifyでビルドするところまで走り切るのがおすすめ。 Tomlファイルの書き方は Configure Hugo に記載されています。 Githubにpush Github にリポジトリを作成。\n作成後以下のコマンドを実行し、サイトをpushします。\n1 2 3 4 5 6 7 8 9 10 cd my-blog echo .hugo_build.lock \u0026gt;\u0026gt; .gitignore git add . git commit -m \u0026#34;first commit\u0026#34; git branch -M main # \u0026lt;user name\u0026gt;は自分のユーザー名に置き換えてください。 # 今回はmy-blogというリポジトリを作成している例です。 git remote add origin git@github.com:\u0026lt;user name\u0026gt;/my-blog git push -u origin main pushが完了するとGithubのUI上でソースコードが閲覧できる状態になっています。\nNetlifyでデプロイ netlify へアクセスし、デプロイを行う。\nHugoの公式で案内 があるのでこちらを参考に連携を行う。\n指示に従いデプロイを完了すると以下のようにDeployの結果が published になる。\nサイト上に表示されたURLをクリックするとデプロイされたサイトにアクセスできる。 これでデプロイまではおしまい。 以降は変更を加えてmainにpushするだけで自動デプロイが走り、サイトの内容が更新されるようになる。\nFaviconを設定する Faviconとは、ウェブサイトのブックマークやタブ、ホーム画面などに表示されるアイコンのことです。 Googleより検索結果に表示されるためのfaviconのガイドラインが公開されており、 こちらに従うことで検索結果にも表示されるようになります。\nFaviconの作成 Faviconを作成するには、以下のサイトを利用します。\nFavicon.ico \u0026amp; App Icon Generator サイトにアクセスし、faviconにしたい画像をアップロードして「Generate Favicon」をクリックします。 その後表示される画面で、「Download the generated favicon」のリンクをクリックすると、faviconがダウンロードできます。\nHugoでFaviconを表示する HugoでFaviconを表示するには、themeによっても違いますが、多くのテーマでは以下のようにtomlに設定するだけでfaviconが表示できます。\n1 2 [params] favicon = \u0026#34;images/favicon.ico\u0026#34; ダウンロードしたfavicon.icoをstatic/images/ディレクトリに配置して、上記の設定を追加してください。\nまとめ Hugo + Netlify + Githubでブログを公開する方法を解説しました。\nHugo: 静的サイトを生成 Github: ソースコードを管理 Netlify: 自動デプロイ Favicon: ブランディングのための設定 この構成なら、Markdownで記事を書いてGithubにpushするだけで自動的にサイトが更新されます。\n関連記事 HugoでGoogle Analyticsの設定をする方法 - アクセス解析の導入 Google Search Consoleでブログを検索対象にする方法 - SEO対策 Lighthouseの使い方の紹介 - パフォーマンス計測 ChatGPTを利用してHugoで作ったブログを多言語対応する方法 - 多言語対応 Twitter Social Cardの設定方法 - SNS共有の最適化 ","date":"2023-12-02T00:59:37+09:00","permalink":"https://bossagyu.com/blog/001-hugo-netlify-build/","title":"Hugo + Netlify + Githubでブログを公開する"}]