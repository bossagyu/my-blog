[{"content":"概要 この記事では、Github Copilot Chatをより便利に使うためのChat toolを使う方法について説明します。 Chat toolを利用することで、Copilotとのチャットの中で様々なタスクを実行できるようになります。\nChat toolとは Chat toolとはGithub Copilotのチャット内で特殊なタスクを実行できる機能です。 例えば、ターミナルからの出力を取得したり、URLから情報を取得したりすることができます。\nChat toolの実行方法 今回はVSCodeでChat toolを利用する方法を説明します。\n基本的には #\u0026lt;command\u0026gt; \u0026lt;args\u0026gt; の形式でコマンドを入力します。\nこれだけです。\n代表的なChat toolのコマンド 以下に代表的なChat toolのコマンドを紹介します。\nコマンド 機能 #codebase 現在のワークスペースを全部検索する #selection 現在のエディタで選択しているコードをコンテキストとしてプロンプトに追加する #terminal_selection 現在のターミナルで選択している部分をコンテキストとしてプロンプトに追加する。\nエラーが出たときにターミナルの出力を参照するのに便利です。 #fetch_webpage URLを指定してwebページからコンテンツを取得してコンテキストとしてプロンプトに追加する 他にもたくさん便利な使い方があるので、公式ドキュメントを参照してください。\nGithub Copilot Chat tools まとめ この記事では、Github Copilot ChatでChat toolを使う方法について説明しました。 Chat toolを使うことで、より便利にCopilotを利用できるようになるので、ぜひ活用してみてください。\n","date":"2025-10-07T08:39:41+09:00","permalink":"https://bossagyu.com/blog/042-github-copilot/","title":"Github CopilotをChat toolを使って便利に使う方法"},{"content":"概要 この記事では、GoでRedisを使う方法について説明します。\nRedisの起動方法 Redisの説明やDockerでの起動方法については、こちらの記事を参照してください。\nRedisを起動する。\n1 2 3 docker run -d --name my-redis \\ -p 6379:6379 \\ redis:7.4.5-alpine redis-cliで接続確認。\n1 docker exec -it my-redis redis-cli ping PONGと返ってくればOK。\nGoのプロジェクトの作成 Goのプロジェクトを作成する。\n1 2 3 mkdir redis-go cd redis-go go mod init redis-go GoでRedisを使う Redisに接続して、書き込み・読み込みを行うサンプルコード。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/redis/go-redis/v9\u0026#34; ) func main() { // パスワードなし、ローカルの6379に接続 rdb := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;127.0.0.1:6379\u0026#34;, Password: \u0026#34;\u0026#34;, // なし DB: 0, DialTimeout: 3 * time.Second, ReadTimeout: 1 * time.Second, WriteTimeout: 1 * time.Second, }) defer rdb.Close() ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() // 接続確認 if err := rdb.Ping(ctx).Err(); err != nil { log.Fatalf(\u0026#34;redis ping failed: %v\u0026#34;, err) } log.Println(\u0026#34;connected to redis ✅\u0026#34;) // 基本操作（SET/GET） if err := rdb.Set(ctx, \u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, 10*time.Minute).Err(); err != nil { log.Fatalf(\u0026#34;SET error: %v\u0026#34;, err) } val, err := rdb.Get(ctx, \u0026#34;hello\u0026#34;).Result() if err != nil { log.Fatalf(\u0026#34;GET error: %v\u0026#34;, err) } fmt.Println(\u0026#34;hello =\u0026#34;, val) // Set型の例 key := \u0026#34;team:dev\u0026#34; if err := rdb.SAdd(ctx, key, \u0026#34;alice\u0026#34;, \u0026#34;bob\u0026#34;, \u0026#34;alice\u0026#34;).Err(); err != nil { log.Fatalf(\u0026#34;SADD error: %v\u0026#34;, err) } members, _ := rdb.SMembers(ctx, key).Result() fmt.Println(\u0026#34;members =\u0026#34;, members) } パッケージのインストールを行う。\n1 go mod tidy 実行する。\n1 go run main.go 以下のように表示されればOK。\n1 2 hello = world members = [alice bob] まとめ GoでRedisを使う方法について説明しました。 GoでRedisを使う場合は、go-redisを使用します。\n","date":"2025-10-05T20:25:02+09:00","permalink":"https://bossagyu.com/blog/041-redis-go/","title":"RedisをGoで使う方法"},{"content":"概要 この記事では、RedisをDockerで起動する方法について説明します。\nRedisとは Redisは、オープンソースのインメモリデータ構造ストアであり、データベース、キャッシュ、メッセージブローカーとして使用されます。高速な読み書き性能を持ち、様々なデータ構造（文字列、リスト、セット、ハッシュなど）をサポートしています。\nDockerでのRedis起動 1. Dockerのインストール まず、Dockerがインストールされていることを確認します。インストールされ ていない場合は、Dockerの公式サイトからインストールしてください。\n2. Redisコンテナの起動 以下のコマンドを実行して、Redisコンテナを起動します。\n1 docker run --name redis -d -p 6379:6379 redis:7.4.5-alpine コンテナにログインする\n1 docker exec -it redis sh 3. Redisの動作確認 Redisコンテナ内で以下のコマンドを実行して、Redisクライアントを起動します。\n1 redis-cli データの設定と取得を試してみます。\n1 2 3 set name \u0026#34;hoge\u0026#34; get name \u0026#34;hoge\u0026#34; これで、Redisが正常に動作していることが確認できました。\n4. Redisコンテナの停止と削除 Redisコンテナを停止するには、以下のコマンドを実行します。\n1 docker stop redis Redisコンテナを削除するには、以下のコマンドを実行します。\n1 docker rm redis まとめ この記事では、Dockerを使用してRedisを起動する方法について説明しました。 Redisは高速なデータベースとして広く利用されており、Dockerを使用することで簡単にセットアップできます。\nぜひ、Redisを活用してみてください。\n","date":"2025-10-03T07:29:31+09:00","permalink":"https://bossagyu.com/blog/040-redis-local/","title":"RedisをDockerで起動する方法"},{"content":"概要 この記事では、Visual Studio Code (VSCode) で AWS Toolkit を使用する方法について説明します。\nAWS Toolkitのインストール VSCodeを起動します。 左側のサイドバーから拡張機能アイコンをクリックします。 検索バーに「AWS Toolkit」と入力し、表示されたリストから「AWS Toolkit for Visual Studio Code」を選択します。 「インストール」ボタンをクリックして、インストールを開始します。 Asia Pacific Tokyoリージョンを設定する方法 EXPLORERからリージョンをして利用するのですが、デフォルトではus-east-1が選択されているため、Asia Pacific Tokyoリージョンを設定する必要があります。 少しわかりにくいので、以下の手順で設定します。\n左側のサイドバーから「AWS Explorer」から、ハンバーガーメニューをクリックします。 Show or Hide Regionsをクリックします。 表示されたリストから「Asia Pacific (Tokyo) ap-northeast-1」を選択します。 これで、AWS ToolkitがAsia Pacific Tokyoリージョンで使用できるようになります。\n","date":"2025-08-11T20:33:30+09:00","permalink":"https://bossagyu.com/blog/039-aws-toolkit-vscode/","title":"Visual Studio CodeでAWS Toolkitを使う方法"},{"content":"概要 MacでVisual Studio Code (VSCode) のアップデート方法を説明します。\nVSCodeのアップデート方法 VScodeを起動します。 Code → 「更新の確認」\n英語の場合は Update Visual Studio Code を選択します。 更新が始まると以下のような画面になります\nダウンロードが完了すると「再起動して更新」が表示されます。\n再起動後「Visual Studio Codeのバージョン情報」をクリックして、バージョンが上がっていることを確認します。\nまとめ VSCodeのアップデートは非常に簡単で、数クリックで完了します。定期的にアップデートを行うことで、新機能やセキュリティパッチを利用できるため、常に最新の状態を保つことが重要です。 アップデート後は、必ずバージョン情報を確認して、正しくアップデートが行われたかをチェックしましょう。\n","date":"2025-07-27T18:12:16+09:00","permalink":"https://bossagyu.com/blog/038-update-vscode/","title":"Visual Studio Codeのアップデート方法"},{"content":"概要 Visual Studio Code (VSCode) では、インデントガイドの色をカスタマイズすることができます。 この記事では、インデントガイドの色を変更する方法を説明します。 indent-rainbowのような拡張機能はインデントガイドが途中で切れるなどの問題がありましたが、VSCodeの設定ファイルを編集するだけで実現できます。\nインデントガイドの色を変更する方法 設定ファイルを開く VSCodeの設定ファイルを開きます。 Command Palette (Ctrl + Shift + P) を開き、Preferences: Open Settings (JSON) を選択します。\n設定を追加する 以下を参考に設定を追加します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \u0026#34;workbench.colorCustomizations\u0026#34;: { \u0026#34;editorIndentGuide.background1\u0026#34;: \u0026#34;#006400\u0026#34;, \u0026#34;editorIndentGuide.background2\u0026#34;: \u0026#34;#008000\u0026#34;, \u0026#34;editorIndentGuide.background3\u0026#34;: \u0026#34;#00a000\u0026#34;, \u0026#34;editorIndentGuide.background4\u0026#34;: \u0026#34;#006400\u0026#34;, \u0026#34;editorIndentGuide.background5\u0026#34;: \u0026#34;#008000\u0026#34;, \u0026#34;editorIndentGuide.background6\u0026#34;: \u0026#34;#00a000\u0026#34;, \u0026#34;editorIndentGuide.activeBackground1\u0026#34;: \u0026#34;#00ff00\u0026#34;, \u0026#34;editorIndentGuide.activeBackground2\u0026#34;: \u0026#34;#00ff00\u0026#34;, \u0026#34;editorIndentGuide.activeBackground3\u0026#34;: \u0026#34;#00ff00\u0026#34;, \u0026#34;editorIndentGuide.activeBackground4\u0026#34;: \u0026#34;#00ff00\u0026#34;, \u0026#34;editorIndentGuide.activeBackground5\u0026#34;: \u0026#34;#00ff00\u0026#34;, \u0026#34;editorIndentGuide.activeBackground6\u0026#34;: \u0026#34;#00ff00\u0026#34;, \u0026#34;editorWhitespace.foreground\u0026#34;: \u0026#34;#393A3D\u0026#34; }, } 設定内容 説明 editorIndentGuide.background\u0026lt;number\u0026gt; インデントガイドの背景色を指定します。\n\u0026lt;number\u0026gt; は インデントの深さに対応します。 editorIndentGuide.activeBackground\u0026lt;number\u0026gt; アクティブなインデントガイドの背景色を指定します。\n\u0026lt;number\u0026gt; はインデントの深さに対応します。 editorWhitespace.foreground 空白文字の色を指定します。 実際に設定を追加した結果以下の画像のようにインデントガイドの色が変更されます。 まとめ VSCodeでは、インデントガイドの色を簡単にカスタマイズできます。 設定ファイルに必要な項目を追加するだけで、好みの色に変更できます。 これにより、コードの可読性が向上し、インデントの深さを視覚的に把握しやすくなります。 ぜひ試してみてください。\n","date":"2025-07-22T21:01:13+09:00","permalink":"https://bossagyu.com/blog/036-vscode-indent/","title":"vscodeのインデントガイドをカスタマイズする方法"},{"content":"概要 Github Copilotは、デフォルトではMarkdownファイルでの補完が無効になっています。 この記事では、GitHub CopilotをMarkdownで有効にする方法を説明します。\nGitHub Copilotの設定 VSCodeでGitHub Copilotのプラグインのページを開きます。\n歯車アイコンをクリックし、設定を開きます。\n設定画面で mardown が false になっているので true に変更します。 まとめ GitHub CopilotをMarkdownで有効にすることで、Markdownファイルの編集がより効率的になります。 VSCodeの設定を変更するだけで簡単に有効化できるので、ぜひ試してみてください。\n","date":"2025-07-21T10:00:00+09:00","permalink":"https://bossagyu.com/blog/035-github-copilot-markdown/","title":"GitHub CopilotをMarkdownで有効にする方法"},{"content":"概要 IntelliJやPyCharmなどの JetBrains製品のメジャーバージョンを上げる際にサイドダウンロードするが必要になります。 設定を引き継ぎつつなるべく簡単にアップデートする方法について説明します。\nJetBrains Toolbox Appのインストール JetBrains製品のメジャーバージョンを上げる際は、JetBrains Toolbox Appを利用することをおすすめします。\n上記リンクよりJetBrains Toolbox Appをダウンロードし、インストールします。\nアップデート JetBrains Toolbox Appを起動し、アップデートしたい製品を選択します。\n今回はPyCharmをの更新を行うのでクリックすると、アップデートが開始されます。\nアップデートが完了すると、以下のように新しいバージョンがインストールされます。\nまとめ JetBrains製品のメジャーバージョンを上げる際は、JetBrains Toolbox Appを利用することで簡単にアップデートが可能です。\n","date":"2025-06-12T20:46:53+09:00","permalink":"https://bossagyu.com/blog/034-jetbrains-update/","title":"JetBrains製品のメジャーバージョンを上げる方法"},{"content":"概要 今回はFeastのチュートリアルを参考に、Macで実行してみます。\n事前準備 MacでUVを用いてPythonの開発環境を構築するを参考にUVを用いて開発できる環境を整えてください。\nuvで構築した環境にfeastをインストールする。\nFeastのインストールからUIの起動まで 1 \u0026gt; uv pip install feast==0.40.1 2025/01/13現在、Feastにバグがあり最新版をインストールするとUIが起動しないので注意してください。(issue)\nFeature Repositoryを作成する。\n1 2 3 \u0026gt; feast init my_feature_repo Creating a new Feast repository in /Users/kouhei/Program/ML/feast/my_feature_repo. 以下のようなリポジトリが作成される。\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; tree . └── my_feature_repo ├── README.md ├── __init__.py └── feature_repo ├── __init__.py ├── data │ └── driver_stats.parquet ├── example_repo.py ├── feature_store.yaml └── test_workflow.py Feastのチュートリアルの設定を反映する。\n1 2 cd my_feature_repo/feature_repo feast apply Feastのuiを起動する。\n1 \u0026gt; feast ui http://0.0.0.0:8888/p/my_feature_repo へアクセスするとUIが表示されます。\nFeastへのデータ操作 5.Build a training datasetからは jupyter notebookを使うのでインストールしておきます。\n1 uv pip install jupyter notebookの起動\n1 jupyter notebook Jupyter Notebookで以下の内容を実行し、トレーニングに利用するデータセットを準備する。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from feast import FeatureStore import pandas as pd from datetime import datetime entity_df = pd.DataFrame.from_dict({ \u0026#34;driver_id\u0026#34;: [1001, 1002, 1003, 1004], \u0026#34;event_timestamp\u0026#34;: [ datetime(2021, 4, 12, 10, 59, 42), datetime(2021, 4, 12, 8, 12, 10), datetime(2021, 4, 12, 16, 40, 26), datetime(2021, 4, 12, 15, 1 , 12) ] }) store = FeatureStore(repo_path=\u0026#34;.\u0026#34;) training_df = store.get_historical_features( entity_df=entity_df, features = [ \u0026#39;driver_hourly_stats:conv_rate\u0026#39;, \u0026#39;driver_hourly_stats:acc_rate\u0026#39;, \u0026#39;driver_hourly_stats:avg_daily_trips\u0026#39; ], ).to_df() print(training_df.head()) # Train model # model = ml.fit(training_df) NoteBookでの実行結果は以下の通りとなります。\nオンラインストアにデータを入れる。\nサンプルで記載されている feast materialize-incremental $CURRENT_TIME ではうまく動作しなかったので、データ全体を対象とするように時刻の範囲を設定しています。\n1 2 3 4 5 6 7 8 9 10 feast materialize 1970-01-01T00:00:00Z 2025-01-04T01:24:24Z 01/04/2025 10:28:40 AM root WARNING: _list_feature_views will make breaking changes. Please use _list_batch_feature_views instead. _list_feature_views will behave like _list_all_feature_views in the future. Materializing 2 feature views from 1970-01-01 09:00:00+09:00 to 2025-01-04 10:24:24+09:00 into the sqlite online store. driver_hourly_stats_fresh: 0%| | 0/5 [00:00\u0026lt;?, ?it/s]01/04/2025 10:28:40 AM root WARNING: Cannot use sqlite_vec for vector search 100%|███████████████████████████████████████████████████████████████| 5/5 [00:00\u0026lt;00:00, 1299.11it/s] driver_hourly_stats: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:00\u0026lt;00:00, 4569.95it/s] Jupyter Notebookを用いてオンラインストアからデータを取得します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pprint import pprint from feast import FeatureStore store = FeatureStore(repo_path=\u0026#34;.\u0026#34;) feature_vector = store.get_online_features( features=[ \u0026#39;driver_hourly_stats:conv_rate\u0026#39;, \u0026#39;driver_hourly_stats:acc_rate\u0026#39;, \u0026#39;driver_hourly_stats:avg_daily_trips\u0026#39; ], entity_rows=[{\u0026#34;driver_id\u0026#34;: 1001}] ).to_dict() pprint(feature_vector) # Make prediction # model.predict(feature_vector) 出力結果\n1 2 3 4 {\u0026#39;acc_rate\u0026#39;: [0.5004482269287109], \u0026#39;avg_daily_trips\u0026#39;: [691], \u0026#39;conv_rate\u0026#39;: [0.3067885637283325], \u0026#39;driver_id\u0026#39;: [1001]} Feastのオンラインストアにマテリアラズして、オンラインストアからデータを取得することができました。\nまとめ 本記事では、Feastのチュートリアルを基に、Mac上でFeastを使用してデータを管理する方法を紹介しました。UVを使ったPython開発環境の構築から、Feastのインストール、UIの起動、そしてトレーニングデータの準備やオンラインストアへのデータマテリアライズまで、一連の操作を丁寧に解説しました。\nこれにより、Feastを利用してトレーニングデータと推論データの管理を効率的に行うことができ、トレーニングスキューの回避が可能になります。\n","date":"2025-01-13T11:49:53+09:00","permalink":"https://bossagyu.com/blog/033-feast-tutorial/","title":"FeastのチュートリアルをMacで実行する"},{"content":"概要 MacでPythonの開発環境を構築する方法を紹介します。\nuvとは 2024年中旬に発表されたばかりのパッケージ管理ツール。\nRustで書かれており、他のパッケージ管理マネージャよりも高速であることが特徴です。\n公式の説明は こちら を参照してください。\n利用方法 uvのインストール 1 curl -LsSf https://astral.sh/uv/install.sh | sh PATHを通す\n1 2 source $HOME/.local/bin/env (sh, bash, zsh) source $HOME/.local/bin/env.fish (fish) インストールできたことを確認\n1 2 uv --version uv 0.5.13 (c456bae5e 2024-12-27) 使い方 仮想環境の作成\n1 uv venv 仮想環境のアクティベート\n1 source .venv/bin/activate パッケージのインストール\n1 uv pip install \u0026lt;package name\u0026gt; uvの細かい説明は 公式のドキュメント を参照してください。\nまとめ MacでPythonの開発環境を構築する方法を紹介しました。 uvは高速かつ使いやすいので2025年1月時点ではPythonの開発環境構築において有用なツールと言えるでしょう。\n","date":"2025-01-01T14:39:53+09:00","permalink":"https://bossagyu.com/blog/032-python-uv/","title":"MacでUVを用いてPythonの開発環境を構築する"},{"content":"概要 Scrumを採用しているチームでは、セレモニーを行います。 セレモニーはチームメンバーが集まり、スプリントの進捗や課題を共有し、次のスプリントに向けての計画を立てるための重要な場です。 しかし、Scrumを惰性で続けてしまっているとセレモニーが長引いたり、効果的な議論が行われなかったりすることがあります。\nこの記事では、セレモニーの見直しによってチームのパフォーマンスを向上させる方法について考察します。\nセレモニーの目的を明らかにする 各セレモニーの目的は以下の通りです。\nもっとも大切なことはこの目的を達成するために必要な議論がなされるアジェンダになっていることです。 目的についてはスクラムガイドを参考に記載していますが、やや筆者の意訳が含まれています。\nセレモニー 目的 デイリースクラム スプリントゴールを達成するために、チームの一日の計画を立てる。 スプリントレビュー スプリントの成果を検査し、今後の適応を決定すること。 プロダクトバックログリファインメント プロダクトバックログを整理し、プロダクトゴールを達成するための計画を立てる。 スプリントレトロスペクティブ スプリントの振り返りを行い、品質と高価を高めるための適応を計画もしくは実行する。 スプリントプランニング 次のスプリント期間中の作業計画を立てる タイムボックスの徹底 セレモニーを効率的に行うためには、タイムボックスを徹底することが重要です。 タイムボックスとは、セレモニーの時間を決めてその時間内に議論を終えるというルールのことです。\nセレモニーではタイムボックスを超えないようにし、超えてしまった場合はなぜ超えてしまったのか、超えないようにはどうしていくと良いのかを振り返ることが重要です。\nセレモニーごとの効率的な議論を行うためのポイント セレモニーのうち特に時間を取られがちなものについて、効率的な議論を行うためのポイントを以下に示します。\nデイリースクラム デイリースクラムではチームが今日何をするべきかが明確になるようにすることに主眼を置きます。\nそれ以外の議論や報告についてはデイリースクラムとは呼ばずに関係者を絞り別途時間を設けるようにします。\nチームメンバー全員の時間を使って行う価値があるかどうかを常に意識することが重要です。\nポイント 15分を超えないようにする 報告内容を絞る 議論が発生する場合は、別途時間を設け議論を行うというルールにする スプリントレビュー スプリントレビューではステークホルダーに対してフィードバックをもらい、次のスプリントに向けての計画を立てるための情報を得ることが目的です。 漫然とインクリメントを見せるだけではなく、事前にフィードバックを得たい内容を明確にしておくことが重要です。\nポイント レビューの準備を事前に行う 誰に、どのようなフィードバック得たいのか事前に明らかにしておく（仮説を立てる） プロダクトバックログリファインメント プロダクトバックログリファインメントは一般的には最大でスプリントの10%程度の時間を割くことが推奨されています。\n特に見積もりに時間がかかることが多いのである程度見切りをつけて行うことが大切です。\nポイント バックログアイテムに対する詳細な議論をしすぎない。 次スプリント以降のアイテムはすぐにやらない可能性もあるので、詳細な議論は不要です。スプリント計画のタイミングで実施しましょう。 見積もりについては相対見積もりを採用し、細かい数値にこだわらない 不確実性のコーンコーンが示す通り、見積もりは基本的にはブレるものであり、細かい数値にこだわる必要はありません。 まとめ 今回はセレモニーを効率的に進める方法について説明しました。\nセレモニーを効率的に進めることで、チームのパフォーマンスを向上させることができます。\n","date":"2024-11-18T09:36:05+09:00","permalink":"https://bossagyu.com/blog/028-scrum-ceremony/","title":"セレモニーの見直しによるチームパフォーマンス向上"},{"content":"概要 MacでGo言語の開発環境を構築して、最速で Hello World する方法を紹介します。\nGo言語のインストール brewを使ってGo言語をインストールします。\n1 \u0026gt; brew install go バージョンを確認\n1 2 \u0026gt; go version go version go1.21.3 darwin/arm64 Hello World の実行 以下のコードを main.go として保存します。\n1 2 3 4 5 6 7 package main import \u0026#34;fmt\u0026#34; func main() { fmt.Printf(\u0026#34;Hello World\\n\u0026#34;) } 実行する\n1 2 \u0026gt; go run hello.go Hello World バイナリをビルドして実行する\n1 2 3 4 5 6 7 \u0026gt; go build hello.go \u0026gt; ls hello* hello.go \u0026gt; ./hello Hello World まとめ 以上で、MacでGo言語の開発環境を構築して、最速で Hello World する方法を紹介しました。\n","date":"2024-09-15T16:52:04+09:00","permalink":"https://bossagyu.com/blog/030-go-environment-construction/","title":"macでGoの開発環境を構築して最速でHello Worldする"},{"content":"概要 macでparquetファイルをコマンドラインで簡単に読む方法を紹介します。\nparquet-cliを使って読む 今回は Feastのサンプルで提供されているParquetファイルを読んでみます。\nparquet-cliをbrewでmacにインストールする。\n1 brew install parquet-cli meta情報を確認する。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ parquet meta driver_stats.parquet File path: driver_stats.parquet Created by: parquet-cpp-arrow version 18.1.0 Properties: pandas: {\u0026#34;index_columns\u0026#34;: [{\u0026#34;kind\u0026#34;: \u0026#34;range\u0026#34;, \u0026#34;name\u0026#34;: null, \u0026#34;start\u0026#34;: 0, \u0026#34;stop\u0026#34;: 1807, \u0026#34;step\u0026#34;: 1}], \u0026#34;column_indexes\u0026#34;: [{\u0026#34;name\u0026#34;: null, \u0026#34;field_name\u0026#34;: null, \u0026#34;pandas_type\u0026#34;: \u0026#34;unicode\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;metadata\u0026#34;: {\u0026#34;encoding\u0026#34;: \u0026#34;UTF-8\u0026#34;}}], \u0026#34;columns\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;event_timestamp\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;event_timestamp\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;datetimetz\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;datetime64[ns]\u0026#34;, \u0026#34;metadata\u0026#34;: {\u0026#34;timezone\u0026#34;: \u0026#34;UTC\u0026#34;}}, {\u0026#34;name\u0026#34;: \u0026#34;driver_id\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;driver_id\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;int64\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;int64\u0026#34;, \u0026#34;metadata\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;conv_rate\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;conv_rate\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;metadata\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;acc_rate\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;acc_rate\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;metadata\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;avg_daily_trips\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;avg_daily_trips\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;int32\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;int32\u0026#34;, \u0026#34;metadata\u0026#34;: null}, {\u0026#34;name\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;field_name\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;pandas_type\u0026#34;: \u0026#34;datetime\u0026#34;, \u0026#34;numpy_type\u0026#34;: \u0026#34;datetime64[us]\u0026#34;, \u0026#34;metadata\u0026#34;: null}], \u0026#34;creator\u0026#34;: {\u0026#34;library\u0026#34;: \u0026#34;pyarrow\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;18.1.0\u0026#34;}, \u0026#34;pandas_version\u0026#34;: \u0026#34;2.2.3\u0026#34;} ARROW:schema: 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 Schema: message schema { optional int64 event_timestamp (TIMESTAMP(NANOS,true)); optional int64 driver_id; optional float conv_rate; optional float acc_rate; optional int32 avg_daily_trips; optional int64 created (TIMESTAMP(MICROS,false)); } Row group 0: count: 1807 16.88 B records start: 4 total(compressed): 29.796 kB total(uncompressed):29.760 kB -------------------------------------------------------------------------------- type encodings count avg size nulls min / max event_timestamp INT64 S _ R 1807 2.78 B 0 \u0026#34;2021-04-12T07:00:00.00000...\u0026#34; / \u0026#34;2024-12-28T14:00:00.00000...\u0026#34; driver_id INT64 S _ R 1807 0.07 B 0 \u0026#34;1001\u0026#34; / \u0026#34;1005\u0026#34; conv_rate FLOAT S _ R 1807 5.42 B 0 \u0026#34;1.9221554E-4\u0026#34; / \u0026#34;0.9998668\u0026#34; acc_rate FLOAT S _ R 1807 5.42 B 0 \u0026#34;2.1329636E-4\u0026#34; / \u0026#34;0.99993944\u0026#34; avg_daily_trips INT32 S _ R 1807 3.14 B 0 \u0026#34;0\u0026#34; / \u0026#34;999\u0026#34; created INT64 S _ R 1807 0.05 B 0 \u0026#34;2024-12-28T15:20:28.266000\u0026#34; / \u0026#34;2024-12-28T15:20:28.266000\u0026#34; headで中を見る。\n1 2 3 4 5 6 7 8 9 10 11 12 $ parquet head driver_stats.parquet {\u0026#34;event_timestamp\u0026#34;: 1734102000000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.27734742, \u0026#34;acc_rate\u0026#34;: 0.7152132, \u0026#34;avg_daily_trips\u0026#34;: 823, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734105600000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.57354224, \u0026#34;acc_rate\u0026#34;: 0.9831811, \u0026#34;avg_daily_trips\u0026#34;: 851, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734109200000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.3287562, \u0026#34;acc_rate\u0026#34;: 0.6172164, \u0026#34;avg_daily_trips\u0026#34;: 116, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734112800000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.045716193, \u0026#34;acc_rate\u0026#34;: 0.032996926, \u0026#34;avg_daily_trips\u0026#34;: 741, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734116400000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.12863782, \u0026#34;acc_rate\u0026#34;: 0.8951942, \u0026#34;avg_daily_trips\u0026#34;: 534, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734120000000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.9555806, \u0026#34;acc_rate\u0026#34;: 0.62216556, \u0026#34;avg_daily_trips\u0026#34;: 216, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734123600000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.75297666, \u0026#34;acc_rate\u0026#34;: 0.37602386, \u0026#34;avg_daily_trips\u0026#34;: 954, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734127200000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.46957988, \u0026#34;acc_rate\u0026#34;: 0.6454945, \u0026#34;avg_daily_trips\u0026#34;: 360, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734130800000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.6702387, \u0026#34;acc_rate\u0026#34;: 0.36532214, \u0026#34;avg_daily_trips\u0026#34;: 396, \u0026#34;created\u0026#34;: 1735399228266000} {\u0026#34;event_timestamp\u0026#34;: 1734134400000000000, \u0026#34;driver_id\u0026#34;: 1005, \u0026#34;conv_rate\u0026#34;: 0.019627139, \u0026#34;acc_rate\u0026#34;: 0.528229, \u0026#34;avg_daily_trips\u0026#34;: 833, \u0026#34;created\u0026#34;: 1735399228266000} スキーマの確認。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 $ parquet schema driver_stats.parquet { \u0026#34;type\u0026#34; : \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;schema\u0026#34;, \u0026#34;fields\u0026#34; : [ { \u0026#34;name\u0026#34; : \u0026#34;event_timestamp\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, \u0026#34;long\u0026#34; ], \u0026#34;default\u0026#34; : null }, { \u0026#34;name\u0026#34; : \u0026#34;driver_id\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, \u0026#34;long\u0026#34; ], \u0026#34;default\u0026#34; : null }, { \u0026#34;name\u0026#34; : \u0026#34;conv_rate\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, \u0026#34;float\u0026#34; ], \u0026#34;default\u0026#34; : null }, { \u0026#34;name\u0026#34; : \u0026#34;acc_rate\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, \u0026#34;float\u0026#34; ], \u0026#34;default\u0026#34; : null }, { \u0026#34;name\u0026#34; : \u0026#34;avg_daily_trips\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, \u0026#34;int\u0026#34; ], \u0026#34;default\u0026#34; : null }, { \u0026#34;name\u0026#34; : \u0026#34;created\u0026#34;, \u0026#34;type\u0026#34; : [ \u0026#34;null\u0026#34;, { \u0026#34;type\u0026#34; : \u0026#34;long\u0026#34;, \u0026#34;logicalType\u0026#34; : \u0026#34;local-timestamp-micros\u0026#34; } ], \u0026#34;default\u0026#34; : null } ] } まとめ macでparquetファイルを簡単に読む方法を紹介しました。\n","date":"2024-09-15T16:52:04+09:00","permalink":"https://bossagyu.com/blog/031-read-parquet-file/","title":"macでparquetファイルを読む方法"},{"content":"共分散とは 共分散は、2つの変数がどのように連動して変化するかを示す統計量です。\n正の値なら同じ方向に変化しやすく、負の値なら逆方向に変化しやすいことを意味します。\n計算式 共分散は次の式で計算されます：\nポイント 共分散が 0 に近い場合、2つの変数に強い関係はないと考えられます。 値がスケールに依存するため、相関係数を使うことが一般的です。 Pythonでの例 以下は、Pythonを使った共分散の計算例です。\n1 2 3 4 5 6 7 8 9 10 11 import numpy as np # サンプルデータ X = [2.1, 2.5, 4.0, 3.6] Y = [8, 10, 12, 14] # 共分散の計算 cov_matrix = np.cov(X, Y, bias=True) covariance = cov_matrix[0][1] print(f\u0026#34;共分散: {covariance}\u0026#34;) 実行結果 1 共分散: 1.53 共分散と相関係数の違い 特徴 共分散 相関係数 スケール依存 あり なし 値の範囲 -∞ から ∞ -1 から 1\\ まとめ 共分散は、2つの変数の関係性を理解するための基本的な指標です。\nただし、スケール依存性があるため、相関係数と併用することでより深い分析が可能です。\n","date":"2024-09-02T10:00:00+09:00","permalink":"https://bossagyu.com/blog/030-covariance/","title":"共分散についての調査"},{"content":"gRPCとは PRCを実現するためにGoogleが開発したプロトコルの一つ Protocol Bufferを使ってデータをシリアライズし、高速な通信を実現できる点が特徴 IDLを使ってあらかじめAPIの仕様を.protoファイルとして定義し、そこから、サーバ側\u0026amp;クライアント側に必要なソースコードを生成する。 REST と gRPCの違い\nRESTはリソース志向、RPCはメソッドの呼び出しが起点となり、データは副産物であるという考え方。 利点と欠点 利点 HTTP/2による高パフォーマンス Protocol Buffersによるデータ転送 IDLを書くことになるので、スキーマファーストで開発することになる 柔軟なストリーミング方式 欠点 HTTP/2非対応 ブラウザの対応状況が不十分 言語によって機能の実装増強にばらつきがある バイナリにシリアライズされると人間が読めない RESTでも十分早い .protファイル gRPCではシリアライズフォーマットとしてProtocol Buffersを利用する。\n.proto を拡張子として持つファイル上にスキーマ定義を行い、 protoc コマンド絵各言語用のコードを生成する。\nProtocol Buffersでは全ての値が型を持つ。型はスカラー型とメッセージ型に分けることができる。\nスカラー型 数値、文字列、真偽値、バイト配列 メッセージ型 複数のフィールドを持ったメッセージ型 メッセージ型は一つの .proto ファイルに複数定義することができる 1 2 3 4 5 message Person { int32 id = 1; string name = 2; string email =3; } gRPCのQuick Startを実施する 今回はPythonの環境を用いて、gRPCのQuick Startを実施する。\nhttps://grpc.io/docs/languages/python/quickstart/\n起動に必要なPythonの環境を整える。\n1 2 python -m pip install grpcio python -m pip install grpcio-tools サンプルコードのダウンロード\n1 2 git clone -b v1.64.0 --depth 1 --shallow-submodules https://github.com/grpc/grpc cd grpc/examples/python/helloworld サーバを起動する。\n1 2 3 4 python greeter_server.py # 出力 Server started, listening on 50051 別のターミナルを起動し、クライアントを起動する。\n1 2 3 4 5 python greeter_client.py ## レスポンス Will try to greet world ... Greeter client received: Hello, you! gRPCのクライアントとサーバを用いて通信を行うことができました。\n.proto ファイルを変更してみる 今回は、helloworld.proto ファイルを変更して、新しいメソッドを追加してみます。\nhelloworld.prot ファイルが格納されている\n1 cd grpc/examples/protos 以下のように修正する\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 syntax = \u0026#34;proto3\u0026#34;; option java_multiple_files = true; option java_package = \u0026#34;io.grpc.examples.helloworld\u0026#34;; option java_outer_classname = \u0026#34;HelloWorldProto\u0026#34;; option objc_class_prefix = \u0026#34;HLW\u0026#34;; package helloworld; // The greeting service definition. service Greeter { // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) {} // 以下の1行を追加 rpc SayHelloAgain (HelloRequest) returns (HelloReply) {} rpc SayHelloStreamReply (HelloRequest) returns (stream HelloReply) {} rpc SayHelloBidiStream (stream HelloRequest) returns (stream HelloReply) {} } // The request message containing the user\u0026#39;s name. message HelloRequest { string name = 1; } // The response message containing the greetings message HelloReply { string message = 1; } grpcのコードを生成する\n1 2 3 cd examples/python/helloworld python -m grpc_tools.protoc -I../../protos --python_out=. --pyi_out=. --grpc_python_out=. ../../protos/helloworld.proto 以下のファイルが再作成されている。\n1 2 3 4 5 ls -l -rw-r--r--@ 1 xx xx 1823 9 1 18:12 helloworld_pb2.py -rw-r--r--@ 1 xx xx 578 9 1 18:12 helloworld_pb2.pyi -rw-r--r--@ 1 xx xx 7018 9 1 18:12 helloworld_pb2_grpc.py 更新される _pd ファイルとは、protocol Buuffersの定義クラスが自動で生成されており基本的にはさわらない。\ngreeter_server.py を更新する。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from concurrent import futures import logging import grpc import helloworld_pb2 import helloworld_pb2_grpc class Greeter(helloworld_pb2_grpc.GreeterServicer): def SayHello(self, request, context): return helloworld_pb2.HelloReply(message=\u0026#34;Hello, %s!\u0026#34; % request.name) # 以下の関数を追加 def SayHelloAgain(self, request, context): return helloworld_pb2.HelloReply(message=\u0026#34;Hello Again, %s!\u0026#34; % request.name) def serve(): port = \u0026#34;50051\u0026#34; server = grpc.server(futures.ThreadPoolExecutor(max_workers=10)) helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(), server) server.add_insecure_port(\u0026#34;[::]:\u0026#34; + port) server.start() print(\u0026#34;Server started, listening on \u0026#34; + port) server.wait_for_termination() if __name__ == \u0026#34;__main__\u0026#34;: logging.basicConfig() serve() greeter_client.py を更新する。\n1 2 3 4 5 6 7 8 9 10 11 def run(): # NOTE(gRPC Python Team): .close() is possible on a channel and should be # used in circumstances in which the with statement does not fit the needs # of the code. print(\u0026#34;Will try to greet world ...\u0026#34;) with grpc.insecure_channel(\u0026#34;localhost:50051\u0026#34;) as channel: stub = helloworld_pb2_grpc.GreeterStub(channel) response = stub.SayHello(helloworld_pb2.HelloRequest(name=\u0026#34;you\u0026#34;)) print(\u0026#34;Greeter client received: \u0026#34; + response.message) response = stub.SayHelloAgain(helloworld_pb2.HelloRequest(name=\u0026#34;you\u0026#34;)) print(\u0026#34;Greeter client received: \u0026#34; + response.message) serverを再起動し、clientを実行する。\n1 2 3 4 5 6 python greeter_server.py python greeter_client.py # 出力 Greeter client received: Hello, you! Greeter client received: Hello Again, you! 追加したメソッドが正常に動作していることが確認できました。\nまとめ 今回はgRPCについて調査の調査と公式ドキュメントのチュートリアルを行いました。\ngRPCはスキーマファーストで開発することができ、HTTP/2による高パフォーマンス通信が可能であることら、最近ではREST APIの代替手段として注目されている技術ですので、ぜひ抑えておきたいです。\n","date":"2024-09-01T17:53:57+09:00","permalink":"https://bossagyu.com/blog/029-grpc/","title":"gRPCについての調査"},{"content":"GPT-4oの登場 2024年5月13日に、OpenAI社より新しいGPTモデル、ChatGPT-4oが発表されました。\nGPT-4oは従来のモデルと比較して以下の内容が向上しています。\n自然な対話の実現 より高速な応答 多言語対応の強化 より高い信頼性 特に注目すべきは、推論速度と質の向上です。これにより、リアルタイムの対話システムにおいても高い性能を発揮し、よりスムーズで自然なコミュニケーションが可能となります。\nリアルタイム対話の応用 以下の動画では、スマートフォンを利用してChatGPT-4oとのリアルタイム対話のデモンストレーションが紹介されています。\n実際にご覧いただけるとわかるように、ChatGPT-4oは人間との対話に近いレベルで応答を返すことができます。\nこれにより、ユーザーは非常に自然な対話を楽しむことができます。\n今までのChatGPTシリーズでは音声を文字列へ変換し、その後GPT-4へ入力することで対話を行っていました。\nこれは、音声に含まれる声色などの感情の情報が失われることを意味しています。\nしかし、ChatGPT-4oでは音声からモデルのトレーニングを行っているため、声色などの情報も考慮されており、より自然な対話が実現されています。\nまた、音声から直接音声を返却するので、テキストを解釈するステップがない分より高速に応答が返ってくるようになっています。\n多言語対応の強化 GPT-4oは、多言語対応の強化も図っています。\n今までは英語で質問をすると、精度が高い回答が得られるが、日本語で質問をすると精度が落ちるという問題がありました。\n今回の多言語対応の強化により、日本語でも高い精度で回答が得られるようになりました。\nまとめ 今回は、OpenAI社が発表したChatGPT-4oについて紹介しました。\nChatGPTのモデルの精度の向上はめざましく、どんどん人間の対話に近づいていると感じます。\n今後エンジニアは生成AIをどれだけうまく使えるかで、生産性が大きく変わってくると感じているので引き続き動向を注視していきたいと思います。\n関連情報 Hello ChatGPT-4o ","date":"2024-05-14T23:22:39+09:00","permalink":"https://bossagyu.com/blog/027-chatgpt-4o/","title":"ChatGPT 4oの紹介"},{"content":"概要 2024年4月26日にEvernoteの日本法人が解散するなど、Evernoteのサービスは終了しないもののいよいよEvernoteの雲行きが怪しくなってきました。\nEvernoteは無料プランに制限が多いため、有料プランを利用しており費用がかさむこともあり、他のノートアプリへの移行を検討しました。\n以下の理由から移行先はNotionを選択しました。\nノートアプリととして基本的な機能を揃えており、Evernoteの代替として十分利用できる。 Notionは無料プランの制限がゆるく無料プランで十分使え移行することで有料プランの費用を削減できる。 Evernoteからインポートする機能がNotion側で提供されており、移行コストが非常に低い。 今回はEvernoteからNotionへの移行方法をまとめます。\n移行方法 NotionにはEvernoteからのインポート機能が提供されているため、こちらを利用するだけで簡単に移行ができます。\nNotionのアプリのメニューから 設定 を選択します。\n設定をクリックすると インポート が表示されるので、これをクリックします。\nインポートをクリックすると、インポート元のアプリケーションが表示されるので、ここで Evernote を選択します。\n連携が完了するとインポートするノートブックが選択できるようになります。\nここでの注意点です！\nノートブックを複数選択すると一気にインポートできるように見えるのですが、一気にインポートするととんでもない時間がかかった挙げ句エラーが出ることがあります。 このため、ノートブックは一つづつインポートすることをおすすめします。\n一つずつインポートした場合でも、ノートの分量が多いと数時間かかったりするので、気長に対応するとよいです。\nインポート後は特に体裁が崩れることもなく、画像やリンク、ラベルも正常にインポートされるので問題なく利用ができました。\nまとめ EvernoteからNotionへの移行方法をまとめました。 無料で機能が豊富なNotionに簡単に移行できるので、ぜひ移行を検討してみてください。\n","date":"2024-04-29T19:32:38+09:00","permalink":"https://bossagyu.com/blog/026-evernote-to-notion/","title":"EvernoteからNotionへの移行方法"},{"content":"概要 .gitignore ファイルをプロジェクトに追加することでプロジェクト事にgitのトラッキング対象から外すことができます。\nしかしながら.idea などIDEがデフォルトで生成するディレクトリを毎回プロジェクト毎にgitignoreに追加するのが面倒です。\n本記事では gitignore に設定した内容をすべてのプロジェクトに 適応する方法をまとめます。\ngitignoreを全体に適応する方法 gitはデフォルトで ~/.config/git/ignore へignore設定を見に行きます。\nこのため、~/.config/git/ignore にignore設定を記述することですべてのプロジェクトにgitignoreの内容を適応できます。\nよく .gitignore_global を作成して、core.excludesfile に登録する方法が案内されていますが、この方法だと .gitconfig に無駄な設定をいれる必要があるため、こちらの方法をおすすめします。\nプロジェクト全体にgitignoreを適応する手順 ignoreファイルを格納するためのディレクトリを作成します。\n1 mkdir -p ~/.config/git/ ignoreファイルを作成し、全プロジェクトで無視したい内容を記述してください。\n1 vim ~/.config/git/ignore 記載例\n1 2 3 .idea/ *.log node_modules/ この設定を行うことで、全プロジェクトで同じignore設定を適応できます。\nすでにトラッキングしているファイルを含む場合は、一度 git rm --cached でトラッキングを解除してください。\n参考 Git - gitignore ","date":"2024-04-16T23:16:25+09:00","permalink":"https://bossagyu.com/blog/025-git-ignore/","title":"グローバルなgitignoreを設定してプロジェクト全体に適応する方法"},{"content":"概要 Bluesky とは、旧Twitter社の元CEOであるジャック・ドーシー氏が立ち上げた分散型SNSです。\nATProtocl というプロトコルを用いて構築されたSNSで、簡単に言うと中央管理者がいないTwitterのようなものです。\n昨今の中央集権である通貨から分散型である仮想通貨への流れのように、SNSも分散型への流れがあるのかなと感じます。\n今回はそんなBlueskyのAPIをPythonを用いて実行する方法をまとめます。\nBluesky APIを使うまでのステップ API実行用パスワードの生成 Python実行環境の構築 スクリプトの作成と実行 API実行用パスワードの生成 APIを実行するためにはアカウント名とAPI実行用のパスワードの発行が必要です。\nまずは、APIの実行に利用するアカウント名を確認します。\nアカウント名は、Blueskyにログインした際に画像の箇所に表示される名前となります。\nこの際先頭の @ は不要で、私のアカウントであれば bossagyu.bsky.social がアカウント名となります。\n次に、API実行用のパスワードを生成します。\nAPI実行用パスワードは 設定 → アプリパスワード から生成できます。\nその後、アプリパスワードを追加 をクリックします。\n追加ボタンを押すと、パスワードにつける名前を聞かれます。\nこれ自体はパスワードとならず管理を容易にすることが目的なので、特にこだわりがなければそのまま作成します。\nパスワードが生成されるので、これをコピーしておきます。\nちなみに二度と表示されなくなるので、コピーを忘れないようにしましょう。コピーを忘れた場合は再生成すればOKです。\nPython実行環境の構築 Pythonの実行環境をセットアップしてください。\nvenvを用いたセットアップについては、こちら にまとめています。\n公式ドキュメント によると、Pythonのバージョンは3.7.1以上を利用する必要がありますので注意しておいてください。\nPythonの実行環境が整えば、ATProtocolを利用するためにライブラリをインストールします。\n1 $ pip install atproto インストールの確認\n1 2 3 $ pip list | grep atproto atproto 0.0.46 これで準備は完了です。\nスクリプトの作成と実行 Blueskyに投稿するスクリプトを作成します。\n1 2 3 4 5 6 7 8 9 from atproto import Client client = Client() user_name = \u0026#34;bossagyu.bsky.social\u0026#34; password = \u0026#34;*******\u0026#34; # 生成したAPI実行用パスワードを入力 client.login(user_name, password) client.send_post(text=\u0026#39;APIからの投稿です\u0026#39;) スクリプトはこれだけで、APIを用いてBlueskyに投稿できます。\nそれでは実行してみましょう。\n1 $ python post_bluesky.py 実行すると、Blueskyに以下のように無事投稿されました。\nまとめ 今回はPythonを用いてBlueskyのAPIを実行する方法をまとめました。\nBlueskyはまだまだ開発途中のSNSですが、TwitterはAPIを課金しないと使えないなどの制約があるので、無料でAPIを使ってSNSで遊んでみたいという方にはおすすめです。\n","date":"2024-04-07T23:52:09+09:00","permalink":"https://bossagyu.com/blog/024-bluesky-api/","title":"PythonでBluesky APIを用いて自動投稿する方法"},{"content":"概要 Stable Diffusionなどの画像生成用のモデルではなく ChatGPTでも画像が生成できるので、生成の方法を説明します。\nChatGPTの有料プランを利用している人は新たに課金などせずに利用できるので、大きな手間をかけずに商用利用可能な画像を生成できます。\n今回は、DALL-Eと呼ばれるChatGPT Plusの機能を利用して画像を生成します。 DALL-E3については OpenAI の公式ページを参照してください。\n画像生成の方法 サイドバーから 「Explore GPTs」を選択\n検索窓で DALL-E と入力し検索を行う。\nStart Chat をクリックし、画像生成を開始する。\n後は、生成したい画像の説明を入力するだけで画像が生成されます。\n実際に画像を生成してみる このブログで利用されている、ピンク色のサングラスを掛けた犬の画像を生成してみます。\nとりあえず ピンク色のサングラスをかけた犬 というプロンプトで打ってみます。\nまた、普段のChat GPTを使うように出力された画像に対して追加のプロンプトを入力することで加工できます。\n今回は アニメ調にしてください と追加のプロンプトを入力してみます。\nアニメ調になっているのがわかります。 このように追加のオーダーをすることでどんどん目的の画像に近づけつつ生成できます。\nまとめ 今回はChatGPTを利用して画像を生成する方法を説明しました。\n簡単に画像を生成できるのが非常に便利ですが、Stable Diffusion同様期待する画像を出力することはなかなか難しく、プロンプトを調整する必要があります。 Stable Diffusionを利用したときよりもプロンプトを工夫しなくても良い画像が生成されるので、このあたりはモデルの性能差なのかなと思いました。\nちなみに、やりすぎると以下のような文章が出力され、待つように言われるので生成回数には制限がついていそうです。 マシンソースがある人はやっぱりローカルmacでStable Diffusionでやるのが良さそうですね。\n","date":"2024-03-31T17:35:07+09:00","permalink":"https://bossagyu.com/blog/023-chatgpt-create-image/","title":"chatGPTで画像を生成する方法"},{"content":"概要 この記事では、TypeScriptにおけるEnumの使い方について説明します。\nEnumとは Enum（列挙型）は、特定の値の集合を表す型です。\n多くの言語に実装されていますが、JavaScriptには存在しません。しかし、TypeScriptではEnumがサポートされています。\nEnumの使い方 以下のようにEnumを定義します。\n1 2 3 4 5 6 7 enum Status { zero, one, two } console.log(Status.zero); // 0 Enumは、デフォルトで数値を割り当てられ、0から始まります。 生成されるJavaScriptコードは以下の通りです。\n1 2 3 4 5 6 7 var Status; (function (Status) { Status[Status[\u0026#34;zero\u0026#34;] = 0] = \u0026#34;zero\u0026#34;; Status[Status[\u0026#34;one\u0026#34;] = 1] = \u0026#34;one\u0026#34;; Status[Status[\u0026#34;two\u0026#34;] = 2] = \u0026#34;two\u0026#34;; })(Status || (Status = {})); console.log(Status.zero); // 0 また、enumの値を文字列で指定することもできます。\n1 2 3 4 5 6 7 enum Status { zero = \u0026#39;zero\u0026#39;, one = \u0026#39;one\u0026#39;, two = \u0026#39;two\u0026#39; } console.log(Status.zero); // zero 文字列比較を行う場合は、以下のように記述します。\n1 2 3 4 5 6 7 8 const stringZero :String = \u0026#39;zero\u0026#39;; const value = stringZero as StringStatus; if (value === StringStatus.zero) { console.log(\u0026#39;value is zero\u0026#39;); } else { console.log(\u0026#39;value is not zero\u0026#39;); } まとめ この記事では、TypeScriptにおけるEnumの使い方について説明しました。\nenumを利用することで、コードの可読性、保守性を向上させることができます。\n","date":"2024-03-23T13:11:13+09:00","permalink":"https://bossagyu.com/blog/022-typescript-enum/","title":"TyeScriptにおけるEnumの使い方"},{"content":"概要 この記事では、TypeScriptの開発環境を簡単にセットアップする方法について説明します。 本記事ではMacOSを対象にしています。\nVoltaとは VoltaはNode.jsのバージョン管理ツールです。\nVoltaの公式サイト で紹介されている通り以下の特徴を備えています。\n高速 Rustで構築されており、Node.jsのバージョン切り替えが高速です。 信頼できる プロジェクトの全員が同じツールを利用可能 ユニバーサル パッケージマネージャー、ノードランタイム、OSに依存なく利用可能。 今まではnodebrewなどを利用することが、一般的でしたが、現在はVoltaを利用するケースが増えている印象です。\nVoltaとNode.jsのインストール voltaのインストールは以下のコマンドだけで完了です。\n1 curl https://get.volta.sh | bash パスが通っていないことがあるのでzshを利用している方は以下のコマンドでパスを通してください。\n1 2 3 echo \u0026#39;VOLTA_HOME=$HOME/.volta\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo \u0026#39;export PATH=$VOLTA_HOME/bin:$PATH\u0026#39; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc 動作を確認。バージョンが表示されれば問題なくインストールができています。\n1 volta -v voltaを利用してNode.jsをインストールします。\nバージョンの指定をしない場合最新のLTSがインストールされます。\n1 volta install node yarnをインストールしてTypeScriptのプロジェクトを作成 npmとyarnの違い npm, yarnともにNode.jsのパッケージマネージャーとなります。\nそれぞれの特徴は以下の通りです。\nnpm\nNode.jsがリリースされた翌年（2010年）リリース Node Package Managerの略 package-lock.jsonファイルを自動的に生成する Node.jsをインストールすれば自動的にインストールされる yarn\n2016年リリース Facebook、Google、Exponent、Tildeによって開発された新しいJavaScriptパッケージマネージャー npｍと互換性がある 同じpackage.jsonが使える npmより厳密にモジュールのバージョンを固定できる npmよりインストールが速い yarnの方が優れているように見えますが、最近ではnpmがアップデートされて機能の差はあまりないようです。\n今回はyarnを利用してTypeScriptのプロジェクトを作成します。\nyarnのインストール voltaを利用してyarnをインストールします。\n1 volta install yarn インストールされているかを確認します。\nlistの結果にyarnが表示されれば問題なくインストールされています。\n1 volta list TypeScriptのプロジェクトを作成 yarnの初期化\n1 yarn init -y Node.jsのインストール\n1 volta pin node@20.0.0 TypeScriptのインストール\n1 yarn add typescript node-tsのインストール\n1 yarn add --dev ts-node tsconfig.jsonを作成します。\ntsconfig.jsonはTypeScriptの設定ファイルで、コンパイル時の設定を記述します。\n今回は、console.logを利用するため、今回はtargetを es2016 に設定します。特にデフォルトで生成されるものから変更する必要はありません。\n1 yarn tsc --init サンプルプログラムを実行してみる。\n1 2 3 4 5 echo \u0026#34;console.log(\u0026#39;Hello, TypeScript!\u0026#39;);\u0026#34; \u0026gt; hello.ts yarn ts-node hello.ts # 以下のように表示されれば成功 Hello, TypeScript! 無事にテストスクリプトが動きました。\n以上でTypeScriptの開発環境のセットアップが完了です。\nまとめ 本記事では、Voltaを利用してTypeScriptの開発環境を簡単にセットアップする方法について説明しました。 Voltaを利用することで、Node.jsのバージョン管理が簡単になり、開発環境のセットアップがスムーズに行えます。 また、VoltaでNode.jsのバージョンを指定すると、package.jsonにバージョンが記述され、他の開発者とのバージョンの差異を解消することができる点も魅力的ですね。\n","date":"2024-03-10T13:11:13+09:00","permalink":"https://bossagyu.com/blog/021-typescript-setup/","title":"Voltaを利用してTypeScriptの開発環境を簡単にセットアップする方法"},{"content":"概要 この記事では、ITIL v4のキャパシティ及びパフォーマンス管理について説明します。 た、理解した内容をもとに自分の経験を当てはめキャパシティ及びパフォーマンス管理のプロセスについて説明します。\nキャパシティおよびパフォーマンス管理とは サービス及びサービスを支えるリソースのパフォーマンスを管理することです。\nキャパシティとパフォーマンスの管理活動を通じて、サービスのパフォーマンスを最適化し、サービスのキャパシティを適切に確保することが目的です。\nキャパシティ及びパフォーマンス管理のプロセス キャパシティ及びパフォーマンス管理のプロセスは以下の2つがあります。\nキャパシティとパフォーマンスコントロールの確立 サービスのキャパシティとパフォーマンスの分析と改善 キャパシティとパフォーマンスコントロールの確立 キャパシティとパフォーマンスコントロールの確立は、サービスが利用するITリソースの使用量と性能基準について、要件を利害関係者と合意し、それらを評価するタイミング・基準値・報告形式を決めることです。\n以下の流れで実現されます。\nサービスキャパシティとパフォーマンス要件の特定 サービスキャパシティとパフォーマンス要件の合意 キャパシティとパフォーマンスの要件の決定 キャパシティとパフォーマンス評価指標とレポートの設計 上記プロセスに対して筆者の経験を当てはめると以下の通りになりました。\nサービスキャパシティとパフォーマンス要件の特定 筆者は社内PFとしてAPIを提供していたため、社内の利用者から求められるレイテンシー性能（99%ile Nms）を特定しました。 上記のしきい値をベースにパフォーマンス検証を行い、1インスタンスあたりのスループットを測定しました。 スループットをベースに必要となる金額を算出しました。 サービスキャパシティとパフォーマンス要件の合意 レイテンシー性能とスループットを関係者と合意を行いました。 キャパシティとパフォーマンスの要件の決定 こちらについては合意内容と変わらず キャパシティとパフォーマンス評価指標のレポートの設計 パフォーマンスについてはDynatraceと呼ばれるトレーシングツールを利用して計測、レポートを作成しました。 サービスのキャパシティとパフォーマンスの分析と改善 サービスの出力ログ・インシデント情報から使用量と性能状況の問題点を分析する。\n以下の流れで実現されます。\nキャパシティとパフォーマンスの分析 キャパシティとパフォーマンスの報告 キャパシティとパフォーマンスの計画と設計 上記プロセスに対して筆者の経験を当てはめると以下の通りになりました。\nキャパシティとパフォーマンスの分析 パフォーマンスの分析については、Dynatraceを利用して、レイテンシー性能とスループットを分析しました。 インシデント情報から、パフォーマンスの問題点を特定しました。 キャパシティとパフォーマンスの報告 パフォーマンスの報告については、Dynatraceのダッシュボードを利用して、レイテンシー性能とスループットを可視化しました。 キャパシティとパフォーマンスの計画と設計 利用者の拡大によって現状のキャパシティでは受け入れが難しくなります。 需要予測を立て必要なキャパシティの増強計画を立てて、実行に移すようにしています。 まとめ 今回はキャパシティ及びパフォーマンス管理について学習した内容に基づき筆者の経験を当てはめ説明しました。 キャパシティ及びパフォーマンス管理では、可用性管理相当のことをキャパシティとパフォーマンスの観点から行うことが理解できました。\n筆者の経験ではパフォーマンスの話と可用性の話についてはセットで行うことが多いのであまり独立して行わないと感じました。\n","date":"2024-02-27T08:53:36+09:00","permalink":"https://bossagyu.com/blog/020-itilv4-capacity-and-performance-management/","title":"ITIL v4 キャパシティ及びパフォーマンス管理について解説"},{"content":"概要 この記事ではMacにStable Diffusion Web UIをインストールし、ローカルで利用する方法を紹介します。\nStable Diffusionとは Stable Diffusionは、AIを用いた画像処理技術の一つです。 テキストを入力することで、そのテキストに対応する画像を生成することができます。\n以下は、Stable Diffusionのアニメ画像を出力できるモデルで、黒髪の少女を出力した例です。\nStable Diffusionを利用する方法 Stable Diffusionを利用方法は大きく以下の2種類に分かれます。\nHugging Face, Dream Studio などのwebアプリケーションを利用する ローカルでStable Diffusion Web UIを利用する 本記事では、ローカルでStable Diffusion Web UIを利用する方法を紹介します。 試しに使って見るだけであれば、webアプリケーションを利用するのが簡単ですが、画像を大量に生成する場合は制限があったり、費用がかかったりするので、 ある程度の量を生成する場合はローカルで利用することをおすすめします。\nStable Diffusion Web UIをローカルで利用する方法 今回はAUTOMATIC1111氏が公開している、stable-diffusion-web-uiを利用します。\n動作する環境を整える stable-diffusion-web-uiをインストールする モデルファイルを配置する stable-diffusion-web-uiを起動し画像を生成する 1. 動作する環境を整える まずは、ローカルで動作させるにあたって、Pythonやその他ライブラリが必要であるため、homebrewを利用してインストールします。\nhomebrewのインストール\n1 /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; homebrewのパスを通す\n1 export PATH=\u0026#34;$PATH:/opt/homebrew/bin/\u0026#34; 関連ライブラリのインストール\n1 brew install cmake protobuf rust pyenv git wget pyenvを用いて、Python環境のセットアップ。複数のPythonのバージョンを使い分けられるようにします。 このあとにvenvも登場します。pythonの環境の構築についてはこちらの記事を参考にしてください。\n1 2 pyenv install 3.10.6 pyenv local 3.10.6 2. stable-diffusion-web-uiをインストールする git cloneでリポジトリをクローンします。\n1 2 git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git cd stable-diffusion-webui venvで仮想環境を設定、他の環境を汚さないようにします。\n1 2 python -m venv venv source venv/bin/activate これで環境構築は完了です。\n3. モデルファイルを配置する 次にモデルファイルをダウンロードしてきて、stable-diffusion-webui/models/Stable-diffusion/ ディレクトリに配置します。\nモデルファイルは以下のサイトからダウンロードできます。\nCivitai Hugging Face 今回はCivilaiから bule_pencil のモデルをダウンロードしてきて利用してみます。\nCivilaiの検索窓にbule_pencilと入力し、検索します。\n検索結果からbule_pencilを選択し、Downloadボタンをクリックします。\nダウンロードしたモデルをディレクトリに移します。\n1 mv ~/Downloads/bluePencilXL_v401.safetensors models/Stable-diffusion/ 4. stable-diffusion-web-uiを起動し画像を生成する 最後に、stable-diffusion-web-uiを起動し、画像を生成します。\n1 ./webui.sh 起動したらプロンプトにテキストを入力し、画像を生成します。\nStable Diffusion checkpointで先程ダウンロードしたbule_pencilを選択します。 promptに生成したい画像の要素を入力します Negative promptに生成してほしくない画像の要素を入れます。 Generateをクリックします。 ピンク色のサングラスをかけた犬という意味のテキストを入力してみたところ、ちゃんと出力されました。\nもちろん、他のテキストを入力することで、様々な画像を生成することができます。\nまとめ この記事ではMacにStable Diffusion Web UIをインストールし、ローカルで利用する方法を紹介しました。 ローカルで利用することで、制限があったり、費用がかかったりするwebアプリケーションを利用するよりも、自由に画像を生成することができます。\n","date":"2024-02-12T11:24:59+09:00","permalink":"https://bossagyu.com/blog/019-stable-diffusion/","title":"MacでStable Diffusion Web UIを使う方法"},{"content":"概要 ITIL v4の事業分析について、学習し理解した内容をまとめます。\nまた、理解した内容をもとに自分の経験を当てはめ、事業分析のプロセスについて説明します。\n事業分析とは 事業分析とは、事業または他の何らかの要素を分析し、それらのニーズへの対応やビジネス上の課題を解決するためのソリューションを提案するプロセスです。\n「事業」分析とされていますが、対象は事業だけでないことに注意が必要です。\n事業分析手法の例 事業分析手法の例として以下のようなものがあります。\nSWOT分析 ユーザーストーリー 具体的ない方法についてはここでは本題ではないため他のサイトに譲ります。\n私は例には挙げられていませんが、カスタマージャーニーマップをよく使います。\n事業分析のプロセス 事業分析のプロセスは以下の２つのプロセスがあります。\n事業分析アプローチの設計と維持 事業分析とソリューションの特定 事業分析アプローチの設計と維持 このプロセスの焦点は、組織の現在および予想されるニーズに対処することにより、ビジネス分析に対する一貫した効果的なアプローチを確立することです。\n以下の流れで実行されます。\n組織と要件を分析する ビジネス分析のアプローチ手法のレビュー ビジネス分析アプローチを実行する 私が所属する組織ではあまり、事業レイヤーレベルの分析を行うことはないですが、一般的に要件・要求があればそれを特定の手法で分析しましょうというお話であると理解しています。\n事業分析とソリューションの特定 このプロセスは、利害関係者のニーズと要件を分析することに重点が置かれています。 分析の結果から利害関係者のニーズと要件に対処するためのソリューションの特定と提案が含まれています。\n以下の流れで実行されます。\nステークホルダーからの情報収集と分析 ソリューションのオプションを定義し、推奨されるソリューションを特定する ソリューション提供チームへのサポートの提供 ソリューションのパフォーマンスと評価 上記活動について、私の経験を当てはめると以下のように理解しました。\n1,2について ステークホルダーから情報を収集、得た情報の分析を行う。 分析結果から解決すべき課題(why)を特定、課題に対してどのような解決策があるか(what)を決定する。 3について 1,2で特定されたwhy, whatに対して、どのような解決策(how)があるかをプロダクトチームと一緒に考える この際に解決手法とともに評価基準をきめ、どのような変化があればこのソリューションが成功したと言えるかを決定する。 またこの際に効果はどの程度の時間軸で現れるのかの認識を合わせておくとよい。 4について 定めた評価基準に対して、ソリューションがどの程度達成されているかを定期的に評価する。 数値については手動集計だと見なくなるので、個人的には自動化してGrafanaなどで可視化することをおすすめします。 まとめ 今回は事業分析について学習した内容に基づき私の経験を当てはめ説明しました。\n個人的な理解としては事業分析といいつつ事業だけを対象としないこと、分析といいつつ分析したあとのプロセスも含まれていることが理解できました。\n参考 Business analysis management: ITIL 4 Practice Guide ","date":"2024-02-09T09:00:56+09:00","permalink":"https://bossagyu.com/blog/018-itilv4-business-analysis/","title":"ITIL v4 事業分析について解説"},{"content":"概要 この記事では、VSCodeでGithub Copilotを設定して使う方法を説明します。 前提としてGithub Copilotのアカウントが必要です。\nVSCodeでGithub Copilotを使えるようにするまで 拡張機能をインストール まずは、VSCodeに拡張機能をインストールします。 VSCode を開き、左メニューの四角形が4つあるアイコンをクリックし、検索用テキスト入力に「copilot」と入力します。 「install」をクリックし、インストールを開始してください。\nGitHubとの連携 installをクリックし、installが完了すれば以下のような画面が表示されるので、「Sign in to GitHub」をクリックします。\nGitHubのアカウントへのアクセスを要求されるので「Allow」で許可します。\n「Authorize Visual Studio Code」をクリックし、許可します。\nこれでGithub CopilotとVSCodeの連携が完了し、使えるようになりました。\n使い方 基本的には、コードを書いていくだけで自動的に保管されるようになります。 保管内容が提案されるので以下のコマンドを使いながらコードを書いていくと良いでしょう。\nチートシート 機能 キー 提案を受け入れる Tab 提案を拒否する Esc Copilotを開く Ctrl + Enter 次の提案 Alt/Option + ] 前の提案 Alt/Option + [ インラインCopilotをトリガーする Alt/Option + \\ まとめ VSCodeでGithub Copilotを設定して使う方法について説明しました。\nまた、Github Copilotはソースコードだけではなく文章にも保管を行ってくれます。\nこのブログもGithub Copilotで保管を行いながら書いており、かなり効率化できています。\nぜひ使ってみてください。\n","date":"2024-02-04T22:34:51+09:00","permalink":"https://bossagyu.com/blog/017-vscode-copilot/","title":"VSCodeでGithub Copilotを設定して使う方法"},{"content":"概要 この記事では、ITIL v4の可用性管理について説明します。 また、理解した内容をもとに自分の経験を当てはあめ、可用性管理のプロセスについて説明します。\n可用性管理とは 可用性管理とは、サービスの可用性を確保するための活動のことです。 可用性管理の目的は、サービスが顧客とユーザーのニーズを満たすために合意されたレベルの可用性を確実に提供することです。\n可用性管理のプロセス 可用性管理のプロセスは以下２つがあります。\nサービス可用性制御の確立 サービス可用性の分析と改善 サービス可用性制御の確立 サービス可用性制御の確立は、サービスの可用性を確保するための活動のことです。 以下の流れで実現されます。\nサービス可用性要件の特定 サービス可用性要件の合意 可用性測定要件の決定 可用性メトリクスと報告の設計 上記プロセスに対して自分の経験を当てはめると以下のように理解しました。\nサービス可用性要件の特定 どのような利用者がおり、サービスが停止したときの事業リスクなどの影響を特定する。 自分のサービスは社内のPFなので、各サービスがPFとして利用しておりそれぞれどのような影響があるかを特定しました。 サービス可用性要件の合意 SLAの形でサービスの可用性（稼働率99%）などを合意する 稼働率では停止判定の基準や、例外事由なども明らかにしました。 可用性測定要件の決定 測定要件については可用性要件合意の段階で何をサービス停止とするかを決めているので、測定要件は特になし。 可用性メトリクスと報告の設計 基本的には「ダウンタイム/稼働時間」で設計 報告については、可用性メトリクスを可視化するためのダッシュボードを作成しました。 サービス可用性の分析と改善 サービス可用性の分析と改善は、その名の通り可用性の分析と改善を行うプロセスです。 以下の流れで実現されます。\nサービス可用性の分析 サービス可用性の報告 サービス可用性の計画と設計 上記プロセスに対して自分の経験を当てはめると以下のように理解しました。\nサービス可用性の分析 サービス可用性が達成されていることを確認し、集計します。 サービス可用性の報告 可用性をダッシュボードに反映、誰でも見られる状態にします。 サービス可用性の計画と設計 可用性を割るような障害が発生した場合、再発防止のための計画を立てました。 参考 Availability management: ITIL 4 Practice Guide ","date":"2024-01-30T20:34:58+09:00","permalink":"https://bossagyu.com/blog/016-itilv4-availability-management/","title":"ITIL v4 可用性管理について解説"},{"content":"概要 このページではPythonでs3のオブジェクトの存在確認をする方法を説明します。\nboto3を利用して確認する方法 boto3.resourceを利用する場合は以下のようなコードでチェックできます。\n1 2 3 4 5 6 7 8 9 10 s3 = boto3.resource(\u0026#39;s3\u0026#39;) try: s3.Object(\u0026#39;bucket_name\u0026#39;, \u0026#39;object_name\u0026#39;).load() print(\u0026#34;True\u0026#34;) except ClientError as e: error_code = e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] if error_code == \u0026#39;404\u0026#39;: print(\u0026#34;Object does not exist.\u0026#34;) else: print(f\u0026#34;An error occurred: {e}\u0026#34;) boto3.clientを利用する場合は以下のようなコードでチェックできます。\n1 2 3 4 5 6 7 8 9 10 s3 = boto3.client(\u0026#39;s3\u0026#39;) try: s3.head_object(Bucket=\u0026#39;bucket_name\u0026#39;, Key=\u0026#39;object_name\u0026#39;) print(\u0026#34;True\u0026#34;) except ClientError as e: error_code = e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] if error_code == \u0026#39;404\u0026#39;: print(\u0026#34;Object does not exist.\u0026#34;) else: print(f\u0026#34;An error occurred: {e}\u0026#34;) 参考 check if a file exists in s3 bucket using boto3 ","date":"2024-01-27T21:41:37+09:00","permalink":"https://bossagyu.com/blog/015-s3-object-check/","title":"s3のオブジェクトの存在確認をする方法"},{"content":"概要 AWS API GatewayとLambdaを連携させることで、API GatewayからLambdaを呼び出すことができます。 本記事AWS API GatewayとLambdaを連携させる方法を紹介します。\n前提条件 Lambda関数については作成されていることを前提としています。 作成していない場合は、下記記事を参考に作成してください。\nAWS Lambdaを作成する方法 どの形式でAPI GatewayとLambdaを連携させるかを考える API GatewayとLambdaを連携させるに当たってどのような方式で連携させるか以下の2点について考える必要があります。\nAPI Gatewayのリクエスト形式について プロキシ統合が非プロキシ統合か API Gatewayのリクエスト形式について 以下の形式から選択できます。\nREST API HTTP API WebSocket API この内、REST APIの形式で利用する場合は、REST APIかHTTP APIのどちらかを選択することになります。\nREST APIの方が機能が多いですが、HTTP APIと比べコストがかかります。\n特に複雑なことをしないのであればHTTP APIを選択するのが良いと思います。\n詳細な比較内容については公式ドキュメントで公開されているのでそちらを参照してください。\nプロキシ統合が非プロキシ統合か プロキシ統合を利用することで、Lambdaから返される値のフォーマットが固定化されます。 基本的にはプロキシ統合を利用することをおすすめします。\nAPI Gateway で Lambda プロキシ統合を設定する 設定 Lambda関数を作成したら、トリガー追加を選択します。\nAPI Gatewayを選択します。\nトリガーを追加の画面で以下のように設定を行います。 設定がうまくいくと以下のような画面になります。\nAPI endpoint に記載のエンドポイントにcurlなどでアクセスするとLambda関数が実行されます。\n1 2 3 $ curl https://xxxxxxxxx.execute-api.ap-northeast-1.amazonaws.com/default/apigateway-get-sample \u0026#34;Hello from Lambda!\u0026#34;% まとめ 本記事ではAWS API GatewayとLambdaを連携させる方法を紹介しました。 API Gatewayと連携させることで外部から任意のタイミングでLambda関数を呼び出すことができるようになります。\n","date":"2024-01-13T18:06:52+09:00","permalink":"https://bossagyu.com/blog/014-aws-apigateway-lambda/","title":"AWS API GatewayとLambdaを連携させる方法"},{"content":"概要 良いプロダクト戦略の作り方について、「良い戦略、悪い戦略」という書籍をベースにまとめました。\n背景 業務でプロダクトオーナーとして、前プロダクトオーナーからプロダクトを引き継ぎました。\n引き継いだプロダクトにはプロダクト戦略がなく、プロダクトの方向性が定まっていない状態であったため、今後プロダクトをどうするかを含めて、プロダクト戦略を作成しました。\nプロダクト戦略を作成するにあたって、そもそも「戦略」というものが人によって様々な意味に解釈されており、なんでも「戦略」という言葉を使ってしまう傾向があると常々感じていました。 そこで、プロダクト戦略を作成するにあたって、どのような戦略を作成すれば良いのかを知るために、「良い戦略、悪い戦略」を読みました。\n良い戦略とは 良い戦略とは、こちらの打つ手の効果が一気に高まるようなポイントを見つけ、そこに狙いを絞って手持ちのリソースを集中させることで、効果を最大化することができる戦略です。 戦略とは組織が前に進むためにどのようにしたら良いかを示すものである必要があります。\n良い戦略は、以下の3つの基本構造を持っています。\n診断 基本方針 行動 診断 診断とは、状況を診断し取り組むべき課題を見極めることです。\n良い診断は死活的に重要な問題点をより分け、複雑に絡み合った状況を明快に整理することができます。\n戦略を立てる作業の多くは、今何が置きているのかを洗い出すことにあります。まずは情報を集めることが何よりも大切です。\n本書ではコンサルが提案するようなフレームに割り当てるだけではまともな戦略はできない。とこき下ろしていますが情報をしっかりと集めた上でフレームに割り当て整理することを診断の段階で行うことは有意義だと私は考えています。\n実際に私がプロダクト戦略を作成するにあたって、SWOT分析でプロダクトの置かれる状況を、インパクトマッピングを用いて現状の施策が一体誰にどのような影響を与えるのかの整理を行いました。\nそれぞれのやり方については、以下の記事や書籍を参照してください。\nSWOT分析 インパクトマッピング 基本方針 基本方針とは、診断で見つかった課題にどう取り組むか、大きな方向性と総合的な方針を示すことです。\n良い基本方針とは、目標でもビジョンではない、何曲に立ち向かう方法を固め、他の選択肢を排除することが基本方針である。と本書では述べられていました。\n決定的な一点に努力を集中させることによって、大きな効果を上げることができます。\nこのため良い戦略の中には、その戦略に従うことによって何に対してリソースを割くのかがはっきりと分かるようになっていることが大切です。\n私が戦略を立てた際には、インパクトマッピングで整理した内容をベースに、プロダクトの置かれた現状やビジョンをもとに勘案し、プロダクトの方向性を決めました。\nリソースを選択する意味では、どのターゲットのセグメントに対して、どのような価値を提供するのかを明確にすることを意識しました。\n行動 基本方針を実行するために設計された一貫性のある一連の行動のことです。 戦略が存在することですべての行動をコーディネートして方針を実行することができます。\nこのため、良い戦略は、行動を実行するための指針が含まれていることが大切となります。\n私が立てた戦略 上記をベースに私が立てた戦略は以下となりました。\n会社で立てた戦略ですので、一部ぼかして書いています。\n1 xx機能の利用者の新規利用コストを低減する かなりシンプルなものになりましたが、以下のように良い戦略の3つの基本構造を満たしていると考えています。\n診断 プロダクトの状況、課題をベースに考えたときにxxの新規利用者を増やすことが会社の利益につながると考えました。 基本方針 新規利用者を確保するために導入コストを下げるという基本方針を定めました。 行動 基本方針を実現するためにいくつかのアプローチを用意し優先順位をつけました。 ここについては行動を連想できるようなワードを戦略に含められればと考えましたが上記対応としました。 悪い戦略とは 最後に陥りがちな悪い戦略のパターンについて書いておきます。\n悪い戦略の特徴\n空疎である わかり切っていることを専門用語や業界用語で煙に巻くような内容 重大な問題に取り組まない 本来困難な課題を克服し、障害を乗り越えるためのものが戦略である。 達成容易性のみを考えた戦略は悪い戦略である。 目標と戦略と取り違えている 売りあげ10%向上など。それはただの目標である。 間違った戦略目標を掲げている 十分な周辺・原因の調査がなく戦略を掲げている状態。 まとめ 今回は、良いプロダクト戦略の作り方について、「良い戦略、悪い戦略」という書籍をベースにまとめました。 一回でいきなり良い戦略は立てられないと考えています。ただ、戦略のない環境はただ闇雲に走っているだけなのでそれが成功したのか失敗したのかすら判断することができない最低の状態です。\n最初は下手な戦略でも良いので、良い戦略とは何かを意識しつつ常に周りの状況を観察しながら、戦略をアップデートしプロダクトと組織の方向性を定めていくことが大切だと思います。\n","date":"2024-01-08T21:55:15+09:00","permalink":"https://bossagyu.com/blog/013-good-strategy-bad-strategy/","title":"プロダクト戦略の作り方"},{"content":"概要 この記事では、Hugoで作ったブログにTwitter Social Cardを設定する方法を説明します。\nTwitter Social Cardとは Twitter Social Cardとは、Twitterで記事をシェアした際に表示される画像のことです。 以下のような画像がTwitter Social Cardです。\nTwitter Social Cardは、以下の種類があります。\nSummary Card Summary Card with Large Image App Card Player Card この中でもブログのシェアを行う場合は、Summary Card か Summary Card with Large Image を利用することが多いです。\nそのれぞれのカードがどのようなものかについてはTwitterの公式ドキュメントを参照してください。\nTwitter Social Cardの設定方法 Twitter Social Cardの設定方法は、以下の2つの方法があります。\nテーマによる設定 テーマに依存しない設定 テーマによる設定 テーマによっては、Twitter Social Cardの設定を行うことができます。\n今回は私が採用しているStackを例に説明します。\nStackでは、config.toml 対して以下のような設定を行うことでTwitter Social Cardの設定が可能です。\n1 2 3 4 5 6 7 8 [opengraph.twitter] site = \u0026#34;\u0026#34; card = \u0026#34;summary\u0026#34; # summary or summary_large_image [defaultImage.opengraph] enabled = true local = false src = \u0026#34;/images/share.webp\u0026#34; # デフォルトで設定したいimageのパス テーマに依存しない設定 テーマによっては、Twitter Social Cardの設定を行うことができません。 自前で実装して、設定を行う必要があります。\nHugoの公式で実装のテンプレートが公開されていますので、それを利用することで容易に実装することが可能です。\nうまくいかない場合 うまくいかない場合はうまく設定が反映されていない、megaタグが正しく設定されていない可能性があります。 Twitterから提供されているデバッグツールを利用して、設定が正しく反映されているか確認してみましょう。\nまとめ この記事では、Hugoで作ったブログにTwitter Social Cardを設定する方法を説明しました。 Social Cardを設定することで、Twitterで記事をシェアした際に、より多くの人に記事を読んでもらうことができますのでぜひ設定しましょう。\n","date":"2024-01-06T21:45:12+09:00","permalink":"https://bossagyu.com/blog/012-social-card/","title":"Twitter Social Cardの設定方法"},{"content":"概要 この記事では、ChatGPTを利用してHugoで作ったブログを多言語対応する方法を説明します。\nGhatGPTを利用して記事を英語化する方法 Markdownで書かれた記事をChatGPTに英語化させます。 英語化を実施するにあたって、なるべき体裁を崩さないようにするために、以下のようなプロンプトをChatGPTへ入力します。\n1 2 3 マークダウンを体裁を崩さずに英語にしてください。 マークダウン以外の余分な出力は行わないでください。 英語化された内容をそのままコピーできる形で出力してください。 その後、日本語で書いた記事をそのまま貼り付けると、英語化されたMarkdownが出力されます。 出力結果の左下のコピーボタンから出力結果をコピーすれば、英語化は完了です。\nまた、ChatGPTを利用する際はGPT3.5ではなく、課金をしてGPT4を利用することを強くおすすめします。 GPT4は月々お金がかかりますが、GPT3.5と比べて圧倒的に正しい回答を返す確率が高いので、英語化以外の用途にも使えます。\nHugoで多言語対応する方法 Hugoで多言語化を行うための設定方法を記載します。\n設定ファイルの作成 config.tomlに以下のような設定を追加します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # デフォルトの言語を設定、設定しない場合英語がデフォルト判定されます。 defaultContentLanguage = \u0026#34;jp\u0026#34; [languages] # 言語ごとに設定を変更します [languages.jp] title = \u0026#34;Bossagyu Blog\u0026#34; languageName = \u0026#34;ja-jp 🇯🇵\u0026#34; LanguageCode = \u0026#34;ja-jp\u0026#34; contentDir = \u0026#34;content\u0026#34; # 日本語ブログ記事を格納するディレクトリ [languages.jp.params] [languages.en] title = \u0026#34;Bossagyu Blog\u0026#34; languageName = \u0026#34;en-US 🇺🇸\u0026#34; LanguageCode = \u0026#34;en-US\u0026#34; contentDir = \u0026#34;content.en\u0026#34; # 英語ブログ記事を格納するディレクトリ [languages.en.params] 上記のような設定を行うことで、contentディレクトリに日本語の記事を、content.enディレクトリに英語の記事を書くことで多言語対応が可能となります。\n最終的なディレクトリ構成は以下のような形になります。\n1 2 3 4 5 6 7 8 9 10 11 project/ ├── content/ │ ├── index.md │ └── blog/ │ ├── index.md │ └── article1.md └── content.en/ ├── index.md └── blog/ ├── index.md └── article1.en.md また英語化したディレクトリの中には 記事名.en.md の拡張子とすることで、デフォルト言語に対する英語の記事として認識され、記事に言語変換用のアイコンが出力されるようになります。 記事についてはGhatGPTを用いて英語化したものをコピー\u0026amp;ペーストするだけでOKです。\nまとめ この記事では、ChatGPTを利用してHugoで作ったブログを多言語対応する方法を説明しました。 ChatGPTを利用することで、英語化を簡単に行うことができます。 また、多言語対応することで記事を日本以外の国にもリーチでき、より多くの人に記事を読んでもらえます。\nローコストで多言語対応できるのでぜひやってみましょう。\n","date":"2023-12-31T20:46:36+09:00","permalink":"https://bossagyu.com/blog/011-hugo-multilingul-support/","title":"ChatGPTを利用してHugoで作ったブログを多言語対応する方法"},{"content":"概要 Faviconを簡単に作る方法を解説します。 また、HugoでFaviconを表示する方法も解説します。\nFaviconとは Faviconとは、ウェブサイトのブックマークやタブ、ホーム画面などに表示されるアイコンのことです。 Googleより検索結果に表示されるためのfaviconのガイドラインが公開されており、 こちらに従うことで検索結果に表示されるようになります。\nFaviconの作成 Faviconを作成するには、以下のサイトを利用します。\nFavicon.ico \u0026amp; App Icon Generator サイトにアクセスすると、以下のような画面が表示されます。\nfaviconを作成したいサイトのURLを入力し、Generate Faviconをクリックします。 その後表示される画面で、「Download the generated favicon」のリンクをクリックすると、faviconがダウンロードできます。\nHugoでFaviconを表示する HugoでFaviconを表示するには、themeによっても違うますが、 bearcub の場合は 以下のようにtomlに設定するだけでfaviconが表示できます。\n1 2 [params] favicon = \u0026#34;images/favicon.ico\u0026#34; まとめ Faviconを作成する方法と、HugoでFaviconを表示する方法を解説しました。\nFaviconは、ブログのブックマークやタブ、ホーム画面などに表示されるので、作成しておくと良いでしょう。\n","date":"2023-12-24T22:14:39+09:00","permalink":"https://bossagyu.com/blog/010-favicon/","title":"Faviconを作ってHugoで表示する方法"},{"content":"概要 Lighthouseを用いて、ブログのパフォーマンスを計測する方法を解説します。\nLighthouseとは LighthouseはGoogleが提供している、Webサイトのパフォーマンスを計測するツールです。 Google Chromeの拡張機能として提供されており、プラグインをインストールすることで利用することができます。\nLighthouseのインストール Lighthouseをchromeウェブストアからインストールします。\n分析したいサイトを開き、Lighthouseのアイコンをクリックします。\nGenerate reportをクリックすると、分析が始まります。\n今回は私のブログのページで実行しました。\n実行すると、以下のような結果が表示されます。\n実行の完了までに約1分くらい時間がかかります。\n結果の見方 Performance ページの読み込み速度や画像の表示速度など、webサイトのパフォーマンスが評価される。 See calculatorのリンクをクリックすると詳細に飛べる。\nAccessibility すべてのユーザーがコンテンツにアクセス、サイト内を効率的に移動できるかどうかを確認する。 スクロールすると、Accessibilityで指摘されている箇所が表示される。\nコードスニペットを貼っているところの色のコントラストが弱いことと、リンクに説明がないことを指摘されています。\nただ、指摘されている内容は自分の記述ではなく、テンプレートに依存している部分なので、これを直そうと思うとHugoのテンプレートをオーバーライドする必要がありますね。。\nBest Practices ウェブページの健全性についてテストを行います。 検証項目については結果から閲覧できます。 SEO ページが検索エンジンの結果ランキング向けに最適化されているかを確認できます。 Progressive Web App スマートフォン上のウェブページの読み込み速度を高速化できているか、PWAに最適化できているかを確認できます。 今回はチェックしてません。\nまとめ Lighthouseを用いて、ブログのパフォーマンスを計測する方法を解説しました。 特にSEOについては、Googleの検索結果に表示されるかどうかに影響するので、しっかりと対応しておきましょう。\n","date":"2023-12-22T23:08:00+09:00","permalink":"https://bossagyu.com/blog/009-light-house/","title":"Lighthouseの使い方の紹介"},{"content":"概要 AWS EventBridgeを用いてLambdaを定期実行する方法を解説します。\nAWS EventBridgeとは AWS EventBridgeは、AWSのサービス間でイベントを受け渡すためのサービスです。 EventBridgeを利用することで、イベント駆動型のアーキテクチャを構築することができます。\n詳細な説明は AWS公式ドキュメント を参照してください。\n前提 Lambda関数についてはすでに作成されていることを前提としています。 Lambda関数の作成方法については、AWA Labda 開始方法 を参照して作成してください。\n手順 EventBridgeで実行する予定のLambda関数を選択し「トリガーを追加」を選択します。\nトリガーから「EventBridge」を選択します。\nトリガーの選択を行うと、ルールの作成画面が表示されるので設定します。\n今回はcron形式で5分ごとに実行するように設定しています。\ncronのsyntaxについては Schedule type on EventBridge Scheduler のページを参考にしてください。\n設定が完了するとLambda関数のダイアグラムのトリガーにEventBridgeが追加されます。\nちなみに私はLINEにメッセージを通知するFunctionを作って動かしてみました。\nこんな感じで5分に1回通知がくるようになりました。\nまとめ AWS EventBridgeを用いてLambdaを定期実行する方法を解説しました。 今回設定したEventBridgeについてはこのまま放置しておくと、課金が発生するので、不要になったら削除しておきましょう。\n","date":"2023-12-21T23:03:13+09:00","permalink":"https://bossagyu.com/blog/008-aws-eventbrdge/","title":"AWS EventBridgeを用いてLambdaを定期実行する方法"},{"content":"概要 Google検索で引っかかるようにするために、SEO対策を行いましょうとありますが、まずはGoogle検索に認識されないと話になりません。 この記事では、Google Search Consoleを用いて自身が作成した独自ドメインのブログが、Google検索の対象となる方法を解説します。\n実現までの流れ Google Search Consoleの登録 ドメインの所有権の確認 サイトマップの登録 インデックス登録をリクエスト まとめ Google Search Consoleの登録 Google Search Consoleに登録します。\nドメインを選択肢、URLを入力します。\nドメインの所有権の確認 以下のような画像が表示されDNSの所有権を確認します。\n(念のためTXTレコードの内容は黒く塗りつぶしています。) ドメインのTXTにGoogleが指定した文字列を追加することで、所有権を確認することができます。 ドメインのDNSの設定画面に移動し、TXTレコードを追加します。\n私の場合はNetlifyでドメインを取得しているので、NetlifyのDNSの設定画面に移動します。 Domains -\u0026gt; Domain Settings -\u0026gt; DNS Records に移動し、TXTレコードを追加します。 レコードの内容はGoogle Search Consoleに表示されているものをコピーしてValueに貼り付けます。\n(Valueの部分は黒く塗りつぶしています。) DNSの反映を待ちます。ものによっては数時間かかる場合があります。\nDNSの反映はコマンドラインからでも確認できます。\n1 dig -t txt bossagyu.com その後、Google Search Consoleの所有権の確認を押します。\nこれで所有権の確認が完了し、Google Search Consoleにドメインが登録されます。\nサイトマップの登録 サイトマップを登録することで、Googleにサイトの構造を伝え、サイトのクロールを促進することができます。 Hugoで作成したブログの場合は、/sitemap.xmlにサイトマップが作成されているので、これを登録します。\nGoogle Search Consoleの左側のメニューから「サイトマップ」を選択し、サイトマップを追加します。 インデックス登録をリクエスト サイトマップに登録されていても、Googleがクロールして、その後インデックスが登録されるまでには時間がかかります。 私の場合は数日待ってもインデックスが登録されなかったので、インデックス登録をリクエストしました。\nGoogle Search Consoleの検索窓で登録したいURLを検索し、検索結果の右側にある「インデックス登録をリクエスト」を押します。 これでインデックス登録をリクエストできます。 クリックしてから数時間でインデックスが登録されました。\nまとめ Google Search Consoleを用いて自身が作成した独自ドメインのブログが検索に引っかかるようにする方法を解説しました。\nせっかくブログを作成したのに、Google検索に引っかからないのはもったいないので、ぜひ試してみてください。\n","date":"2023-12-18T19:10:04+09:00","permalink":"https://bossagyu.com/blog/007-google-search-console/","title":"Google Search Consoleを用いてブログをGoogle検索の対象にする方法"},{"content":"概要 IntellijでAWS Toolkitを使ってLambdaを効率よく開発する方法を解説します。\n実現までの流れ 事前準備 AWS Toolkitのインストール AWS Toolkitの設定 Lambdaの開発 Lambdaをローカルで実行 まとめ 事前準備 dockerのインストール intellijで利用するAWS Toolkitでは、Lambda動作させるためにDockerを使用します。\nこのため事前に こちらを参考にDockerをインストールしておいてください。\nAWS CLIのインストール AWS CLI(SAM)をインストールします。\nインストール方法は こちら を参考にしてください。\nIntellijにSAM CLI executableのパスを File -\u0026gt; Settings -\u0026gt; Tools -\u0026gt; AWS Toolkit から設定します。\n私の環境ではbrewでインストールしたので、以下のパスを設定しました。 AWS Toolkitのインストール IntellijのプラグインからAWS Toolkitをインストールします。 プラグインのインストールは こちら を参考にしてください。\nAWS Toolkitの設定 AWS Toolkitを利用するためにはAWSの認証情報を設定する必要があります。\nAWS ExplorerからAWSの認証情報を設定します。 Access Key IDとSecret Access KeyをAWSのコンソールから取得し、設定します。 設定が完了したら、AWS ExplorerにAWSのリソースが表示されるようになります。 この画像ではリージョンが us-east-1 になっていますが、Lambdaを作成するリージョンに合わせてください。\nLambdaの開発 以下のようなコードスニペットを作成します。\nlamda-sample.py\n1 2 3 def lambda_handler(event, context): print(\u0026#34;Hello World\u0026#34;) return \u0026#34;Hello World!\u0026#34; AWS ExplorerからLambdaを作成します。\nCreate Lambda Functionを選択し、必要な値を入力します。\nHandlerにはコードスニペットの \u0026lt;ファイル名\u0026gt;.\u0026lt;関数名\u0026gt; を入力してください。\nこれでLambdaの作成が完了しました。\nLambdaをローカルで実行 またToolkitを利用すると、Lambdaをローカルで実行することができます。 Runを選択すると、Lambdaがローカルで実行されます。\nまとめ IntellijでAWS Toolkitを使ってLambdaを効率よく開発する方法を解説しました。 Intellijで開発してローカルで実行できるので、開発効率がかなり上がります。\n","date":"2023-12-12T22:40:05+09:00","permalink":"https://bossagyu.com/blog/006-intellij-lamda-setup/","title":"IntellijでAWS Toolkitを使ってLambdaを効率よく開発する"},{"content":"概要 Github CopilotをIntellijで使う方法を解説します。 合わせてショートカットのチートシートを記載します。\n実現までの流れ Github Copilotの登録 Intellijの設定 Github Copilotの利用 まとめ Github Copilotの登録 Github Copilot のリンクからGithub Copilotに登録します。\nIntellijの設定 IntellijのプラグインからGithub Copilotをインストールします。\nインストールが完了したら、Intellijを再起動します。\nGithub Copilotの利用 ショートカット一覧 Intellijでコードを書いていると、Github Copilotがコードを補完してくれます。\nmacのショートカットの一覧は以下のとおりです。\nショートカット 機能 tab コードを補完する Option + ] 次の補完候補を表示する Option + [ 前の補完候補を表示する Command + → 提案の次の単語のみ受け入れる コメントによるコード補完 Github Copilotはコメントによるコード補完も行うことができます。\n例えば、以下のようなコメントを書くと、コメントの内容に応じてコードを補完してくれます。\n1 2 3 4 // このメソッドは、引数の値を2倍にして返す ← 書いたコメント public int double(int value) { // ← 生成されたコード return value * 2; } まとめ IntellijでGithub Copilotを利用する方法を解説しました。\nこの記事はGithub Copilotを利用して書いており、Markdownでのブログ作成でもかなり補完してくれるので、気になる人は試してみてください。\n","date":"2023-12-11T22:45:40+09:00","permalink":"https://bossagyu.com/blog/005-github-copilot/","title":"IntellijでのGithub Copilotの使い方"},{"content":"概要 macのローカル環境で開発を行う際のpythonの環境構築の方法について記載する。\n今回は以下の２つの仕組みを利用して、pythonのバージョン管理と仮想環境の管理を行う。\npyenv 複数のpythonのバージョンを扱うために利用する。 venv プロジェクトごとに環境を分けるために利用する。 それぞれの違いや必要性の解説はこちらの記事が参考になります。\nPythonのインストール まずは、ローカル環境にPyenvをインストールし、任意のPythonバージョンを利用できるようにします。\npyenvをインストールします。\n1 brew install pyenv インストールしたpyenvのバージョンを確認します。\n1 2 pyenv --version pyenv 2.3.35 zshに設定を追加する。\n1 2 3 echo \u0026#39;export PYENV_ROOT=\u0026#34;$HOME/.pyenv\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo \u0026#39;export PATH=\u0026#34;$PYENV_ROOT/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo -e \u0026#39;if command -v pyenv 1\u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then\\n eval \u0026#34;$(pyenv init -)\u0026#34;\\nfi\u0026#39; \u0026gt;\u0026gt; ~/.zshrc .zshrcの内容を読み込む。\n1 source ~/.zshrc インストール可能なPythonのバージョン一覧を表示する。\n1 pyenv install --list 指定したバージョンをインストールする。\n1 pyenv install 3.11.7 プロジェクトフォルダに指定したPythonのバージョンを利用する。\n1 2 3 cd \u0026lt;作成したプロジェクトフォルダ\u0026gt; pyenv local 3.11.7 pyenv versions globalの場合は全体に反映される。\n1 pyenv global 3.11.7 実行されているpythonのバージョンを確認する。\n1 python -V venvで仮想環境の作成 プロジェクトのディレクトリに仮想環境を作成する。\n1 2 # python -m venv \u0026lt;仮想環境名\u0026gt; python -m venv venv 仮想環境を有効化する。\n1 source venv/bin/activate ディアクティベートは以下のコマンドで実行できる。\n1 deactivate 以上でローカル環境の構築が完了です。\n","date":"2023-12-10T23:19:33+09:00","permalink":"https://bossagyu.com/blog/004-paython-setup/","title":"Pyenvとvenvを用いたローカル環境のセットアップ方法"},{"content":"概要 HugoをでGoogle Analyticsを設定する方法をサクッと解説します。\n実現までの流れ Google Analyticsへの登録 トラッキングIDの取得 Hugoの設定にトラッキングIDを追加 Google Analyticsの登録 [GA4] アナリティクスで新しいウェブサイトまたはアプリのセットアップを行う に従い登録を行います。\nデータストリームを追加すると、トラッキングIDが取得できるのでメモしておきます。 ※ トラッキングIDは日本語訳の影響か、測定IDという表示になっています。\nHugoの設定にトラッキングIDを追加 tomlに設定を追加 config.tomlにgoogleAnalytics = トラッキングIDを追加します。\n1 2 3 4 5 6 baseURL = \u0026#39;https://bossagyu.com\u0026#39; languageCode = \u0026#39;ja-jp\u0026#39; title = \u0026#39;Bossagyu Blog\u0026#39; theme = \u0026#39;hugo-bearcub\u0026#39; googleAnalytics = \u0026#34;G-1234ABCDEF\u0026#34; # ↑ この行を追加、トラッキングIDは自分のものに変更してください。 トラッキングコードを埋め込む テンプレートによってはtomlの設定を入れるだけで読めるものもあるみたいですが、 私の使用しているbearcubのテンプレートは対応してなかったので、 自分でヘッダにトラッキングコードを読み込むように追加します。\nコードスニペットについてはまくまく Hugo ノートを参考にさせていただきました。\nトラッキングコードを読むために layouts/partials/analytics.html を作成する。\n1 2 3 4 5 6 7 8 9 10 11 12 13 {{ if not .Site.IsServer }} {{ with .Site.GoogleAnalytics }} \u0026lt;!-- Google tag (gtag.js) --\u0026gt; \u0026lt;script async src=\u0026#34;https://www.googletagmanager.com/gtag/js?id={{ . }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;js\u0026#39;, new Date()); gtag(\u0026#39;config\u0026#39;, \u0026#39;{{ . }}\u0026#39;); \u0026lt;/script\u0026gt; {{ end }} {{ end }} ページヘッダで analytics.html を読み込むようにする。\n1 2 3 # テンプレートの内容をコピーしてきてオーバーライドする cp themes/hugo-bearcub/layouts/_default/baseof.html layouts/_default/baseof.html vim layouts/_default/baseof.html baseof.htmlに{{- partial \u0026quot;analytics\u0026quot; . -}} を追加する。\n1 2 3 4 5 6 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;{{ with .Site.LanguageCode }}{{ . }}{{ else }}en-US{{ end }}\u0026#34;\u0026gt; \u0026lt;head\u0026gt; {{- partial \u0026#34;analytics\u0026#34; . -}} \u0026lt;meta http-equiv=\u0026#34;X-Clacks-Overhead\u0026#34; content=\u0026#34;GNU Terry Pratchett\u0026#34; / 上記ソースコードを追加した状態で、再度ビルドするとGoogle Analyticsにデータが送信されるようになります。\ntips こちらの対応をしても、Google Analyticsにデータが連携されていないように見える場合は、タグの追加がうまくいっていない可能性があります。\nまずはタグがちゃんと入っているかの切り分けをするために、googleデベロッパーツールを開きトラッキングがhtml内に含まれているか確認してみるとよいです。\n","date":"2023-12-09T18:09:42+09:00","permalink":"https://bossagyu.com/blog/003-google-analytics/","title":"HugoでGoogle Analyticsの設定をする方法"},{"content":"概要 LINEのBotを利用してアプリケーションを作ってみようと考えたのでまずはBotを利用できる状態にする。\n本ページでは一番最初にLINE Message APIの登録の方法とcurlでコマンドラインからメッセージを送る方法を紹介します。\nMessaging APIを利用する LINE Developers にログインしてプロバイダーを作成する。\nプロバイダーとは(説明)\n1 2 LINE Developersサイトでは、サービスを提供し、ユーザーの情報を取得する開発者個人、 企業、または団体等をサービス提供者（LINEミニアプリではサービス事業主）と呼びます。 なので好きな文字列を入れる。\nそのまま新規チャンネルを作成する。 このまま作成ボタンを押すと新規チャンネルが作成される\nコマンドラインからポストをする Messaging API設定からQRコードを読みこんで友達追加する。\nMessaging API設定から「チャンネルアクセストークン(長期)」を取得 チャンネル基本設定から「あなたのユーザーID」取得\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 TOKEN=\u0026#34;\u0026lt;チャンネルアクセストークン(長期)\u0026gt;\u0026#34; ID=\u0026#34;\u0026lt;あなたのユーザーID\u0026gt;\u0026#34; UUID=$(uuidgen | tr \u0026#34;[:upper:]\u0026#34; \u0026#34;[:lower:]\u0026#34;) curl -v -X POST https://api.line.me/v2/bot/message/push \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ -H \u0026#34;X-Line-Retry-Key: \u0026#34; \\ -d \u0026#34;{ \\\u0026#34;to\\\u0026#34;: \\\u0026#34;${ID}\\\u0026#34;, \\\u0026#34;messages\\\u0026#34;:[ { \\\u0026#34;type\\\u0026#34;:\\\u0026#34;text\\\u0026#34;, \\\u0026#34;text\\\u0026#34;:\\\u0026#34;Hello, world1\\\u0026#34; } ] }\u0026#34; レスポンスが帰ってきて、LINEのトーク画面でBotからの投稿が行われていれば成功！\n","date":"2023-12-07T09:37:00+09:00","permalink":"https://bossagyu.com/blog/002-line-messaging-api/","title":"LINE Messaging APIの登録と使い方"},{"content":"概要 Hugoで作ったサイトをGithubで管理、Netlifyでビルドした手順を0から作れるよう記載します。\nこの方式にすると手元でMarkDownで書いたブログをGithubにPushするだけで簡単に公開できるようになります。\n流れ Hugoでサイトを生成 Githubにプッシュ Netlifyでデプロイ Hugoで静的サイトを生成 まずはHugoをインストールします。\n1 brew install hugo blogの雛形を作成します。\n1 hugo new site my-blog ブログに適応するテーマをsubmoduleとして追加します。\n1 2 3 4 5 cd my-blog git init # テーマをgithubのsubmoduleとして追加 git submodule add https://github.com/theNewDynamic/gohugo-theme-ananke.git themes/ananke hugo.tomlに記載することでテーマを適応する。\n1 echo \u0026#34;theme = \u0026#39;ananke\u0026#39;\u0026#34; \u0026gt;\u0026gt; config.toml サーバを起動させます。\n1 hugo server 起動ログの Web Server is available at http://localhost:51517/ (bind address 127.0.0.1) のような記述の http://localhost:51517/ にアクセスすればローカルに起動した静的サイトが閲覧できます。\nTips Hugoのテーマを変えたい場合は、Hugo Themas から好きなものを選んで変えてください。 これは後からでも変えられるのでとりあえずNetlifyでビルドするところまで走り切るのがおすすめ。 Tomlファイルの書き方は Configure Hugo に記載されています。 Githubにpush Github にリポジトリを作成。\n作成後以下のコマンドを実行し、サイトをpushします。\n1 2 3 4 5 6 7 8 9 10 cd my-blog echo .hugo_build.lock \u0026gt;\u0026gt; .gitignore git add . git commit -m \u0026#34;first commit\u0026#34; git branch -M main # \u0026lt;user name\u0026gt;は自分のユーザー名に置き換えてください。 # 今回はmy-blogというリポジトリを作成している例です。 git remote add origin git@github.com:\u0026lt;user name\u0026gt;/my-blog git push -u origin main pushが完了するとGithubのUI上でソースコードが閲覧できる状態になっています。\nNetlifyでデプロイ netlify へアクセスし、デプロイを行う。\nHugoの公式で案内 があるのでこちらを参考に連携を行う。\n指示に従いデプロイを完了すると以下のようにDeployの結果が published になる。\nサイト上に表示されたURLをクリックするとデプロイされたサイトにアクセスできる。 これでデプロイまではおしまい。\n以降は変更を加えてmainにpushするだけで自動デプロイが走り、サイトの内容が更新されるようになる。\n","date":"2023-12-02T00:59:37+09:00","permalink":"https://bossagyu.com/blog/001-hugo-netlify-build/","title":"Hugo + Netlify + Githubでブログを公開する"}]